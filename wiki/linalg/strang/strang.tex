\documentclass{report} 
\title{Strang}
\date{Started 5th June 2025}
\author{Malcolm}
\usepackage{amsmath} %import math
\usepackage{mathtools} %more math
\usepackage{amssymb} %for QED symbol
\usepackage{amsthm} %
\usepackage{bm}%bold math
\usepackage{graphicx} %import imaging
\graphicspath{{./images/}} %set imaging path
\begin{document}
\maketitle

\tableofcontents

\newpage

\chapter{Vectors and Matrices}
\section{Intuition for Dot product, Cosine formula, Schwarz and Triangle inequalities}
\textbf{Intuition for dot product}\\
The unit vectors $\bm{v}=(\cos\alpha,\sin\alpha)$ and $\bm{w}=(\cos\beta,\sin\beta)$ are plotted as follows
\begin{center}
\includegraphics[width=10cm]{1}
\end{center}
See first that when fixed in this form, the magnitude of both vectors is 1, with an angle $\beta-\alpha$ between them. These unit vectors have dot product
\begin{equation*}
\bm{v}\cdot\bm{w}=\cos\alpha\cos\beta+\sin\alpha\sin\beta=\cos(\beta-\alpha)
\end{equation*}
We have $\theta$ as the angle between the two vectors; see that the sign of $\bm v\cdot\bm w$ tells us whether $\theta$ is below or above a right angle (due
to the cosine function being negative for its argument $>\pi/2$ and positive for $<\pi/2$):
\begin{center}
\includegraphics[width=9cm]{2}
\end{center}
(next page)\newpage
\noindent\textbf{Cont.}\\
The idea here is that the dot product reveals the exact angle $\theta$; for unit vectors $\bm u$ and $\bm U$, the dot product $\bm u\cdot\bm U$ is the cosine of 
$\theta$. Ths remains true in $n$ dimensions (not shown).\\
\vspace{1mm}\\
See that any $\bm{u}$ and $\bm{v}$ can be fixed in the above form by normalising their lengths to get
$\bm u=\bm v/||\bm v||$ and $\bm U=\bm w/||\bm w||$. After which their dot product would give $\cos\theta$. This leads us to the \textit{cosine formula}:
\begin{equation*}
\text{\textbf{Cosine formula}: }
\frac{\bm v\cdot\bm w}{||\bm v||\,||\bm w||}=\cos\theta\quad\text{ if $\bm v$ and $\bm w$ are nonzero vectors}
\end{equation*}
\textbf{Perpendicular vectors}\\
See that when the angle between $\bm v$ and $\bm w$ is $90^\circ$, its cosine is 0; this gives us a way to test this. Also see that for perpendicular vectors:
\begin{equation*}
||\bm v+\bm w||^2=||\bm v||^2+||\bm w||^2
\end{equation*}
because
\begin{equation*}
||\bm v+\bm w||^2=(\bm v+\bm w)\cdot(\bm v+\bm w)=\bm v\cdot\bm v+\bm v\cdot\bm w+\bm w\cdot\bm v+\bm w\cdot\bm w
\end{equation*}
where $\bm v\cdot \bm w=0$.\\
\vspace{1mm}\\
\textbf{Schwarz and Triangle inequalities}\\
First, see from the cosine formula that the dot product of $\bm v/||\bm v||$ and
$\bm w/||\bm w||$ never exceeds one (since $\cos\theta$ never exceeds one). This is the the 
\textit{Schwarz inequality}:
\begin{equation*}
\text{\textbf{Schwarz inequality}: }|\bm v\cdot\bm w|\leq||\bm v||\,||\bm w||
\end{equation*}
The \textit{Triangle inequality} comes directly from the Schwarz inequality:
\begin{equation*}
\text{\textbf{Triangle inequality}: }||\bm v+\bm w||\leq||\bm v||+||\bm w||
\end{equation*}
This can be seen from
\begin{equation*}
||\bm v+\bm w||^2=\bm v\cdot\bm v+\bm v\cdot\bm w+\bm w\cdot\bm v+\bm w\cdot\bm w
\leq||\bm v||^2+2||\bm v||\,||\bm w||+||\bm w||^2
\end{equation*}
The square root gives us the triangle equality (side 3 cannot exceed side 1 + side 2).
\newpage

\section{Intuition for column rank being equal to row rank}
If all columns are in the same direction, why does it happen that all the rows are the same direction?\\
\vspace{1mm}\\
Consider the matrix, see that column 2 is $m$ times column 1:
\begin{equation*}
\bm A=\left[\begin{array}{cc}
a&ma\\
b&mb
\end{array}\right]
\end{equation*}
See that the second row is just $b/a$ times the first row---if the column rank is 1, then the row rank is 1. See that transposing the matrix, we have
\begin{equation*}
\bm A=\left[\begin{array}{cc}
a(1)&b(1)\\
a(m)&b(m)
\end{array}\right]
\end{equation*}
which still has one independent column. Now consider the 3x3 case:
\begin{equation*}
\bm A=\left[\begin{array}{ccc}
a&ma&pa\\
b&mb&pb\\
c&mc&pc
\end{array}\right]
\end{equation*}
See that a similar deduction can also be made in this case, where the row rank of $A$ is equal to its column rank.\\
(next page)\newpage
\noindent\textbf{An informal proof}\\
Consider any matrix $\bm A$, suppose we go from left to right, looking for independent columns of $\bm A$ using the following procedure:
\begin{enumerate}
\item If column 1 of $\bm A$ is not zero, put it in matrix $\bm C$
\item If column 2 of $\bm A$ is not a multiple of column 1, put it in into $\bm C$
\item If column 3 of $\bm A$ is not a combination of columns 1 and 2, put it into $C$. \textit{continue}
\end{enumerate}
See that at the end $\bm C$ will have $r$ columns taken from $\bm A$, where $r$ is the rank of $\bm A$ and $\bm C$. While the $n$ columns of $\bm A$ are dependent, 
the $r$ columns of $\bm C$ will surely be independent.\\
\vspace{1mm}\\
For instance consider $\bm A$ with rank 2
\begin{equation*}
\bm A=\left[\begin{array}{ccc}
2&6&4\\
4&12&8\\
1&3&5
\end{array}\right]\quad\text{leads to}\quad \bm C=
\left[\begin{array}{cc}
2&4\\
4&8\\
1&5
\end{array}\right]
\end{equation*}
Now consider another matrix $\bm R$ to be multiplied by $\bm C$ such that 
$\bm A=\bm{CR}$. The first and third columns of $\bm A$ are already in $\bm C$, so those respective columns in $\bm R$ make up a \textit{identity matrix};
the second column of $\bm A$ is a multiple of the first, so we have
\begin{equation*}
\bm A=\bm{CR}\quad\text{is }\left[\begin{array}{ccc}
2&6&4\\
4&12&8\\
1&3&5
\end{array}\right]=
\left[\begin{array}{cc}
2&4\\
4&8\\
1&5
\end{array}\right]
\left[\begin{array}{ccc}
1&3&0\\
0&0&1
\end{array}\right]
\end{equation*}
(See that the $i$th row of $\bm A$ can be seen as a linear combination of the rows of $\bm R$ specified the $i$th row of $\bm C$. 
(or just consider $\bm A^T=\bm R^T\bm C^T$). We know that
\begin{enumerate}
\item $\bm C$ contains the full set of $r$ independent columns of $\bm A$.
\item $\bm R=[\bm{I\,\bm F}]$ contains the identity matrix $\bm I$ in the same $r$ columns that held $\bm C$.
\item The dependent columns of $\bm A$ are combinations of $\bm{CF}$ of the independent columns in $\bm C$.
\end{enumerate}
Where the matrix $\bm F$ goes into the other $n-r$ columns of $\bm R=[\bm{I\,\bm F}]$. ($\bm A=\bm{CR}$ becomes $\bm A=\bm{C[I,F]}=\bm{[C,CF]}=
[\text{indep cols of $\bm A$},\text{ dep cols of $\bm A$}]$ (in correct order).\\
\vspace{1mm}\\
See that $\bm C$ has the same column space as $\bm A$, and $\bm R$ \textit{has the same row space} as $\bm A$ (every row of $\bm A$ is a combination of the rows
of $\bm R$).\\
(next page)\newpage
\noindent\textbf{Cont.}\\
We had the example
\begin{equation*}
\bm A=\bm{CR}\quad\text{is }\left[\begin{array}{ccc}
2&6&4\\
4&12&8\\
1&3&5
\end{array}\right]=
\left[\begin{array}{cc}
2&4\\
4&8\\
1&5
\end{array}\right]
\left[\begin{array}{ccc}
1&3&0\\
0&0&1
\end{array}\right]
\end{equation*}
\textit{Here is an informal proof that the row rank of $\bm A$ equals the column rank of $\bm A$} (based from facts we already know)
\begin{enumerate}
\item The $r$ columns of $\bm C$ are independent (chosen that way from $\bm A$)
\item Every column of $\bm A$ is a combination of those $r$ columns of $\bm C$ (since $\bm A=\bm{CR}$)
\item The $r$ rows of $\bm R$ are independent (they contain the $r$ by $r$ matrix $\bm I$)
\item Every row of $\bm A$ is a combination of the $\bm r$ rows of $\bm R$
\end{enumerate}
See that for every column of $\bm A$ that goes into $\bm C$, a column of $\bm I$ goes into $\bm R$, where each column of $\bm I$ in $\bm R$ adds an independent row.\\
\vspace{1mm}\\
This means that the column rank of $\bm C$ (column space of $\bm A$) is always equal to the row rank of $\bm R$ (row space of $\bm A$)---the 
column rank of $\bm A$ is equal to the row rank of $\bm A$.\\
\vspace{1mm}\\
\textbf{More examples}\\
Rank 2:
\begin{equation*}
\left[\begin{array}{ccc}
1&2&3\\
4&5&6\\
7&8&9
\end{array}\right]=
\left[\begin{array}{cc}
1&2\\
4&5\\
7&8
\end{array}\right]
\left[\begin{array}{ccc}
1&0&-1\\
0&1&2
\end{array}\right]
\end{equation*}
Rank 2:
\begin{equation*}
\left[\begin{array}{cccc}
1&2&3&4\\
1&2&4&5
\end{array}\right]=
\left[\begin{array}{cc}
1&3\\
1&4
\end{array}\right]
\left[\begin{array}{cccc}
1&2&0&1\\
0&0&1&1
\end{array}\right]
\end{equation*}
Rank 1:
\begin{equation*}
\left[\begin{array}{cccc}
1&2&10&100\\
3&6&30&300\\
2&4&20&200
\end{array}\right]=
\left[\begin{array}{c}
1\\3\\2
\end{array}\right]
\left[\begin{array}{cccc}
1&2&10&100
\end{array}\right]
\end{equation*}
\newpage

\section{Ways to multiply $\bm{AB}=\bm C$}
\textbf{Multiplcation by columns of $\bm A$ and rows of $\bm B$}\\
A lesser known way to multiply $\bm{AB}$ is through considering the columns of $\bm A$ and the rows of $\bm B$ (contrary to the usual ideas where each 
entry of the result is a dot product of a row of $\bm A$ and column of $\bm B$):
\begin{center}
\includegraphics[width=10cm]{3}
\end{center}
We multiply each column of $\bm A$ by each row of $\bm B$; this gives us $n$ rank 1 matrices, which we then sum together; these matrices are called \textit{outer products}\\
\vspace{1mm}\\
(usually we see the $i$th column of the result as a linear combination of the columns of $\bm A$ specified by the $i$th column of $\bm B$. However
in this case the $i$th outer product is a matrix of the same size as the result, that contains all the contributions of the $i$th column of $\bm A$ to the final product.
By summing this over all $n$ columns of $\bm A$ we get the result.)\\
\vspace{1mm}\\
\textbf{Summary of methods}
\begin{center}
\includegraphics[width=10cm]{4}
\end{center}
(A nice way to intuit the the third method is to consider $(\bm{AB})^T=\bm B^T\bm A^T$; where the columns of $(\bm{AB})^T$ are the rows of $\bm{AB}$)
\newpage

\chapter{Solving linear equations $\bm{Ax}=\bm b$}\newpage

\section{Solutions to $\bm{Ax}=\bm b$}
Given a $n$x$n$ matrix $\bm A$ and an $n$x1 column vector $b$, there are three outcomes for the vector $\bm x$ that solves $\bm{Ax}=\bm b$.\\
\vspace{1mm}\\
First there may be \textit{no vector} $\bm x$ that solves $\bm{Ax}=\bm b$, or there may be exactly \textit{one} solution, or there may be \textit{infinitely many}
solution vectors $\bm x$. Here are the possibilities:\\
\vspace{1mm}\\
1. \textbf{Exactly one solution} to $\bm{Ax}=\bm b$ means that $\bm A$ has independent columns (only one particular linear combination of the columns of $\bm A$
leads to $\bm b$. That combination is specified by $\bm x$). $\bm A$ is full rank and the only solution to $\bm{Ax}=\bm{0}$ is $\bm x=\bm0$. 
$\bm A$ \textit{has an inverse matrix} $\bm A^{-1}$ (given $\bm b$, we can work backward to get $\bm x$ since only one $\bm x$ leads to $\bm b$).\\
\vspace{1mm}\\
2. \textbf{No solution} to $\bm{Ax}=\bm b$ means that $\bm b$ is not in the column space of $\bm A$, so $\bm A$ is not full rank.\\
\vspace{1mm}\\
3. \textbf{Infinitely many solutions}. See that when the columns of $\bm A$ are not independent (not full rank), then there are infinitely many ways to
produce the zero vector $\bm b=\bm0$ (this is the meaning of dependent columns), and so there are infinitely many solutions to $\bm{AX}=\bm0$.\\
\vspace{1mm}\\ 
Also see that if $\bm A$ is not full rank it means that its column space is some subspace, where
solutions only exist for $\bm b$ within that subspace.\\
\vspace{1mm}\\
As such, if there so happens to be a solution to $\bm{Ax}=\bm b$ then we can add any solution to $\bm{AX}=\bm0$:
\begin{equation*}
\bm A(\bm x+\alpha\bm X)=\bm A\bm x+\alpha\bm A\bm X=\bm b+\bm0=\bm b
\end{equation*}
For some constant $\alpha$, which gives us $\bm b$ again---we have infinitely many solutions.
\newpage

\section{Elimination and Back Substitution}
\textbf{Elimination}\\
We want to produce an \textit{upper triangular} matrix $\bm U$ from a square matrix $\bm A$. This is done through elimination; the procedure (for a 3x3 matrix) is as
follows (assuming no row exchanges):
\begin{enumerate}
\item Use the first equation(row) to produce zeros in column 1 below the first pivot.
\item Use the new second equation(row) to clear out column 2 below pivot 2 in row 2.
\item \textit{Continue to column 3}. The expected result is an upper triangular matrix $\bm U$.
\end{enumerate}
These steps can be carried out using elimination matrices $\bm E$.\\
\vspace{1mm}\\
Consider $\bm A$ and $\bm b$
\begin{equation*}
\bm A=\left[\begin{array}{ccc}
2&3&4\\
4&11&14\\
2&8&17
\end{array}\right]\quad
\bm b=\left[\begin{array}{c}
19\\55\\50
\end{array}\right]
\end{equation*}
$\bm E_{21}$ multiplies equation 1 by 2 and subtracts that from equation 2 to get a zero in the first column below the first pivot:
\begin{equation*}
\bm E_{21}=\left[\begin{array}{ccc}
1&0&0\\
-2&1&0\\
0&0&1
\end{array}\right]\implies
\bm E_{21}\bm A=\left[\begin{array}{ccc}
2&3&4\\
0&5&6\\
2&8&17
\end{array}\right],\quad
\bm E_{21}\bm b=\left[\begin{array}{c}
19\\17\\50
\end{array}\right]
\end{equation*}
(For intuition on the elimination matrices, consider the row perspective of matrix multiplication) This produced the desired zero in column 1. It changed equation 2.
To make the first pivot column zero, we subtract row 1 from row 3 using $\bm E_{31}$
\begin{equation*}
\bm E_{31}=\left[\begin{array}{ccc}
1&0&0\\
0&1&0\\
-1&0&1
\end{array}\right]\implies
\bm E_{31}\bm E_{21}\bm A=\left[\begin{array}{ccc}
2&3&4\\
0&5&6\\
0&5&13
\end{array}\right],\quad
\bm E_{31}\bm E_{21}\bm b=\left[\begin{array}{c}
19\\17\\31
\end{array}\right]
\end{equation*}
This completes elimination in column 1. Moving on to column 2 row 2 (the second pivot row). We use $\bm E_{32}$ to subtract equation 2 from equation 3
\begin{equation*}
\bm E_{31}=\left[\begin{array}{ccc}
1&0&0\\
0&1&0\\
0&-1&1
\end{array}\right]\implies
\bm U=\left[\begin{array}{ccc}
2&3&4\\
0&5&6\\
0&0&7
\end{array}\right],\quad
\bm c=\left[\begin{array}{c}
19\\17\\14
\end{array}\right]
\end{equation*}
$\bm E_{32}\bm E_{31}\bm E_{21}\bm A=\bm U$ is triangular. See that the same steps were applied to the right hand side 
$\bm b$ to produce a new right hand side $\bm c$.\\
(next page)\newpage
\noindent\textbf{Possible breakdown of elimination}\\
Elimination might fail. This occurs when zero appears in a pivot position---subtracting that zero from lower rows will not clear out the column
below that pivot. For example:
\begin{equation*}
\bm A=\left[\begin{array}{ccc}
2&3&4\\
4&6&14\\
2&8&17
\end{array}\right]\to
\left[\begin{array}{ccc}
2&3&4\\
0&0&6\\
0&5&13
\end{array}\right]=\bm B
\end{equation*}
A possible way to get around this would be to \textit{exchange} row 2 (with the zero pivot) for row (with the nonzero in that column), then carry out elimination
as per usual. This exchange is carried out using the permuation matrix $\bm P$:
\begin{equation*}
\bm{PB}=
\left[\begin{array}{ccc}
1&0&0\\
0&0&1\\
0&1&0
\end{array}\right]
\left[\begin{array}{ccc}
2&3&4\\
0&0&6\\
0&5&13
\end{array}\right]=
\left[\begin{array}{ccc}
2&3&4\\
0&5&13\\
0&0&6
\end{array}\right]
\end{equation*}
In this example the exchange produced $\bm U$ with nonzero pivots; normally there may be more columns eliminate before $\bm U$ is reached.\\
\vspace{1mm}\\
At times this exchange strategy may not work. This occurs when there is no pivot is available. Consider $\bm A'$:
\begin{equation*}
\bm A'=\left[\begin{array}{ccc}
2&3&4\\
4&6&14\\
2&3&17
\end{array}\right]\to
\left[\begin{array}{ccc}
2&3&4\\
0&0&6\\
0&0&13
\end{array}\right]=\bm U'
\end{equation*}
There is no second pivot in this case. This tells us that the matrix $\bm A'$ \textit{did not have full rank} (intuitively see that the non-pivot row can be expressed
as a linear combination of the other pivot rows, so the row space 
(which is equal to the row space of the original $\bm A'$ since the elimination steps are just linear combinations of the existing rows) 
is not full rank, and so the column space of the 
original $\bm A'$ is also not full rank.\\
\vspace{1mm}\\
In this case there will be nonzero solutions $\bm X$ to $\bm A'\bm X=0$. The columns of $\bm U'$ (and $\bm A'$) are not independent.\\
\vspace{1mm}\\
\textbf{Augmented matrix}\\
During elimination, in order to make sure that the operations on the matrix $\bm A$ are also executed on $\bm b$, one can 
\textit{include $\bm b$ as an extra column of $\bm A$}; this combination $[\bm A,\bm b]$ is called an \textit{augmented matrix}:
\begin{equation*}
\left[\begin{array}{cc}\bm A&\bm b\end{array}\right]=
\left[\begin{array}{cccc}
2&3&4&19\\
4&11&14&55\\
2&8&17&50
\end{array}\right]\xrightarrow{E}
\left[\begin{array}{cccc}
2&3&4&19\\
0&5&6&17\\
0&0&7&14
\end{array}\right]=
\left[\begin{array}{cc}\bm U&\bm c\end{array}\right]
\end{equation*}
(next page)\newpage
\noindent\textbf{Back Substitution to solve $\bm{Ux}=\bm c$}\\
Elimination (ideally) produces an upper triangular matrix $\bm U$ that has all zeros below the diagonal, with nonzero pivots. For instance we had, for some $\bm x$
\begin{equation*}
\bm{Ax}=\bm b,\quad\bm A=\left[\begin{array}{ccc}
2&3&4\\
4&11&14\\
2&8&17
\end{array}\right]\quad
\bm b=\left[\begin{array}{c}
19\\55\\50
\end{array}\right]
\end{equation*}
undergo elimination to to become
\begin{equation*}
\bm{Ux}=\bm c,\quad\bm U=\left[\begin{array}{ccc}
2&3&4\\
0&5&6\\
0&0&7
\end{array}\right]\quad
\bm c=\left[\begin{array}{c}
19\\17\\14
\end{array}\right]
\end{equation*}
See that this form allows us to easily solve the equations by going from bottom to top in a procedural manner, finding $x_3$, then $x_2$, then $x_1$:
\begin{enumerate}
\item\textit{Back substitution}: The last equation $7x_3=14$ gives $x_3=2$
\item\textit{Work upwards}: The next equation $5x_2+6(2)=17$ gives $x_2=1$
\item\textit{repeat}: The first equation $2x_1+3(1)+4(2)=19$ gives $x_1=4$
\end{enumerate}
giving us the only solution to this example $\bm x=(4,1,2)$. Remember the pivots need to be nonzero(full rank) for a single specific solution to be found.
\newpage

\section{Elimination matrices and inverse matrices}
\textbf{Elimination matrices}\\
The basic elimination step \textit{subtracts} a multiple $\ell_{ij}$ of equation $j$ from equation $i$. We always speak about \textit{subtractions} as elimination 
proceeds. For instance even if the first pivot $a_{11}=3$ and below it is
$a_{21}=-3$ where we could just add equation 1 to 2, we \textit{subtract} $\ell_{21}=-1$ \textit{times equation 1 from equation 2} (which gives
us the same result).\\
\vspace{1mm}\\
For instance here is the matrix that subtracts 2 times row 1 from row 3:
\begin{equation*}
\bm E_{31}=\left[\begin{array}{ccc}
1&0&0\\0&1&0\\-2&0&1\end{array}\right]
\end{equation*} 
If no row exchanges are needed, then three elimination matrices $\bm E_{21},
\bm E_{31},\bm E_{32}$ will produce three zeros below the diagonal to change $\bm A$ to the upper triangular $\bm U$(this just carries out elimination 
using matrices to represent each step).\\
\vspace{1mm}\\
\textbf{Inverse}\\
See that the \textit{inverse} of each matrix $\bm E_{ij}$ just \textit{adds back} $\ell_{ij}\,\cdot$ (row $j$) to row $i$. 
This leads to the inverse of their product $\bm E=\bm E_{32}\bm E_{31}\bm E_{21}$. We denote the inverse of $\bm E$ by $\bm L$. 
For instance, say some $\bm E$ subtracts 5 times row 1 from row 2, then $\bm E^{-1}$ adds 5 times row 1 to row 2:
\begin{equation*}
\bm E=\left[\begin{array}{ccc}
1&0&0\\-5&1&0\\0&0&1\end{array}\right],\quad
\bm E^{-1}=\left[\begin{array}{ccc}
1&0&0\\5&1&0\\0&0&1\end{array}\right]
\end{equation*}
(See that sequential application of these matrices, in either order, to some vector leads to no net change---as if we had multiplied by the identity.)\\
\vspace{1mm}\\
Now lets consider $\bm F$ which subtracts 4 times row 2 from row 3 (which might be a next step during elimination), naturally $\bm F^{-1}$ adds it back:
\begin{equation*}
\bm F=\left[\begin{array}{ccc}
1&0&0\\0&1&0\\0&-4&1\end{array}\right],\quad
\bm F^{-1}=\left[\begin{array}{ccc}
1&0&0\\0&1&0\\0&4&1\end{array}\right]
\end{equation*}
During elimination we would first apply $\bm E$ then $\bm F$, which is the same as applying $\bm{FE}$. Reversing the elimination would amount to 
$(\bm{FE})^{-1}$, which is the same as applying $\bm E^{-1}\bm F^{-1}$:
\begin{equation*}
\bm F\bm E=\left[\begin{array}{ccc}
1&0&0\\-5&1&0\\20&-4&1\end{array}\right]\quad\text{is inverted by}\quad
\bm E^{-1}\bm F^{-1}=\left[\begin{array}{ccc}
1&0&0\\5&1&0\\0&4&1\end{array}\right]
\end{equation*}
See that the product $\bm{FE}$ contains `20' but its inverse doesn't. In $\bm{FE}$, row 3 feels an effect of size 20 from row 1. However in $\bm E^{-1}\bm F^{-1}$ 
that doesn't happen.\\
(next page)\newpage
\noindent\textbf{$\bm L$ is the inverse of $\bm E$}\\
$\bm E$ is the product of all the elimination matrices $\bm E_{ij}$, it turns $\bm A$ into its upper triangular form $\bm{EA}=\bm U$ (assuming no row exchanges). 
The difficulty with $\bm E$ is multiplying all the separate elimination steps $\bm E_{ij}$ does not produce a good formula; illustrating 
$\bm E=\bm E_{32}\bm E_{31}\bm E_{21}$:
\begin{center}
\includegraphics[width=10cm]{5}
\end{center}
See that the bottom left corner is dependent on multiple constants (since mutating the third row requries knowledge of the mutations that already occured to the
second row).\\
\vspace{1mm}\\
Now consider the inverse $\bm E^{-1}_{21}\bm E^{-1}_{31}\bm E^{-1}_{32}=\bm E^{-1}=\bm L$ (see that inverses need to be applied in reverse):
\begin{center}
\includegraphics[width=10cm]{6}
\end{center}
(In the inverse matrix, the mutation of each row doesn't depend on previous mutations, so the multipliers fall into place in the lower triangular $\bm L$. 
Also see that the final matrix is only lower triangular because the elimination algorithm specifies that we don't manipulate the first row, and that we don't
manipulate the second row with the third row.)\\
\vspace{1mm}\\
This is why we might want to consider $\bm A=\bm{LU}$ to go back from triangular $\bm U$ to the original $\bm A$.
\newpage

\section{Gauss-Jordan elimination}
How would one compute the inverse of an $n$x$n$ matrix $\bm A$? Before answering that question, one might want to consider whether it is really necessary to know
$\bm A^{-1}$; although it is possible to find the solution to $\bm{Ax}=\bm b$ using $\bm x=\bm A^{-1}\bm{b}$, computing $\bm A^{-1}$ and taking $\bm A^{-1}\bm b$ is
a very slow way to find $\bm x$.\\
\vspace{1mm}\\
Say we want to compute $\bm A^{-1}$. This is equivalent to solving for $\bm{AX}=\bm I$. In that sense, by performing the manipulations on $\bm A$ to make it look
like $\bm I$. We essentially replicate the effect of $\bm A^{-1}$ on $\bm A$; by repeating those steps on the identity, 
it becomes as if we were taking $\bm A^{-1}\bm I=\bm A^{-1}$, allowing us to obtain the desired matrix.\\
\vspace{1mm}\\
This whole process can be done with an augmented matrix using \textit{Gauss-Jordan elimination}, where we essentially take steps to reduce $\bm A$ to reduced row
echelon form, while repeating said steps on the identity to obtain the inverse:
\begin{center}
\includegraphics[width=10cm]{7}
\end{center}
The Gauss-Jordan essentially turns $[\bm A,\bm I]$ into $[I,\bm A^{-1}]$, where the elimination steps are essentially equivalent to multiplicaiton by $\bm A^{-1}$.
\newpage

\section{Proving $\bm A=\bm{LU}$}
Elimination is expressed by $\bm{EA}=\bm U$ and inverted by $\bm{LU}=\bm A$. It starts with $\bm A$ and ends with upper triangular $\bm U$, with each elimination step
being carried out by elimination matrices $\bm E_{ij}$. To invert one elimination step we add rows instead of subtracting:
\begin{center}
\includegraphics[width=10cm]{8}
\end{center}
Recall that $\bm E=\bm E_{32}\bm E_{31}\bm E_{21}$ gives us a fairly messy result:
\begin{center}
\includegraphics[width=10cm]{5}
\end{center}
while the inverse, $\bm E^{-1}=\bm E_{21}^{-1}\bm E_{31}^{-1}\bm E_{32}^{-1}=\bm L$ produces a much simpler result
\begin{center}
\includegraphics[width=10cm]{6}
\end{center}
We then have a elegant expression in $\bm A=\bm E^{-1}\bm U=\bm{LU}$. We now show that this equation holds over larger matrices of size $n$.\\
\vspace{1mm}\\
\textbf{Proof 1}\\
Following each step in elimination, consider the pivot rows that are subtracted from lower rows; see that these rows are not original rows of $\bm A$, since
they have been mutated by the previous elimination steps; they are instead rows of $\bm U$.\\
\vspace{1mm}\\
When computing the, say, third row of $\bm U$, we subtract multiples of earlier rows of $\bm U$:
\begin{equation*}
\text{Row 3 of $\bm U$}=(\text{Row 3 of $\bm A$})-\ell_{31}(\text{Row 1 of $\bm U$})-\ell_{32}(\text{Row 2 of $\bm U$})
\end{equation*}
Rewriting, see that
\begin{equation*}
\text{Row 3 of $\bm A$}=\ell_{31}(\text{Row 1 of $\bm U$})+\ell_{32}(\text{Row 2 of $\bm U$})+1(\text{Row 3 of $\bm U$})
\end{equation*}
Row $[\ell_{31},\ell_{32},1]$ is multiplying the matrix $\bm U$. This is \textit{exactly row 3 of $\bm A=\bm{LU}$}. All rows look like this regardless of the size of
$\bm A$. With no row exchanges, we have $\bm A=\bm{LU}$.\\
(next page)\newpage
\noindent\textbf{Proof 2}\\
Here is another proof. The idea here is to see elimination as removing one rank 1 matrix at a time---one column of $\bm L$ times one row of $\bm U$ from $\bm A$; 
where the problem becomes one size smaller with each iteration.\\
\vspace{1mm}\\
Elimination begins with pivot row = row 1 of $\bm A$. We multiply that pivot row by the numbers $\ell_{21}$, then $\ell_{31}$, and eventually $\ell_{n1}$; we subtract
the respective products from row 2, row 3, and eventually row $n$ of $\bm A$.
By choosing $\ell_{21}=a_{21}/a_{11}$, $\ell_{31}=a_{31}/a_{11}$ and so on until
$\ell_{n1}=a_{n1}/a_{11}$; now consider
if we also subtracted away the pivot row away from itself---this subtraction leaves zeros in column 1:
\begin{center}
\includegraphics[width=10cm]{9}
\end{center}
The idea here is that we \textit{removed a rank 1 matrix} with columns made up of multiples of $\bm\ell_1=(1,\ell_{21},\ell_{31},\ell_{41},\ldots)$, 
scaled by
each respective entry of the first row of $\bm A$, which is also the first pivot row $\bm u_1$ of the final upper triangular matrix.\\
\vspace{1mm}\\
We continue with elimination in the second pivot column, using the second row as the pivot row; as before we also subtract the second pivot row from itself. 
See that this is again equivalent to subtracting a rank 1 matrix with basis
$\bm\ell_2=(0,1,\ell_{32},\ell_{42},\ldots)$. Recall that the second pivot row is the second row of the upper triangular matrix $\bm U$ in our factorisation.
\begin{center}
\includegraphics[width=10cm]{10}
\end{center}
Notice that both $\bm\ell_1$ and $\bm\ell_2$ are columns of $\bm L$.
We removed a column $\bm\ell_2$ times the second pivot row. Continuing in the same way, we successively remove columns with each step removing a column
$\bm\ell_j$ of $\bm L$ times a pivot row $\bm u_j$ of $\bm U$. See that this entire process can be depicted as
\begin{center}
\includegraphics[width=10cm]{11}
\end{center}
Notice that $\bm U$ is upper triangular; the pivot row $u_k$ begins with $k-1$ zeros. $\bm L$ is lower triangular with 1's on the main diagonal. Column 
$\bm\ell_k$ also begins with $k-1$ zeros. (See that this proof makes use of the
idea that matrix multiplication can be seen as a sum of rank 1 matrices)
\newpage

\section{Permutation matrices}
\textbf{Examples}\\
Permutation matrices have a 1 in every row and a 1 in every column. All other entries are 0. When this matrix $\bm P$ multiplies a vector, it changes the order of 
its components; for instance:
\begin{center}
\includegraphics[width=10cm]{12}
\end{center}
(A nonzero $i$th entry on row $m$ of $\bm P$ means that row $i$ of $\bm x$ goes on that row $m$ in the result) Other examples include
\begin{center}
\includegraphics[width=10cm]{13}
\end{center}
Recall that elimination may require row exchanges. If $\bm A$ is invertible, then there is a permuation $\bm P$ to order its rows in advance, so that elimination
on $\bm{PA}$ meets no zeros in the pivot positions. Then $\bm{PA}=\bm{LU}$.\\
\vspace{1mm}\\
\textbf{Inverse}\\
We can intuit that the inverse of $\bm P$ is just its transpose:
\begin{center}
\includegraphics[width=10cm]{14}
\end{center}
(For instance, a nonzero entry in row 1 column 2 means `row 2 of $\bm x$ becomes row 1 of result'. Its transpose corresponds to a nonzero entry in 
row 2 column 1, so `row 1 of $\bm x$ becomes row 2 in the result'---reversing the change. This logic can be extrapolated to every other row.)\\
(next page)\newpage
\noindent\textbf{$\bm{PA}=\bm{LU}$ factorisation}\\
Recall that elimination may require row exchanges; consider for instance
\begin{center}
\includegraphics[width=12cm]{15}
\end{center}
To rescue elimination, $\bm P$ exchanced row 2 with 3, bringing 1 to the second pivot so elimination could continue.\\
\vspace{1mm}\\
See that we could order the rows in advance, first exchanging rows 2 and 3 to get $\bm{PA}$, then $\bm{LU}$ factorisation becomes $\bm{PA}=\bm{LU}$; the matrix
$\bm{PA}$ sails through elimination without seeing that zero pivot:
\begin{center}
\includegraphics[width=12cm]{16}
\end{center}
We may require several row exchanges; in that case the an overall permutation $\bm P$ would include them all, still producing $\bm{PA}=\bm{LU}$. A useful way to keep
track of the permutations might be to add a column of indices at the end of $\bm A$ so that the original indices are displayed:
\begin{center}
\includegraphics[width=12cm]{17}
\end{center}
\textbf{Column permutations}\\
We know that we can reorder rows using 
\begin{center}
\includegraphics[width=10cm]{18}
\end{center}
See that we can also reorder columns by applying a permutation matrix on the right:
\begin{center}
\includegraphics[width=8cm]{19}
\end{center}
Note that these two operations don't cover all possible permutations of the 9 entries in $\bm A$. The first index is constant on every row and the second index is
constant on every column.
\newpage

\section{Transposes and symmetric matrices}
\subsection{The transpose of $\bm{AB}$ is $\bm B^T\bm A^T$: Intuition}
We have
\begin{equation*}
(\bm{AB})^T=\bm B^T\bm A^T
\end{equation*}
Consider $\bm{AB}$. Computing the first \textit{row} of the result (the first column of $(\bm{AB})^T$); this can be seen as a linear combination of the 
\textit{rows} of $\bm B$ (the columns of $\bm B^T$) specified by the entries
in the first \textit{row} of $\bm A$ (the first column of $\bm A^T$).
\subsection{Showing $(\bm A^T)^{-1}=(\bm A^{-1})^T$}
Consider taking the transpose of $\bm A^{-1}\bm A$,
\begin{center}
\includegraphics[width=12cm]{20}
\end{center}
Similarly, transposing $\bm{AA}^{-1}=\bm I$ leads to $({\bm A}^{-1})^T\bm A^T=\bm I$. Notice especially that $\bm A^T$ \textit{is invertible exactly when $\bm A$ is 
invertible}.
\subsection{$\bm A^T\bm A$ is symmetric}
Choose any matrix $\bm A$, probably rectangular. Multiply $\bm A^T$ times $\bm A$, then the product $\bm S=\bm A^T\bm A$ is automatically a square symmetric matrix:
\begin{center}
\includegraphics[width=10cm]{21}
\end{center}
The matrix $\bm{AA}^T$ is also symmetric (see that their shapes permit multiplication in either order), note howerver that $\bm{AA}^T$ \textit{is a different matrix} 
from $\bm A^T\bm A$.
\newpage

\chapter{The Four Fundamental Subspaces}

\section{Reduced Row Echelon Form}
\textbf{Motivation}\\
Consider reducing a 2x4 matrix $\bm A$ to its reduced row echelon form $\bm R$:
\begin{center}
\includegraphics[width=12cm]{25}
\end{center}
See how the added upward elimination essentially inverted the a portion $\bm W$ of $\bm A$:
\begin{equation*}
\bm W=\left[\begin{array}{cc}1&2\\3&7\end{array}\right]
\end{equation*}
to turn that part of the matrix into the identity:
\begin{center}
\includegraphics[width=12cm]{26}
\end{center}
We have $\bm H=\bm{WF}$, where $\bm H$ refers to the other parts of $\bm A$, and $\bm W$ the portion that was inverted---we can
combine the components of $\bm W$ (the first independent columns) to produce the other (dependent) columns. The matrix $\bm F$ specifies the parameters to do this:
\begin{center}
\includegraphics[width=10cm]{27}
\end{center}
See that the first $r$ independent columns of $\bm A$ locate the columns of $\bm R$ containing $\bm I$.
Also see that the last $m-r$ rows of $\bm R$ will be rows
of zeros (the dependent columns in the reduced form can be expressed in terms of the identity contained within that form, so there can't be any additional nonzero 
rows).\\
(next page)\newpage
\noindent\textbf{Algorithm}\\
Elimination goes a column at a time from left to right; after $k$ columns, that part of the matrix is in the reduced form, and we move to the $(k+1)$th column;
this new column has an upper part $\bm u$ and a lower part $\bm\ell$:
\begin{center}
\includegraphics[width=10cm]{28}
\end{center}
The idea here is to decide whether this $(k+1)$th column joins $\bm I_k$ or $\bm F_k$.\\
\vspace{1mm}\\
See that if $\bm\ell$ is \textit{all zeros}, the new column is \textit{dependent} on the first $k$ columns; then $\bm u$ joins with $\bm F_k$ to produce 
$\bm F_{k+1}$.\\
\vspace{1mm}\\
If $\bm\ell$ is \textit{not} all zero, then the column is \textit{independent} of the first $k$ columns. Pick any nonzero in $\bm\ell$ as the pivot, move that row of 
$\bm A$ up into row $k+1$, then subtract multiples of that pivot row to zero out all the rest of column $k+1$ (eliminate up and down). If necessary, 
divide the row by its first nonzero entry to have a pivot of 1. Column $k+1$ joins $I_k$ (see that it adds a new nonzero row to the identity) to produce $I_{k+1}$.\\
\vspace{1mm}\\
Repeat for the next column and so on.\\
\vspace{1mm}\\
\textbf{Row operations}\\
There are three row operations allowed in elimination from $\bm A$ to $\bm R$:
\begin{enumerate}
\item Subtract a multiple of one row from another (below or above)
\item Divide a row by its first nonzero entry (to reach pivot 1)
\item Exchange rows (to move all zero rows to the bottom)
\end{enumerate}
A different series of steps could be used reach the same $\bm R$. But the result $\bm R$ can't change.
\newpage

\section{$\bm A=\bm{CR}$ factorisation}
We can apply elimination to reduce $\bm A$ to $\bm R_0$ (reduced echelon form with zero rows); then 
$\bm I$ in $\bm R_0$ locates the matrix $\bm C$ of \textit{independent columns} in $\bm A$. Removing any zero rows from $\bm R_0$ produces 
the row matrix $\bm R$ such that $\bm A=\bm{CR}$. For instance, 
\begin{center}
\includegraphics[width=10cm]{30}
\end{center}
where the independent columns of $\bm A$ are 1 and 3, and
\begin{equation*}
\bm R=\left[\begin{array}{ccc}
1&2&0\\0&0&1
\end{array}\right],\quad
\bm C=\left[\begin{array}{cc}
1&1\\2&5\\3&9
\end{array}\right]
\end{equation*}
so
\begin{center}
\includegraphics[width=12cm]{29}
\end{center}
\newpage

\section{Systematic nullspace computation}
Elimination gives us a systematic way to find a basis for the nullspace. Say we have
\begin{center}
\includegraphics[width=12cm]{31}
\end{center}
Consider attempting to find a basis for the nullspace, meaning a basis for the solutions of $\bm{Ax}=\bm0$; say that $\bm{W^{-1}A}=\bm R_0$, then for some $\bm x$,
\begin{equation*}
\bm{Ax}=\bm0\implies\bm{W^{-1}Ax}=\bm{W^{-1}0}\implies\bm{R_0x}=\bm0
\end{equation*}
See that the elimination process doesn't change the space of solutions that satisfy $\bm{Ax}=\bm0$. Also see that using $\bm R$ instead of $\bm R_0$ only removes the 
redundant zero rows without removing any solutions.\\
\vspace{1mm}\\
Given some $\bm R$, we have a systematic way to find a basis for the nullspace: considering all the indices of $\bm x$ corresponding to dependent indices,
by letting one of those indices be 1 and the rest 0, we have a simple solution for $\bm{Rx}=\bm0$:
\begin{center}
\includegraphics[width=10cm]{32}
\end{center}
See that if we were to write $\bm R$ as $[\bm I,\bm F]$, the solutions can be found in an algorithmic manner:
\begin{center}
\includegraphics[width=12cm]{33}
\end{center}
The second case occurs should we have to permute $\bm R$ to reorder the independent columns. Recall that $\bm{PP}^T=\bm I$, and so
\begin{center}
\includegraphics[width=12cm]{34}
\end{center}
(next page)\newpage
\noindent\textbf{Cont.}\\
To put these two ideas together, suppose the $m$x$n$ matrix $\bm A$ has rank $r$. To find the $n-r$ special solutions to $\bm{Ax}=\bm0$, compute the reduced row
echelon form $\bm R_0$ of $\bm A$; remove the $m-r$ zero rows of $\bm R_0$ to produce $\bm R=[\bm I,\bm F]\bm P$, where $\bm A=\bm{CR}$.\\
\vspace{1mm}\\
Then the special solutions to $\bm{Ax}=0$ are the $n-r$ columns of $\bm P^T\left[\begin{array}{c}-\bm F\\\bm I\end{array}\right]$.
\newpage

\section{The complete solution to $\bm{Ax}=\bm b$}
\textbf{Finding a particular solution}\\
We can reduce $\bm{Ax}=\bm b$ to a simpler system $\bm R_0\bm x=\bm d$ with the same solutions (if any). A useful aid here is to augment 
$\bm A$ with $\bm b$ to produce an augmented matrix $[\bm A,\bm b]$:
\begin{center}
\includegraphics[width=11cm]{35}
\end{center}
Applying elimination to reach $\bm R_0$, the augmented part also undergoes the same elimination steps to produce a $\bm d$:
\begin{center}
\includegraphics[width=11cm]{36}
\end{center}
For an easy solution $x_p$ we can \textit{choose the free variables to be 0}. In this case $x_2=x_4=0$, then the pivot variables can just be read off from $\bm d$:
\begin{center}
\includegraphics[width=12cm]{37}
\end{center}
See that for a solution to exist, the zero rows in $\bm R_0$ must also be zero in $\bm d$.\\
\vspace{1mm}\\
This procedure allows us to come up with a particular solution. For a complete solution, we consider any sum of the particular solution $x_p$ with any nullspace 
vectors $x_n$:
\begin{center}
\includegraphics[width=10cm]{38}\\
\includegraphics[width=11cm]{39}
\end{center}
The nullspace can be found as per outlined earlier (setting one variable to one and the rest to zero to get each nullspace basis vector). See 
that the complete solution includes all nullspace basis vectors, and 
that if the nullspace is just the zero vector then every particular solution $\bm x_p$ is the only solution to that $\bm b$.\\
(next page)\newpage
\subsection{Rank and solution}
\textbf{Full column rank}\\
In the case where $\bm A$ has \textit{full column rank}, every column has a pivot and the rank is $r=n$. The matrix is tall and thin ($m\geq n$). 
Row reduction puts $R=I$ at the top when $\bm A$ is reduced to $\bm R_0$ with rank $n$:
\begin{center}
\includegraphics[width=10cm]{40}\\
\end{center}
See that in this case the matrix has the following properties:
\begin{center}
\includegraphics[width=11cm]{41}\\
\end{center}
With full column rank, $\bm{Ax}=\bm b$ will have \textit{one solution or no solution}. (think of a mapping from $n$ dimensional space into $m$ dimensional space
where $m\geq n$, there aren't enough vectors in $n$ space to encompass all of $m$ space, so there may be no solution for a particular vector in $m$ space; or see 
that the columns only span a $n$ dimensional hyperplane in a higher $m$ dimensional space---any $m$ dimensional vector not on that hyperplane has no solution)\\
\vspace{1mm}\\
\textbf{Full row rank}\\
The other case is full row rank---now $\bm{Ax}=\bm b$ has \textit{one or infinitely many solutions}. In this case $\bm A$ must be \textit{short and wide} ($m\leq n$);
the matrix has full row rank if $r=m$, where every row has a pivot.\\
\vspace{1mm}\\
(If $m<n$, then a nullspace consisting of more than just the zero vector exists and there will be multiple solutions to $\bm{Ax}=\bm b$.) See that a matrix like
this has the following properties:
\begin{center}
\includegraphics[width=10cm]{42}\\
\end{center}
(next page)\newpage
\noindent\textbf{Cont.}
\begin{center}
\includegraphics[width=12cm]{43}
\end{center}
If the matrix $\bm A$ is square and invertible, it indicates a mapping from $m=n=r$ space into that same space; the basis specified by $\bm A$ spans the whole of 
$r$ space
and there is only one linear combination of that basis that solves for $\bm{Ax}=\bm b$ (since $\bm A$ has linearly independent columns and therefore a trivial nullspace).\\
\vspace{1mm}\\
If the matrix is not full rank, then the columns span a hyperplane in the space equal to the row length, meaning some $\bm{Ax}=\bm b$ may not have a solution, and if
some $\bm b$ does lie within that hyperplane there will be infinite solutions to it (since the columns are not independent and therefore a nontrivial nullspace exists).\\
\vspace{1mm}\\
Their reduced row echelon form will fall into the following categories:
\begin{center}
\includegraphics[width=12cm]{44}
\end{center}
Cases 1 and 2 have full row rank, case 3 has full column rank but not row rank, case 4 is not full rank.
See that in the last two cases, when $\bm{Ax}=\bm b$ is reduced to $\bm R_0\bm x=\bm d$, $\bm d$ must end in $m-r$ zeros for the equation to be solvable. $\bm F$ refers
to the top part of the dependent columns.
\newpage

\section{Intuition for four subspaces}
We have
\begin{center}
\includegraphics[width=10cm]{45}
\end{center}
\textbf{Column space of reduced form not the same as original matrix}\\
Note that although the reduced row echelon form provides the pivot positions to determine the row and column rank of a matrix, while the \textit{rows} of the rref form 
span the same row space as the original unreduced matrix, the columns of the rref form cannot act as substitutes for the original column space, instead only providing
information on dimensionality and on which original columns act as basis vectors for the column space. That is 
\begin{equation*}
\bm C(\bm A)\neq\bm C(\bm R_0)\quad\text{but}\quad\bm C(\bm A^T)=\bm C(\bm R_0^T)
\end{equation*}
(none of the row operations in elimination change the row space, but they do change the column space)\\
\vspace{1mm}\\
\textbf{Nullspace dimension is $n-r$}\\
The intuition for the dimension of $\bm N(A)$ being $n-r$ is clear from elimination, where each dependent column provides a new basis vector for the nullspace (we can
find a basis by substituting one free variable as 1 and the rest zero, repeating for all free variables), so the number of basis vectors equals the number of
dependent columns equals total columns minus independent columns---so $n-r$.
\vspace{1mm}\\
\textbf{Left nullspace dimensionality}\\
We know the dimensions for $\bm A$ is the same as that of $\bm A^T$. Transposing $\bm A$ gives us $m$ columns, of which we know $r$ are independent. So there are $m-r$ 
dependent columns which each provide a basis vector for the nullspace.
\newpage

\section{$\bm A=\bm{CR}$ and Block elimination}
The earlier notes on the rref form introduced it as a simple way to express the entire matrix in terms of its dependent rows, where for a matrix $\bm A$, with independent
components $\bm W$ and dependent components $\bm H$, reduction to rref looks like:
\begin{equation*}
\bm A=\left[\begin{array}{cc}W&H\end{array}\right]\implies
\bm W^{-1}\bm A=\bm W^{-1}\left[\begin{array}{cc}W&H\end{array}\right]=
\left[\begin{array}{cc}I&W^{-1}H\end{array}\right]
\end{equation*}
See that we can recover the dependent components of $\bm H$ with $\bm W^{-1}\bm H$ by multiplying it by the independent columns $\bm W$, so
\begin{equation*}
\bm A=\underbrace{\bm W}_{\bm C}\underbrace{\left[\begin{array}{cc}I&W^{-1}H\end{array}\right]}_{\bm R}
\end{equation*}
where $\bm C$ means the same thing as $\bm W$, that is, the independent columns of $\bm A$. See that
$\bm W$ is gauranteed to be square since row and column rank are equal.\\
\vspace{1mm}\\
\textbf{Block elimination}\\
The above doesn't show how the zero rows in $\bm R_0$ (if they exist) are redundant. 
We can develop a better idea of elimination if we consider working with blocks of the original $\bm A$. This is called block elimination; consider reducing
a matrix $\bm A$ with rank $r$ to rref using the following three steps:\\
\vspace{1mm}\\
\textbf{Step 1}: Exchange columns of $\bm A$ using $\bm P_C$ and exchange rows of $\bm A$ using $\bm P_R$ to put the $r$ independent columns first and $r$ independent rows 
first. This looks like
\begin{equation*}
\bm P_R\bm A\bm P_C=\left[\begin{array}{cc}
\bm W&\bm H\\
\bm J&\bm K\end{array}\right]
\end{equation*}
$\left[\begin{array}{c}\bm W\\\bm J\end{array}\right]$ has full column rank and 
$\left[\begin{array}{cc}\bm W&\bm H\end{array}\right]$ has full row rank. See that $\bm W$ is a square matrix with full rank $r$ and is therefore invertible.\\
\vspace{1mm}\\
\textbf{Step 2}: Multiply the top rows by $\bm W^{-1}$:
\begin{equation*}
\left[\begin{array}{cc}
\bm W&\bm H\\
\bm J&\bm K\end{array}\right]\to
\left[\begin{array}{cc}
\bm I&\bm W^{-1}\bm H\\
\bm J&\bm K\end{array}\right]
\end{equation*}
\textbf{Step 3}: Subtract $\bm J[\begin{array}{cc}\bm I&\bm W^{-1}\bm H\end{array}]$
from the $m-r$ rows $[\begin{array}{cc}\bm J&\bm K\end{array}]$ to produce
$[\begin{array}{cc}\bm0&\bm0\end{array}]$:
\begin{equation*}
\left[\begin{array}{cc}
\bm I&\bm W^{-1}\bm H\\
\bm J&\bm K\end{array}\right]\to
\left[\begin{array}{cc}
\bm I&\bm W^{-1}\bm H\\
\bm 0&\bm 0\end{array}\right]
\end{equation*}
To understand why $\bm J\bm W^{-1}\bm H=\bm K$, we know that the first $r$ rows 
$[\begin{array}{cc}\bm I&\bm W^{-1}\bm H\end{array}]$ are linearly independent, and since $\bm A$ has rank $r$, the lower rows $[\begin{array}{cc}\bm J&\bm K\end{array}]$ 
must be combinations of the upper rows. These combinations must be given by $\bm J$ to get
the first $r$ columns correct: $\bm{JI}=\bm J$. Then $\bm J$ times $\bm W^{-1}\bm H$ must equal $\bm K$ to make the last columns correct.\\
(next page)\newpage
\noindent\textbf{Cont.}\\
We have block elimination as the following transformations:
\begin{center}
\includegraphics[width=12cm]{47}
\end{center}
We can, in a manner similar to the steps taken, factor out parts of $\bm A$ as follows to acquire the reduced row echelon form; for step 2:
\begin{equation*}
\bm P_R\bm A\bm P_C=\left[\begin{array}{cc}
\bm W&\bm H\\
\bm J&\bm K\end{array}\right]=
\left[\begin{array}{cc}
\bm W&\bm 0\\
\bm 0&\bm 1\end{array}\right]
\left[\begin{array}{cc}
\bm I&\bm W^{-1}\bm H\\
\bm J&\bm K\end{array}\right]
\end{equation*}
For step 3:
\begin{equation*}
\left[\begin{array}{cc}
\bm W&\bm 0\\
\bm 0&\bm 1\end{array}\right]
\left[\begin{array}{cc}
\bm I&\bm W^{-1}\bm H\\
\bm J&\bm K\end{array}\right]=
\left[\begin{array}{cc}
\bm W&\bm 0\\
\bm 0&\bm 1\end{array}\right]
\left[\begin{array}{cc}
\bm 1&\bm 0\\
\bm J&\bm 0\end{array}\right]
\left[\begin{array}{cc}
\bm I&\bm W^{-1}\bm H\\
\bm 0&\bm 0\end{array}\right]
\end{equation*}
The result is
\begin{equation*}
\bm P_R\bm A\bm P_C=
\left[\begin{array}{cc}
\bm W&\bm 0\\
\bm J&\bm 0\end{array}\right]
\left[\begin{array}{cc}
\bm I&\bm W^{-1}\bm H\\
\bm 0&\bm 0\end{array}\right]=\left[\begin{array}{c}
\bm W\\
\bm J\end{array}\right]
\left[\begin{array}{cc}
\bm I&\bm W^{-1}\bm H\end{array}\right]
\end{equation*}
See that this is just the $\bm A=\bm{CR}$ factorisation, but this time showing that the zero rows are redundant rather than just using a reduced form where zero rows don't
exist. Another way of writing this result is
\begin{center}
\includegraphics[width=12cm]{48}
\end{center}
\newpage

\chapter{Orthogonality}

\section{Orthogonal subspaces}
Recall that two vectors $\bm v$ and $\bm w$ are orthogonal if $\bm v^T\bm w=0$. To say that two subspaces are orthogonal means that every vector in one space
is orthogonal to every vector in the other space.\\
\vspace{1mm}\\
We can make the claim that for any matrix $\bm A$, \textit{the nullspace of $\bm A$ is orthogonal to the row space of $\bm A$.} To see this just consider $\bm{Ax}=\bm0$:
\begin{center}
\includegraphics[width=10cm]{49}
\end{center}
Every row has a zero dot product with $\bm x$. Then every combination of the rows is perpendicular to $\bm x$. That is, the whole row space $\bm C(\bm A^T)$ is orthogonal to 
the nullspace $\bm N(\bm A)$ in $n$ dimensional space (since $\bm A^T$ has $n$ rows). Another way to show this is
\begin{center}
\includegraphics[width=10cm]{50}
\end{center}
See that we can analagously reason that the \textit{the column space $\bm C(\bm A)$ and the left nullspace $\bm N(\bm A^T)$ are perpendicular inside $m$ dimensional space}.\\
(next page)\newpage
\noindent\textbf{Cont.}\\
See that the only common vector between two orthogonal subspaces would be the zero vector (we can't sum the columns of a matrix to produce a vector orthogonal
to all the columns). There is a very important restriction on the dimension of any
two orthogonal subspaces:
\begin{center}
\includegraphics[width=10cm]{51}
\end{center}
For instance, the row space of a matrix is a subspace of dimension $r$ in $n$ space, and its orthogonal complement, by construction, will have dimensionality $n-r$.
\subsection{Orthogonal complements}
When two orthogonal subspaces account for the whole space, they are called \textit{orthogonal complements}. The orthogonal complement of $\bm V$ is written as $\bm V^\perp$.
The row space and the nullspace, as well as the column space and left nullspace, are
instances of orthogonal complements:
\begin{center}
\includegraphics[width=10cm]{52}
\end{center}
\subsection{Intuition for $\bm x=\bm x_\text{row}+\bm x_\text{null}$}
See that because the row space and the nullspace combined span the entire $n$ space, we can say that any vector in $\mathbb{R}^n$ can be expressed as a sum of vectors from 
both subspaces. That is, any vector $\bm x$ in $\mathbb{R}^n$ is the sum $\bm x=\bm x_\text{row}+\bm x_\text{null}$ of its row space component and its nullspace component.
Any vector $\bm y$ in $\mathbb{R}^m$ is the sum $\bm y=\bm y_\text{col}+\bm y_\text{null}$ of its column space component and its left nullspace component from 
$\bm N(\bm A^T)$:
\begin{center}
\includegraphics[width=9.8cm]{53}
\end{center}
(next page)\newpage
\subsection{$\bm A$ is invertible from the row space to the column space}
Consider again the illustration from the previous page:
\begin{center}
\includegraphics[width=12cm]{53}
\end{center}
Every vector in $\bm{Ax}$ is in the column space; that is obvious. Every $\bm x$ comes from $\mathbb{R}^n$ space, which is made up of the the row space $\bm C(\bm A^T)$ and
its orthogonal complement, the nullspace $\bm N(\bm A)$. In that context, every candidate $\bm x$ either comes from the nullspace, which would map to $\bm0$, or
the row space, which would then map to some $\bm b$ in the column space---every $\bm b$ in the column space can be found by some $\bm{Ax}$ with $\bm x$ from the 
row space.\\
\vspace{1mm}\\
We can also go on to say that \textit{every vector $\bm b$ in the column space comes from exactly one vector $\bm x_r$ in the row space}. Proof: If $\bm{Ax}_r=\bm{Ax}_{r}'$,
the difference $\bm{x}_r-\bm{x}_{r}'$ is in the nullspace (since 
$\bm A(\bm{x}_r-\bm{x}_{r}')=\bm 0$); however, should $\bm{x}_r$ and $\bm{x}_{r}'$ come from the row space, then $\bm{x}_r-\bm{x}_{r}'$ would also have to be in the row
space. The only way for a vector to be in both the nullspace and the row space would be if it were the zero vector; so $\bm{x}_r-\bm{x}_{r}'=\bm 0$ and 
$\bm{x}_r=\bm{x}_{r}'$.\\
\vspace{1mm}\\
So $\bm A$ maps every row space component to a specific column space component; see that there is an $r$ by $r$ invertible matrix hiding inside $\bm A$ if we throw away the 
two nullspaces. $\bm A$ \textit{is invertible from the row space to the column space}.
\newpage

\section{$\bm A^T\bm A$ is invertible only if $\bm A$ has linearly\\ independent columns}
$\bm A^T\bm A$ is a square matrix ($n$ by $n$). We prove the above by showing that $\bm A^T\bm A$ \textit{has the same nullspace as} $\bm A$; so that when the columns of 
$\bm A$ are linearly independent, and its nullspace only contains the zero vector, then $\bm A^T\bm A$, with this same nullspace, is also invertible.\\
\vspace{1mm}\\
First we show that a vector in the nullspace of $\bm A$ is also in that of $\bm A^T\bm A$. Let $\bm A$ be any matrix; if $\bm x$ is in the nullspace, then $\bm{Ax}=\bm0$.
Multiplying by $\bm A^T$ gives $\bm A^T\bm A\bm x=\bm0$, so $\bm x$ is also in the nullspace of $\bm A^T\bm A$.\\
\vspace{1mm}\\
Now the other way, we show that a vector in the nullspace of $\bm A^T\bm A$ is in that of $\bm A$. Starting with $\bm A^T\bm A\bm x=\bm 0$, multiplying by $\bm x^T$:
\begin{center}
\includegraphics[width=9cm]{60}
\end{center}
So if $\bm A^T\bm A\bm x=\bm 0$ then $\bm{Ax}$ has length 0, so $\bm{Ax}=\bm0$.\\
\vspace{1mm}\\
Every vector $\bm x$ in one nullspace is in the other nullspace, so if $\bm A^T\bm A$ has dependent columns then so does $\bm A$, and if $\bm A^T\bm A$ has independent 
columns then so does $\bm A$:
\begin{center}
\includegraphics[width=12cm]{61}
\end{center}
\newpage

\section{Projections}
\subsection{Projection onto a line}
We want to project any $\bm b$ onto the column space of any $m$ by $n$ matrix $\bm A$. This entails finding the point $\bm p$ on that subspace that is closest to our
arbitrary point $\bm b$. We start with a line.\\
\vspace{1mm}\\
Considering a one dimensional subspace (a line) with basis $\bm a=(a_1,\ldots,a_m)$. 
Along that line, we want the point $\bm p$ closest to $\bm b=(b_1,\ldots,b_m)$; the key to projection is orthogonality: \textit{the line from $\bm b$ to $\bm p$, 
that is $\bm b-\bm p$, is perpendicular to the vector $\bm a$}.\\
\vspace{1mm}\\
We know that the projection $\bm p$ will be some multiple of $\bm a$; call it $\bm p=\hat{x}\bm a$ (so here $\hat{x}$ denotes a constant)---computing this number 
$\hat{\bm x}$ will give us the vector $\bm p$. We can call the perpendicular line $\bm b-\bm p$ the `error' $\bm e=\bm b-\hat{x}\bm a$, and use the fact that it is 
perpendicular to $\bm a$ to derive $\hat{x}$:
\begin{center}
\includegraphics[width=6cm]{54}\\
\includegraphics[width=11cm]{55}
\end{center}
With that we can compute the projection $\bm p=\hat{x}\bm a$:
\begin{center}
\includegraphics[width=12cm]{56}
\end{center}
(next page)\newpage
\noindent\textbf{Projection matrix}\\
We had the projection of $\bm b$ onto the line through $\bm a$ as the vector $\bm p$ given by
\begin{equation*}
\bm p=\hat{x}\bm a=\frac{\bm a^T\bm b}{\bm a^T\bm a}\bm a
\end{equation*}
Now comes the \textit{projection matrix}, where we want a matrix $\bm P$ that can be applied to the arbitrary point $\bm b$ to get the desired projection $\bm p$, so 
$\bm{Pb}=\bm p$. This can found easily; since $\hat{x}$ is a constant, we can rearrange the above equation:
\begin{center}
\includegraphics[width=12cm]{57}
\end{center}
The projection matrix $\bm P$ is a $m$ by $m$ rank one matrix. Notice that the line through $\bm a$, the subspace we are projecting onto, is the column space of $\bm P$.
\subsection{Projection onto a subspace}
Now we project onto a $n$-dimensional subspace of $\mathbb{R}^m$. We start with $n$ vectors $\bm a_1,\ldots,\bm a_n$ in $\mathbb{R}^m$, 
\textit{assume that they are linearly independent}.\\
\vspace{1mm}\\
We want to find a projection of $\bm b$ onto the subspace spanned by these vectors, that is, 
\textit{the combination $\bm p=\hat{x}_1\bm a_1+\cdots+\hat{x}_n\bm a_n$ closest to a given vector $\bm b$}. See that this can be written as $\bm A\hat{\bm x}$, where
$\hat{\bm x}=(\hat{x}_1,\ldots,\hat{x}_n)$.\\
\vspace{1mm}\\
As before, the idea here is that the error vector $\bm e=\bm b-\bm p=\bm b-\bm A\hat{\bm x}$ is perpendicular to the subspace we are projecting onto. Every basis vector is
perpendicular to this error vector, so
\begin{center}
\includegraphics[width=12cm]{58}
\end{center}
We have
\begin{equation*}
\bm A^T(\bm b-\bm A\hat{\bm x})=\bm 0\implies \bm A^T\bm A\hat{\bm x}=\bm A^T\bm b
\end{equation*}
(next page)\newpage
\noindent\textbf{Cont.}\\
We had the projection of $\bm b$ onto the column space of $\bm A$ as $\bm p=\bm A\hat{\bm x}$. We showed that 
\begin{equation*}
\bm A^T(\bm b-\bm A\hat{\bm x})=\bm 0\implies \bm A^T\bm A\hat{\bm x}=\bm A^T\bm b
\end{equation*}
With this we can derive the projection matrix:
\begin{center}
\includegraphics[width=12cm]{59}
\end{center}
Again see that this only works if $\bm A^T\bm A$ is invertible, which is only when the column vectors of $\bm A$ are independent. See that the final projection 
matrix formula is analagous to the formula derived in the one dimensional case.\\
\vspace{1mm}\\
The subspace we are projecting onto is the the column space of $\bm A$, that is, $\bm C(\bm A)$. See that the error $\bm e=\bm b-\bm A\hat{\bm x}$ \textit{belongs to the 
perpendicular subspace} $\bm N(\bm A^T)$ (since $\bm A^T(\bm b-\bm A\hat{\bm x})=\bm 0$).\\
\vspace{1mm}\\
\textbf{A note on the projection matrix}\\
The matrix $\bm P=\bm A(\bm A^T\bm A)^{-1}\bm A^T$ is deceptive. One might try to split $(\bm A^T\bm A)^{-1}$ into $\bm A^{-1}$ times $(\bm A^T)^{-1}$ and find that the
entire formula reduces to $\bm I$.\\
\vspace{1mm}\\
This is wrong because \textit{the matrix $\bm A$ is rectangular and therefore has no inverse}. We cannot split $(\bm A^T\bm A)^{-1}$ into $\bm A^{-1}$ times $(\bm A^T)^{-1}$
because there is no $\bm A^{-1}$ in the first place.
\begin{center}
\includegraphics[width=11.2cm]{62}
\end{center}
\newpage

\section{Projection matrices are symmetrical}
Given a projection matrix $\bm P$, we can decompose any vectors $\bm v$ and $\bm w$ into 
\begin{align*}
\bm v&=\bm v_p+\bm v_n\\
\bm w&=\bm w_p+\bm w_n
\end{align*}
With the $p$ subscript indicating the component of the vector in the subspace of $\bm P$, and the $n$ subscript the component outside (normal to) the subspace of $\bm P$.
(the sum of the projection and its orthogonal `error' make up the original vector before projecion)\\
\vspace{1mm}\\
The projection of a vector lies in a subspace. The dot produc of anything in this subspace with anything orthogonal to this subspace is zero. So see that
\begin{align*}
(\bm{Pv})\cdot\bm w&=\bm v_p\cdot\bm w\\
&=\bm v_p\cdot(\bm w_p+\bm w_n)\\
&=\bm v_p\cdot\bm w_p+\bm v_p\cdot\bm w_n\\
&=\bm v_p\cdot\bm w_p
\end{align*}
and similarly
\begin{align*}
\bm v\cdot(\bm{Pw})&=\bm v\cdot\bm w_p\\
&=(\bm v_p+\bm v_n)\cdot\bm w_p\\
&=\bm v_p\cdot\bm w_p+\bm v_n\cdot\bm w_p\\
&=\bm v_p\cdot\bm w_p
\end{align*}
As such we can conclude that
\begin{equation*}
(\bm{Pv})\cdot\bm w=\bm v\cdot(\bm{Pw})
\end{equation*}
and using the definition of the dot product:
\begin{align*}
(\bm{Pv})^T\bm w&=\bm v^T\cdot(\bm{Pv})\\
\bm v^T\bm P^T\bm w&=\bm v^T\bm P\bm w\\
\end{align*}
so
\begin{equation*}
\bm P^T=\bm P
\end{equation*}
\newpage

\section{Least Squares Approximations}
\textbf{Illustrative example}\\
Consider trying to find the closest line to three points $(0,6),(1,0),(2,0)$. No straight line $\bm b=C+D\bm t$ goes through those three points; we want two numbers $C$ and
$D$ that satisfy a system of three equations (this leads us to a 3 by 2 matrix):
\begin{center}
\includegraphics[width=10cm]{63}
\end{center}
(This is essentially sampling a straight line at the three time points and comparing it to the desired data points at those times)
This 3 by 2 system has \textit{no solution}: $\bm b=(6,0,0)$ is not a combination of the columns $(1,1,1)$ and $(0,1,2)$ (of the matrix $\bm A$):
\begin{center}
\includegraphics[width=9cm]{64}
\end{center}
The idea here is to choose a vector of parameters $\bm x$ that makes $\bm e=\bm b-\bm{Ax}$ as small as possible---so finding a point in the column space of $\bm A$ that is
\textit{closest to $\bm b$}---see that this is just the projection. So the best $\bm x$, which we denote as $\hat{\bm x}$, is found by
$\bm A^T\bm A\hat{\bm x}=\bm{A}^T\bm b$, where the projection $\bm p$ of $\bm b$ onto the column space of $\bm A$ is denoted by $\bm p=\bm A\hat{\bm x}$:
\begin{center}
\includegraphics[width=12cm]{65}
\end{center}
See that the solution to $\bm A\hat{\bm x}=\bm p$ results in the least possible squared error:
\begin{center}
\includegraphics[width=10cm]{66}
\end{center}
Since any $\bm x$ other than $\hat{\bm x}$ would lead to $||\bm{Ax}-\bm p||^2>0$. (This expression comes from the pythagorean theorem, where the vector from some
point $\bm{Ax}$ to point $\bm b$, that is $\bm{Ax}-\bm b$, whose length we want to minimise, is the 
hypotenuse, while the the error $\bm e=\bm b-\bm p$, perpendicular to 
the vector $\bm{Ax}-\bm p$, are the opposite and adjacent sides respectively)\\
\vspace{1mm}\\
By choosing $\bm x=\hat{\bm x}$, we leave the smallest possible error $\bm e=(e_1,e_2,e_3)$ that we can't reduce, while reducing $\bm{Ax}-\bm p$ to zero. 
There is some ambiguity in the idea of `smallest', but here it means that we minimise the \textit{squared length of $\bm{Ax}-\bm b$}:
\begin{center}
\includegraphics[width=11cm]{67}
\end{center}
(next page)\newpage
\noindent\textbf{Cont.}\\
By taking $\bm x=\hat{\bm x}$, where $\bm A\hat{\bm x}$ corresponds to the projection of $\bm b$ onto the column space of $\bm A$, we have the $\bm x$ that minimises the 
squared error $||\bm{Ax}-\bm b||^2$:
\begin{center}
\includegraphics[width=11.5cm]{68}
\end{center}
Projection allows us to minimise squared error.\\
\vspace{1mm}\\
\textbf{Intuition from calculus}\\
Consider minimising the squared error by calculus. The total squared error of the system, with parameters $C$ and $D$, is
\begin{center}
\includegraphics[width=10cm]{69}
\end{center}
We take the partial derivative with respect to each parameter and set them to 0:
\begin{center}
\includegraphics[width=10cm]{70}
\end{center}
Simplifying leaves us with the system
\begin{center}
\includegraphics[width=10cm]{71}
\end{center}
This system is exactly $\bm A^T\bm A\hat{\bm x}=\bm A^T\bm b$, where optimisation leads us to the exact same system we derived by projection:
\begin{center}
\includegraphics[width=10cm]{72}
\end{center}
(next page)\newpage
\noindent\textbf{The big picture for least squares}\\
The intuition for the four subspaces can be applied here. Given some $\bm b$ that isn't in the column space of $\bm A$, we can split it into the sum of its projection $\bm p$
in the column space and the minimum error $\bm e$ in the orthogonal subspace; that is, we have $\bm b=\bm p+\bm e$:
\begin{center}
\includegraphics[width=12cm]{73}
\end{center}
As discussed earlier, see that the row space has a solution for any vector in the column space of $\bm A$, meaning that it contains $\hat{\bm x}$. Also notice that since
we construct $\bm A$ as having full column rank, its nullspace will just be the zero vector.\\
(next page)\newpage
\subsection{Fitting a straight line}
Fitting a line is the clearest application of least squares. It starts with $m>2$ points $b_1,\ldots,b_m$, at times $t_1,\ldots,t_m$.
The best fit line $C+D\bm t$ misses the point by vertical distances $e_1,\ldots,e_m$. The least squares line minimises $E=e_1^2+\cdots+e_m^2$ 
(note that this is not the minimum error as in the previous pages).\\
\vspace{1mm}\\
A line goes through all $m$ points when we can exactly solve $\bm{Ax}=\bm b$; generally this isn't possible so we want a best fit. 
Two unknowns $C$ and $D$ determine a line, so $\bm A$ only has $n=2$ columns. To fit $m$ points, we are trying to solve $m$ equations:
\begin{center}
\includegraphics[width=9cm]{74}
\end{center}
The column space is so thin that $\bm b$ is almost certainly outside of it (if $\bm b$ happens to lie in the column space then $\bm b=\bm p$ and the errors are 
$\bm e=\bm 0$). The parameters $\hat{\bm x}$ for the line that minimises the squared error $||\bm{Ax}-\bm b||^2$ is found by the projection:
\begin{center}
\includegraphics[width=11cm]{75}
\end{center}
See that the matrix $\bm A^T\bm A$ looks like
\begin{center}
\includegraphics[width=11cm]{76}
\end{center}
and $\bm A^T\bm b$,
\begin{center}
\includegraphics[width=6cm]{77}
\end{center}
The line $C+D\bm{t}$ minimises the error $\bm E(x)=e_1^2+\cdots+e_m^2=||\bm{Ax}-\bm b||^2$ when $\bm A^T\bm A\hat{\bm x}=\bm A^T\bm b$:
\begin{center}
\includegraphics[width=7.5cm]{78}
\includegraphics[width=10cm]{79}
\end{center}
\newpage

\section{Orthonormal Bases}
The vectors $\bm q_1,\ldots,\bm q_n$ are \textit{orthogonal} when their dot products $\bm q_1\cdots \bm q_j$ are zero---$\bm q_i^T\bm q_j=0$ whenever $i\neq j$. By \textit{
dividing each vector by its length}, the vectors become orthogonal \textit{unit vectors}---their lengths are all 1---in this case the basis is called \textit{orthonormal}:
\begin{center}
\includegraphics[width=12cm]{80}
\end{center}
The matrix $\bm Q$ is easy to work with because $\bm Q^T\bm Q=\bm I$ (since each column is orthonormal):
\begin{center}
\includegraphics[width=10cm]{81}
\end{center}
On the diagonal the unit vectors give $\bm q_i^T\bm q_i=||\bm q_i||^2=1$.
Note an important implication:
\begin{center}
\includegraphics[width=11cm]{82}
\end{center}
Note that generally if $m\neq n$ then $\bm{QQ}^T\neq\bm I$; although $\bm Q^T\bm Q=\bm I$ even when
$\bm Q$ is rectangular, $\bm Q^T$ here is only a 1-sided inverse from the left.
Only for square matrices do we also have $\bm{QQ}^T=\bm I$ (from the definition of the inverse of square matrices as covered earlier).\\
\vspace{1mm}\\
(The matrix must have full column rank since if its columns were to be orthonormal; 
so we have $m\geq n$. A mapping by $\bm Q$ in the case where $m>n$ is invertible since it has full column rank, but the transpose (size $n$x$m$) doesn't have full column 
rank and therefore a specific mapping $\bm Q^T\bm x=\bm b$ would have infinite solutions---a $\bm x$ cannot be exactly recovered given a $\bm Q^T$ and $\bm b$. As such only 
applying $\bm Q$ first works.)\\
\vspace{1mm}\\
If we have a \textit{square} $\bm Q$ with orthonormal columns, its transpose is its inverse from both sides; and as such also has orthonormal columns---both the rows 
and columns of $\bm Q$ are orthonormal. A square matrix with orthonormal columns is called an \textit{orthogonal matrix}.\\
\vspace{1mm}\\
Also see that if the columns are only orthogonal and not orthonormal, their dot products still give a diagonal matrix.\\
(next page)\newpage
\subsection{Multiplication by any orthogonal matrix preserves length and angle}
Length is preserved during multiplication with an orthogonal matrix:
\begin{center}
\includegraphics[width=12cm]{83}
\end{center}
The dot product, and therefore the angle, is also preserved:
\begin{center}
\includegraphics[width=11cm]{84}
\end{center}
\subsection{Projections using orthonormal bases}
Using orthonormal bases for projection greatly simplifies computation. For 
projections onto subspaces, all formulas involve $\bm A^T\bm A$; suppose the basis vectors are orthonormal, then $\bm A^T\bm A$ simplifies to $\bm Q^T\bm Q=\bm I$. Our 
projection becomes
\begin{center}
\includegraphics[width=12cm]{85}
\end{center}
See that no inversion is required---this is the point of an orthonormal basis---the best $\hat{\bm x}=\bm Q^T\bm b$ is just dot products of $\bm q_1,\ldots,\bm q_n$ with 
$\bm b$:
\begin{center}
\includegraphics[width=12cm]{86}
\end{center}
Also see that when $\bm Q$ is square (so an orthonormal basis spanning the entire space), $\hat{\bm x}=\bm Q^T\bm b$ leads to $\bm p=\bm Q\hat{\bm x}=\bm Q\bm Q^T\bm b=
\bm I\bm b=\bm b$. The projection of $\bm b$ onto the basis that spans the entire space
is $\bm b$ itself. (See that $\bm{QQ}^T=\bm I$ only works for square matrices)
\newpage

\section{Gram-Schmidt Process}
Given three \textit{independent vectors} $a,b,c$, we intend to construct three \textit{orthogonal} vectors $\bm A,\bm B,\bm C$. Then, by dividing $\bm A,\bm B,\bm C$ by
their lengths, we can produce three orthonormal vectors $\bm q_1=\bm A/||\bm A||,\,\bm q_2=\bm B/||\bm B||,\,\bm q_3=\bm C/||\bm C||$.\\
\vspace{1mm}\\
Begin by choosing $\bm A=\bm a$ (we make all other vectors orthogonal to this). The next vector $\bm B$ must be perpendicular to $\bm A$; we \textit{start with $\bm b$ and
subtract its projection along $\bm A$}. This leaves the perpendicular part, which
is the orthogonal vector $\bm B$:
\begin{center}
\includegraphics[width=9cm]{87}
\end{center}
We use the one dimensional projection  of $\bm b$ onto $\bm a$ to produce a vector orthogonal to $\bm a$; then we use it as our next basis vector.\\
\vspace{1mm}\\
The third direction starts with $\bm c$. We can say that $\bm c$ is not a comination of $\bm A$ and $\bm B$ because $\bm c$ is not a combination of $\bm a$ and $\bm b$
(see that $B$ is just a combination of $\bm b$ and $\bm A=\bm a$ since $\hat{\bm x}$ in the one dimensional case is just a constant).
It is also unlikely that $\bm c$ is already perpendicular to $\bm A$ and $\bm B$; the idea here is that we subtract off its components in those two directions to get a
perpendicular direction $\bm C$:
\begin{center}
\includegraphics[width=11cm]{88}
\end{center}
This is the entire idea behind the Gram-Schmidt process; \textit{Subtract from every vector its projections in the directions already set}, repeating for each new basis 
vector. At the end, or immediately when each one is found, dividing the orthogonal vectors $\bm{A,B,C,D}$ by their lengths leaves us with \textit{orthonormal} results 
$\bm q_i$
\begin{center}
\includegraphics[width=11.5cm]{89}
\end{center}
\newpage

\section{$\bm A=\bm{QR}$ factorisation}
Using the Gram-Schmidt process we can derive an orthonormal matrix $\bm Q$ with columns $\bm q_1,\bm q_2,$ and $\bm q_3$ from a matrix $\bm A$ with columns 
$\bm a$, $\bm b$, and $\bm c$. The vectors $\bm q_1,\bm q_2,\bm q_3$ come from linear combinations of $\bm a,\bm b,\bm c$ and vice versa. As such there must be a third matrix
that maps $\bm Q$ to $\bm A$.\\ 
\vspace{1mm}\\
Matrix $\bm A$ has independent columns, and so any mapping using its column space has a one to one mapping; a mapping to $\bm Q$ in the same 
dimension suggests a \textit{square invertible matrix} applied to the right of $\bm A$, lets call it $\bm R^{-1}$; so $\bm{AR}^{-1}=\bm Q$ (the linear combinations of the 
columns of $\bm A$ to produce $\bm Q$ must themselves be linearly independent). This square
invertible matrix has an inverse $\bm R$, which when applied to the right of this equation, gives us
\begin{equation*}
\bm{AR}^{-1}\bm R=\bm A=\bm Q\bm R
\end{equation*}
so we can say that there is some $\bm R$ connecting the orthonormal $\bm Q$ back to $\bm A$.\\
\vspace{1mm}\\
To find this $\bm R$, we simply multiply the entire expression by $\bm Q^T$:
\begin{equation*}
\bm Q^T\bm A=\bm Q^T\bm{QR}=\bm R
\end{equation*}
So we have $\bm R=\bm Q^T\bm A$:
\begin{equation*}
\bm R=\left[\begin{array}{c}
\bm q_1^T\\\bm q_2^T\\\bm q_3^T
\end{array}\right]\left[
\begin{array}{ccc}
\bm a&\bm b&\bm c\end{array}\right]=\left[\begin{array}{ccc}
\bm q_1^T\bm a&\bm q_1^T\bm b&\bm q_1^T\bm c\\
\bm q_2^T\bm a&\bm q_2^T\bm b&\bm q_2^T\bm c\\
\bm q_3^T\bm a&\bm q_3^T\bm b&\bm q_3^T\bm c
\end{array}\right]
\end{equation*}
This can be simplified. We know that
\begin{itemize}
\item The vectors $\bm a$ and $\bm A$ and $\bm q_1$ are all along a single line
\item The vectors $\bm a$, $\bm b$ and $\bm A$, $\bm B$ and $\bm q_1$, $\bm q_2$ are all in the same plane
\item The vectors $\bm a$, $\bm b$, $\bm c$ and $\bm A$, $\bm B$, $\bm C$ and $\bm q_1$, $\bm q_2$, $\bm q_3$ are all in the same 3 dimensional subspace
\end{itemize}
The point here is that $q_2$ is orthogonal to that entire first line, and $q_3$ is orthogonal to that entire second plane. Looking back at $\bm R$, see that 
$\bm q_2^T\bm a=0$ by orthogonality, as well as 
$\bm q_3^T\bm a$ and $\bm q_3^T\bm b$. Our simplified matrix looks like
\begin{center}
\includegraphics[width=11cm]{90}
\end{center}
(next page)\newpage
\noindent\textbf{Cont.}\\
We had
\begin{center}
\includegraphics[width=11cm]{90}
\end{center}
Intuitively, at every step $\bm a_1,\ldots,\bm a_k$ are combinations of $\bm q_1,\ldots,\bm q_k$, and so column $i$ of $\bm R$, which specifies $\bm a_i$ as a combination of 
the columns of $\bm Q$, will only have nonzero entries in the rows corresponding to the columns of $\bm Q$ that contribute to that $\bm a_i$.\\
\vspace{1mm}\\
See that this leads to $\bm R$ being \textit{triangular}---any $\bm a_j$ only depends on the $\bm q_i$ where $i<j$---so all the entries below the $j$th row in 
$\bm R$ will be zero. 
\begin{center}
\includegraphics[width=12cm]{91}
\end{center}
\textbf{In the context of Least Squares}\\
See that using our factorisation:
\begin{equation*}
\bm{A}^T\bm A=(\bm{QR})^T\bm{QR}=\bm R^T\bm Q^T\bm{QR}=\bm R^T\bm R
\end{equation*}
the least squares formula
\begin{equation*}
\bm{A}^T\bm A\hat{\bm x}=\bm{A}^T\bm b
\end{equation*}
simplifies to
\begin{align*}
\bm R^T\bm R\hat{\bm x}&=(\bm{QR})^T\bm b\\\
\bm R^T\bm R\hat{\bm x}&=\bm R^T\bm Q^T\bm b\\
\bm R\hat{\bm x}=\bm Q^T\bm b\quad&\text{or}\quad\hat{\bm x}=\bm R^{-1}\bm Q^T\bm b
\end{align*}
($\bm R$ is invertible since it has full column rank. It has full row rank too so $\bm R^T$ also has full column rank and is invertible (justifying the last step))\\
\vspace{1mm}\\
This allows us to solve for the least squares solution $\hat{\bm x}$ using back substitution (which is very fast). The real cost is the computation for Gram-Schmidt,
which is needed to construct the orthogonal $\bm Q$ and the triangular $\bm R=\bm Q^T\bm A$.\\
(next page)\newpage
\noindent\textbf{An algorithm}\\
Here is an algorithm for the Gram-Schmidt process:
\begin{center}
\includegraphics[width=12cm]{92}
\end{center}
Which essentially executes
\begin{center}
\includegraphics[width=12cm]{93}
\end{center}
Where $r$ denotes entries of $\bm R$ and $\bm v$ a column in the original $\bm A$.\\
\vspace{1mm}\\
The diagonal entries of $\bm R$ are just the norms of the unnormalised projections in the Gram-schmidt process. For instance 
\begin{equation*}
\bm q_2^T\bm b=\bm q_2\cdot\bm b=\cos(\theta)||\bm q_2||\,||\bm b||=\cos(\theta)||\bm b||=||\bm B||
\end{equation*}
$\theta$, the angle between $\bm b$ and its projection $\bm q_2$, is the same angle between $\bm b$ and its unnormalised projection $\bm B$.
\newpage

\section{Pseudoinverses}
\textbf{Left Inverse}\\
Consider a `tall and thin' matrix $\bm A$, where $m>n$ with rank $r=n$. A matrix like this only has a \textit{left sided} inverse (consider that $\bm{x}^T\bm{A}$ would 
produce a matrix whose rows are combinations of linearly dependent rows---this cannot be `undone' to recover $\bm x^T$ given a result. On the other hand $\bm{Ax}$ can be
`undone' since this way the mapping $\bm A$ is unique to each $\bm x$.) \\
\vspace{1mm}\\
In this case $\bm A$ has independent columns, so $\bm A^T\bm A$ ($n$x$n$) is invertible, and 
\begin{equation*}
(\bm A^T\bm A)^{-1}\bm A^T\bm A=\bm I
\end{equation*}
We say that 
\begin{equation*}
\bm{A}^{-1}_{\text{left}}=(\bm A^T\bm A)^{-1}\bm A^T
\end{equation*}
is the \textit{left inverse} of $\bm A$ (there may be other left inverses), where 
$\bm{A}^{-1}_{\text{left}}\bm A=\bm I$.\\
\vspace{1mm}\\
\textbf{Right inverse}\\
In another case consider if $\bm A$ were `short and fat', where $m<n$ with rank $r=m$. A matrix like this only has a right sided inverse. $\bm A$ has independent rows, so
now $\bm A\bm A^T$ is invertible and 
\begin{equation*}
\bm A\bm A^T(\bm A\bm A^T)^{-1}=\bm I
\end{equation*}
A nice \textit{right inverse} is therefore:
\begin{equation*}
\bm A^{-1}_{\text{right}}=\bm A^T(\bm A\bm A^T)^{-1}
\end{equation*}
where $\bm A\bm A^{-1}_{\text{right}}=\bm I$.\\
\vspace{1mm}\\
\textbf{Left and right inverses as projections}\\
If $\bm A$ has full column rank and $\bm{A}^{-1}_{\text{left}}=(\bm A^T\bm A)^{-1}\bm A^T$, applying the left inverse on the right leads us to 
\begin{equation*}
\bm{AA}^{-1}_{\text{left}}=\bm A(\bm A^T\bm A)^{-1}\bm A^T=\bm P
\end{equation*}
Is just a matrix which projects $\mathbb{R}^m$ onto the \textit{column space} of $\bm A$.\\
\vspace{1mm}\\
Similarly, if $\bm A$ has full row rank then 
\begin{equation*}
\bm A^{-1}_{\text{right}}\bm A=\bm A^T(\bm A\bm A^T)^{-1}\bm A=\bm P
\end{equation*}
Is just a matrix that projects $\mathbb{R}^n$ onto the \textit{row space} of $\bm A$.\\
(next page)\newpage
\noindent\textbf{Intuition for the pseudoinverse}\\
The remaining case to consider is a matrix $\bm A$ for which $r<n$ and $r<m$. The vector $\bm{Ax}$ is always in the column space of $\bm A$; recall that the correspondence
between vectors $\bm x$ in the ($r$ dimensional) row space and vectors $\bm{Ax}$ in the ($r$ dimensional) column space is one-to-one---meaning that 
if $\bm x\neq \bm y$ are vectors in the row space of $\bm A$ then $\bm{Ax}\neq\bm{Ay}$ in the column space of $\bm A$---we can show this.\\
\vspace{1mm}\\
\textit{Proving that if $\bm x\neq\bm y$ then $\bm{Ax}\neq\bm{Ay}$}: Suppose that the statement is false, then
we can find $\bm x\neq \bm y$ in the row space of $\bm A$ for which $\bm{Ax}=\bm{Ay}$. But then $\bm{A}(\bm x-\bm y)=\bm 0$ and $\bm x-\bm y$ is in the 
nullspace of $\bm A$; however since the row space is closed under linear combinations (including subtraction), $\bm x-\bm y$ is also in the row space of $\bm A$. The only
vector in both the nullspace and row space is the zero vector, so $\bm x-\bm y=\bm 0$,
and the two vectors must be equal.\\
\vspace{1mm}\\
We can therefore conclude that the mapping $\bm x\mapsto\bm{Ax}$ from the row space to the column space is \textit{invertible}. The inverse of this operation is called the
\textit{pseudoinverse}.






\newpage

\chapter{Determinants}
\section{Properties of determinants}
We can derive a number of ideas given the following three properties of determinants:
\begin{center}
\includegraphics[width=11cm]{94}
\end{center}
For a nice geometric way to think of the determinant, consider a unit cube in $N$ dimensional space: the set of $N$ vectors of length 1 with coordinates 0 or 1 
in each spot. The determinant of the linear transformation (matrix) $\bm T$ is the \textit{signed volume of the region after applying $\bm T$ to the unit cube}.\\
\vspace{1mm}\\
If we apply the identity to the unit cube, we get back the unit cube, with volume 1 (the first property). If we stretch the cube by a constant factor in one direction only, 
the new volume is that constant; if we stack two blocks together aligned on the same direction, their combined volume is the sum of their volumes---showing that the signed 
volume is linear in each coordinate (property 3).\\
(next page)\newpage
\noindent\textbf{Cont.---linearity in all rows separately}\\
The third property essentially states that the determinant is linear with respect to row 1; then property 2 leads us further: det($\bm A$) is linear \textit{to every row 
separately}:
\begin{align*}
\text{det}\left[\begin{array}{c}
\text{row 1}\\c\bm v+d\bm w\\\cdots\\\text{row $n$}
\end{array}\right]&=
-\text{det}\left[\begin{array}{c}
c\bm v+d\bm w\\\text{row 1}\\\cdots\\\text{row $n$}
\end{array}\right]\\
&=-c\,\text{det}\left[\begin{array}{c}
\bm v\\\text{row 1}\\\cdots\\\text{row $n$}
\end{array}\right]-
d\,\text{det}\left[\begin{array}{c}
\bm w\\\text{row 1}\\\cdots\\\text{row $n$}
\end{array}\right]\\
&=c\,\text{det}\left[\begin{array}{c}
\text{row 1}\\\bm v\\\cdots\\\text{row $n$}
\end{array}\right]+
d\,\text{det}\left[\begin{array}{c}
\text{row 1}\\\bm w\\\cdots\\\text{row $n$}
\end{array}\right]
\end{align*}

\subsection{If $\bm A$ has two equal rows, its determinant is 0; row operations don't change the determinant}
Consider property 2; if $\bm A$ were to have two equal rows, exchanging them would have no effect on $\bm A$. Then det($\bm A$)=$-$det($\bm A$), 
and det($\bm A$) must be zero.\\
\vspace{1mm}\\
Now consider this with property 3, and see that \textit{subtracting $d$ times row $i$ from row $j$ leaves det($\bm A$) unchanged}:
\begin{equation*}
\text{det}\left[\begin{array}{c}
\text{row 1}\\\text{row 2}-d\,\text{row 1}\\\cdots\\\text{row $n$}
\end{array}\right]=
\text{det}\left[\begin{array}{c}
\text{row 1}\\\text{row 2}\\\cdots\\\text{row $n$}
\end{array}\right]
-d\,\text{det}\left[\begin{array}{c}
\text{row 1}\\\text{row 1}\\\cdots\\\text{row $n$}
\end{array}\right]=\text{det}\left[\begin{array}{c}
\text{row 1}\\\text{row 2}\\\cdots\\\text{row $n$}
\end{array}\right]
\end{equation*}
See that this means that the elimination steps (excluding row exchanges) \textit{does not change the determinant}, and elimination (without row exchanges) is the way to
simplify the determinant of a matrix.
\newpage

\subsection{Upper triangular and diagonal matrices, Transpose}
Finally, see that given any \textit{upper triangular matrix}, row exchanges are not required and upward elimination will reduce the matrix to its diagonal. As such, given an
upper triangular matrix, we can just immediately multiply the diagonal entries to find the determinant:
\begin{center}
\includegraphics[width=11cm]{95}
\end{center}
Now see that transposing an invertible matrix won't change the diagonals, and if the matrix is invertible its transposed form
is guaranteed to have an upper triangular form (taking
into account row exchanges). So by row reductions we can obtain the same diagonal matrix as if we had performed elimination on the orignal matrix:
\begin{equation*}
\text{det}(\bm A)=\text{det(rref form)}=\text{det}(\bm A^T)
\end{equation*}
and so we can conclude that transposing a matrix $\bm A$ doesn't change the determinant:
\begin{center}
\includegraphics[width=9cm]{96}
\end{center}
\subsection{det$(\bm{AB})=($det$\bm A)($det$\bm B)$}
For a singular matrix $\bm A$, recall that it can be reduced to an identity matrix by elimination matrices (all of which are invertible); 
then by using the inverse (which is just the reverse of each row operation) of each elimination matrix, we can express $\bm A$ as 
\begin{equation*}
\bm A=\bm{E}_k\bm{E}_{k-1}\cdots\bm{E}_{1}
\end{equation*} 
These matrices are also called \textit{elementary row matrices}, which are just identity matrices on which exactly \textit{one elementary row operation has been performed}. 
These elementary row operations are the same as those executed during elimination---scaling a row, adding a scaled row to another row, and exchanging rows.\\
\vspace{1mm}\\
We can write
\begin{equation*}
\text{det}(\bm{AB})=\text{det}(\bm{E}_k\bm{E}_{k-1}\cdots\bm{E}_{1}\bm B)
\end{equation*}
See that any elementary row operation just scales the determinant by a constant. Scaling a single row scales the entire determinant by that constant, 
exchanging rows negates the 
determinant, and adding (or subtracting) a scaled row to (or from) another row doesn't change the determinant. As such,
\begin{equation*}
\text{det}(\bm{E}_k\bm{E}_{k-1}\cdots\bm{E}_{1}\bm B)=c_kc_{k-1}\cdots c_1\text{det}(\bm B)=\alpha\,\text{det}(\bm B)
\end{equation*}
where $c$ denotes the constants multiplied to the determinant as a result of the row operations.\\
(next page)\newpage
\noindent\textbf{Cont.}\\
We had
\begin{equation*}
\text{det}(\bm{AB})=\text{det}(\bm{E}_k\bm{E}_{k-1}\cdots\bm{E}_{1}\bm B)=c_kc_{k-1}\cdots c_1\text{det}(\bm B)=\alpha\,\text{det}(\bm B)
\end{equation*}
Then by setting $\bm B$ to $\bm I$:
\begin{align*}
\text{det}(\bm{AI})=\text{det}(\bm{E}_k\bm{E}_{k-1}\cdots\bm{E}_{1}\bm I)&=c_kc_{k-1}\cdots c_1\text{det}(\bm I)=\alpha\,\text{det}(\bm I)\\
\text{det}(\bm A)&=\alpha
\end{align*}
and
\begin{equation*}
\text{det}(\bm{AB})=\text{det}(\bm A)\text{det}(\bm B)
\end{equation*}

\subsection{Orthogonal matrices have determinant 1 or -1}
We know that
\begin{equation*}
\text{det}(\bm A)=\text{det}(\bm A^T)
\end{equation*}
and that
\begin{equation*}
\text{det}(\bm{AB})=\text{det}(\bm A)\text{det}(\bm B)
\end{equation*}
Given an orthogonal matrix $\bm Q$, we know
\begin{equation*}
\bm Q^T\bm Q=\bm I
\end{equation*}
so
\begin{equation*}
\text{det}(\bm Q^T\bm Q)=\text{det}(\bm Q^T)\text{det}(\bm Q)=\text{det}(\bm I)=1
\end{equation*}
and since the determinant doesn't change under transposition:
\begin{equation*}
(\text{det}(\bm Q))^2=1
\end{equation*}
and $\text{det}(\bm Q)$ is $\pm1$.

\subsection{Matrix factorisations and permutation matrix}
Recall the matrix factorisation $\bm A=\bm{LU}$. $\bm L$ is invertible with 1s along the main diagonal, $\bm U$ is upper triangular. In this case
\begin{equation*}
\text{det}\bm A=(\text{det}\bm L)(\text{det}\bm U)=\text{det}\bm U
\end{equation*}
Where the determinant can be found by just multiplying down the diagonal.\\
\vspace{1mm}\\
Similarly, we could have $\bm{PA}=\bm{LU}$ due to row exchanges being required before elimination. We know that the permutation matrix $\bm P$ is just the identity with 
row exchanges applied to it, and so we just have to factor in
$\det\bm P$, which will be equal to 1 or -1 depending on how many row exchanges occur.\\
\vspace{1mm}\\
Elimination, followed by multiplying down the diagonal, is how determinants are computed by virtually all computer systems for linear algebra.
\newpage

\section{Intuition for determinant formula}
Consider a 3 by 3 determinant of the following matrix
\begin{equation*}
\text{det}\left[\begin{array}{ccc}
a&b&c\\
p&q&r\\
x&y&z
\end{array}\right]
\end{equation*}
By linearity,
\begin{align*}
\text{det}\left[\begin{array}{ccc}
a&b&c\\
p&q&r\\
x&y&z
\end{array}\right]&
=\text{det}\left[\begin{array}{ccc}
a&0&0\\
p&q&r\\
x&y&z
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
0&b&0\\
p&q&r\\
x&y&z
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
0&0&c\\
p&q&r\\
x&y&z
\end{array}\right]
\end{align*}
Looking closer at the first matrix on the right side, again by linearity,
\begin{align*}
\text{det}\left[\begin{array}{ccc}
a&0&0\\
p&q&r\\
x&y&z
\end{array}\right]&=
\text{det}\left[\begin{array}{ccc}
a&0&0\\
p&0&0\\
x&y&z
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
a&0&0\\
0&q&0\\
x&y&z
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
a&0&0\\
0&0&r\\
x&y&z
\end{array}\right]\\
=\,&\text{det}\left[\begin{array}{ccc}
a&0&0\\
p&0&0\\
x&0&0
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
a&0&0\\
p&0&0\\
0&y&0
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
a&0&0\\
p&0&0\\
0&0&z
\end{array}\right]+\\
&\text{det}\left[\begin{array}{ccc}
a&0&0\\
0&q&0\\
x&0&0
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
a&0&0\\
0&q&0\\
0&y&0
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
a&0&0\\
0&q&0\\
0&0&z
\end{array}\right]+\\
&\text{det}\left[\begin{array}{ccc}
a&0&0\\
0&0&r\\
x&0&0
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
a&0&0\\
0&0&r\\
0&y&0
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
a&0&0\\
0&0&r\\
0&0&z
\end{array}\right]
\end{align*}
Looking at that final equality, see that the matrices containing a zero column are immediately noninvertible and have determinant 0. This leaves us with the simplification
\begin{equation*}
\text{det}\left[\begin{array}{ccc}
a&0&0\\
p&q&r\\
x&y&z
\end{array}\right]=
\text{det}\left[\begin{array}{ccc}
a&0&0\\
0&q&0\\
0&0&z
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
a&0&0\\
0&0&r\\
0&y&0
\end{array}\right]
\end{equation*}
The other two matrices on the right side of that first equation can be expanded in a similar manner, 
\begin{align*}
\text{det}\left[\begin{array}{ccc}
0&b&0\\
p&q&r\\
x&y&z
\end{array}\right]&=
\text{det}\left[\begin{array}{ccc}
0&b&0\\
p&0&0\\
0&0&z
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
0&b&0\\
0&0&r\\
x&0&0
\end{array}\right]\\
\text{det}\left[\begin{array}{ccc}
0&0&c\\
p&q&r\\
x&y&z
\end{array}\right]&=
\text{det}\left[\begin{array}{ccc}
0&0&c\\
0&q&0\\
x&0&0
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
0&0&c\\
p&0&0\\
0&y&0
\end{array}\right]
\end{align*}
(next page)\newpage
\noindent\textbf{Cont.}\\
As per our reasoning, we can write the determinant of a 3 by 3 matrix as
\begin{align*}
\text{det}\left[\begin{array}{ccc}
a&b&c\\
p&q&r\\
x&y&z
\end{array}\right]&
=\text{det}\left[\begin{array}{ccc}
a&0&0\\
0&q&0\\
0&0&z
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
a&0&0\\
0&0&r\\
0&y&0
\end{array}\right]+\\
&\text{det}\left[\begin{array}{ccc}
0&b&0\\
p&0&0\\
0&0&z
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
0&b&0\\
0&0&r\\
x&0&0
\end{array}\right]+\\
&\text{det}\left[\begin{array}{ccc}
0&0&c\\
0&q&0\\
x&0&0
\end{array}\right]+
\text{det}\left[\begin{array}{ccc}
0&0&c\\
p&0&0\\
0&y&0
\end{array}\right]
\end{align*}
This is the sum of the determinants of the following matrices 
\begin{center}
\includegraphics[width=12cm]{98}
\end{center}
See that this is as if we had taken all the possible row permutations of 
a 3 by 3 identity, and then for each permutation, we replace the ones with the numbers at those same positions from the matrix of interest. 
Then we sum up their determinants.\\
\vspace{1mm}\\
So how do we compute the determinants?
Starting with the identity matrix (det($\bm A$)=1), exchanging two rows negates the determinant to -1, exchanging again puts it back to 1 
and so on. See that there are six ways to permute the rows of the 3 by 3 identity, each with determinants alternating between 1 and -1:
\begin{center}
\includegraphics[width=12cm]{97}
\end{center}
Multiplying a row by a constant multiplies the determinant by that constant. So supposing the three rows are $a,b,c$ and $p,q,r$ and $x,y,z$, 
\begin{center}
\includegraphics[width=12cm]{99}
\end{center}
Summing up all the individual determinants gives the final desired determinant. See that the `checkerboard' pattern of negating the determinant in the laplace expansion
just comes from row exchanges.\\
(next page)\newpage
\noindent\textbf{Link to recursive computation}\\
Consider the 4 by 4 determinant 
\begin{equation*}
\text{det}\left[\begin{array}{cccc}
A&0&0&0\\
0&a&b&c\\
0&p&q&r\\
0&x&y&z
\end{array}\right]
\end{equation*}
and see that by linearity, this just evaluates to
\begin{align*}
A\,\text{det}\left[\begin{array}{cccc}
1&0&0&0\\
0&a&b&c\\
0&p&q&r\\
0&x&y&z
\end{array}\right]&=A\left(\text{det}\left[\begin{array}{cccc}
1&0&0&0\\
0&a&0&0\\
0&0&q&0\\
0&0&0&z
\end{array}\right]+
\text{det}\left[\begin{array}{cccc}
1&0&0&0\\
0&0&b&0\\
0&p&0&0\\
0&0&0&z
\end{array}\right]+\right.\\
&\left.\text{det}\left[\begin{array}{cccc}
1&0&0&0\\
0&0&b&0\\
0&0&0&r\\
0&x&0&0
\end{array}\right]+
\text{det}\left[\begin{array}{cccc}
1&0&0&0\\
0&0&0&c\\
0&0&q&0\\
0&x&0&0
\end{array}\right]+\cdots\right)\\
&=A\,\text{det}\left[\begin{array}{ccc}
a&b&c\\
p&q&r\\
x&y&z
\end{array}\right]
\end{align*}
and so from a general 4 by 4 matrix, (now using different alphabets for the entries so don't confuse this with the earlier notation)
\begin{align*}
\text{det}\left[\begin{array}{cccc}
a&b&c&d\\
e&f&g&h\\
i&j&k&l\\
m&n&o&p
\end{array}\right]=&
\text{det}\left[\begin{array}{cccc}
a&0&0&0\\
e&f&g&h\\
i&j&k&l\\
m&n&o&p
\end{array}\right]+
\text{det}\left[\begin{array}{cccc}
0&b&0&0\\
e&f&g&h\\
i&j&k&l\\
m&n&o&p
\end{array}\right]\\
&+\text{det}\left[\begin{array}{cccc}
0&0&c&0\\
e&f&g&h\\
i&j&k&l\\
m&n&o&p
\end{array}\right]+
\text{det}\left[\begin{array}{cccc}
0&0&0&d\\
e&f&g&h\\
i&j&k&l\\
m&n&o&p
\end{array}\right]
\end{align*}
considering the first right hand side matrix, see that by linearity
\begin{align*}
\text{det}\left[\begin{array}{cccc}
a&0&0&0\\
e&f&g&h\\
i&j&k&l\\
m&n&o&p
\end{array}\right]=\underbrace{\text{det}\left[\begin{array}{cccc}
a&0&0&0\\
e&0&0&0\\
i&j&k&l\\
m&n&o&p
\end{array}\right]}_{=0}+\text{det}\left[\begin{array}{cccc}
a&0&0&0\\
0&f&g&h\\
i&j&k&l\\
m&n&o&p
\end{array}\right]
\end{align*}
where the first matrix on the right hand side is clearly noninvertible and therefore has determinant 0.\\
(next page)\newpage
\noindent\textbf{Cont.}\\
Continuing 
\begin{align*}
\text{det}\left[\begin{array}{cccc}
a&0&0&0\\
e&f&g&h\\
i&j&k&l\\
m&n&o&p
\end{array}\right]&=\text{det}\left[\begin{array}{cccc}
a&0&0&0\\
0&f&g&h\\
i&j&k&l\\
m&n&o&p
\end{array}\right]\\
&=\underbrace{\text{det}\left[\begin{array}{cccc}
a&0&0&0\\
0&f&g&h\\
i&0&0&0\\
m&n&o&p
\end{array}\right]}_{=0}+\text{det}\left[\begin{array}{cccc}
a&0&0&0\\
0&f&g&h\\
0&j&k&l\\
m&n&o&p
\end{array}\right]\\
&=\underbrace{\text{det}\left[\begin{array}{cccc}
a&0&0&0\\
0&f&g&h\\
0&j&k&l\\
m&0&0&0
\end{array}\right]}_{=0}+\text{det}\left[\begin{array}{cccc}
a&0&0&0\\
0&f&g&h\\
0&j&k&l\\
0&n&o&p
\end{array}\right]
\end{align*}
and using our previous idea,
\begin{equation*}
\text{det}\left[\begin{array}{cccc}
a&0&0&0\\
0&f&g&h\\
0&j&k&l\\
0&n&o&p
\end{array}\right]=a\,\text{det}\left[\begin{array}{ccc}
f&g&h\\
j&k&l\\
n&o&p
\end{array}\right]
\end{equation*}
deriving the formula for the laplace expansion from here is fairly straightforward. It is just a matter of applying this reasoning to the other matrices while taking into 
account the negation caused by required row exchanges. We eventually get
\begin{align*}
\text{det}\left[\begin{array}{cccc}
a&b&c&d\\
e&f&g&h\\
i&j&k&l\\
m&n&o&p
\end{array}\right]&=a\,\text{det}\left[\begin{array}{ccc}
f&g&h\\
j&k&l\\
n&o&p
\end{array}\right]-
b\,\text{det}\left[\begin{array}{ccc}
e&g&h\\
i&k&l\\
m&o&p
\end{array}\right]+\\
&c\,\text{det}\left[\begin{array}{ccc}
e&f&h\\
i&j&l\\
m&n&p
\end{array}\right]-
d\,\text{det}\left[\begin{array}{ccc}
e&f&g\\
i&j&k\\
m&n&o
\end{array}\right]
\end{align*}
which is the usual `formula' used when computing matrices by hand.
\newpage

\section{Cofactors, Adjugate, and Inverse}
The laplace expansion allows us to reduce a $n$ by $n$ determinant to a sum of scaled $(n-1)$ by $(n-1)$ determinants. The \textit{cofactors} in a laplace transform are 
essentially these smaller determinants, but with alternanting negation to account for
row exchanges:
\begin{center}
\includegraphics[width=11cm]{100}
\end{center}
So given a 3 by 3 matrix $\bm A$
\begin{equation*}
\bm A=
\left[\begin{array}{ccc}
a&b&c\\d&e&f\\g&h&i
\end{array}\right]
\end{equation*}
Taking the cofactor for every entry in the matrix, we can construct a cofactor matrix $\bm C$ as
\begin{equation*}
\bm C=
\left[\begin{array}{ccc}
\text{det}\left[\begin{array}{cc}e&f\\h&i\end{array}\right]&
-\text{det}\left[\begin{array}{cc}d&f\\g&i\end{array}\right]&
\text{det}\left[\begin{array}{cc}d&e\\g&h\end{array}\right]\\
-\text{det}\left[\begin{array}{cc}b&c\\h&i\end{array}\right]&
\text{det}\left[\begin{array}{cc}a&c\\g&i\end{array}\right]&
-\text{det}\left[\begin{array}{cc}a&b\\g&h\end{array}\right]\\
\text{det}\left[\begin{array}{cc}b&c\\e&f\end{array}\right]&
-\text{det}\left[\begin{array}{cc}a&c\\d&f\end{array}\right]&
\text{det}\left[\begin{array}{cc}a&b\\d&e\end{array}\right]
\end{array}\right]
\end{equation*}
We define $\bm C^T$ as the \textit{adjugate matrix}:
\begin{equation*}
\bm C^T=
\left[\begin{array}{ccc}
\text{det}\left[\begin{array}{cc}e&f\\h&i\end{array}\right]&
-\text{det}\left[\begin{array}{cc}b&c\\h&i\end{array}\right]&
\text{det}\left[\begin{array}{cc}b&c\\e&f\end{array}\right]\\
-\text{det}\left[\begin{array}{cc}d&f\\g&i\end{array}\right]&
\text{det}\left[\begin{array}{cc}a&c\\g&i\end{array}\right]&
-\text{det}\left[\begin{array}{cc}a&c\\d&f\end{array}\right]\\
\text{det}\left[\begin{array}{cc}d&e\\g&h\end{array}\right]&
-\text{det}\left[\begin{array}{cc}a&b\\g&h\end{array}\right]&
\text{det}\left[\begin{array}{cc}a&b\\d&e\end{array}\right]
\end{array}\right]
\end{equation*}
(next page)\newpage
\noindent\textbf{Cont.}\\
We had
\begin{equation*}
\bm A=
\left[\begin{array}{ccc}
a&b&c\\d&e&f\\g&h&i
\end{array}\right]
\end{equation*}
and
\begin{equation*}
\bm C^T=
\left[\begin{array}{ccc}
\text{det}\left[\begin{array}{cc}e&f\\h&i\end{array}\right]&
-\text{det}\left[\begin{array}{cc}b&c\\h&i\end{array}\right]&
\text{det}\left[\begin{array}{cc}b&c\\e&f\end{array}\right]\\
-\text{det}\left[\begin{array}{cc}d&f\\g&i\end{array}\right]&
\text{det}\left[\begin{array}{cc}a&c\\g&i\end{array}\right]&
-\text{det}\left[\begin{array}{cc}a&c\\d&f\end{array}\right]\\
\text{det}\left[\begin{array}{cc}d&e\\g&h\end{array}\right]&
-\text{det}\left[\begin{array}{cc}a&b\\g&h\end{array}\right]&
\text{det}\left[\begin{array}{cc}a&b\\d&e\end{array}\right]
\end{array}\right]
\end{equation*}
Now consider the expression $\bm{AC}^T$. See that computing the \textit{diagonal} of the result is the same as taking the laplace expansion along each row of $\bm A$:
\begin{equation*}
\bm{AC}^T=\left[\begin{array}{ccc}
\text{det}(\bm A)&\cdots&\cdots\\
\cdots&\text{det}(\bm A)&\cdots\\
\cdots&\cdots&\text{det}(\bm A)
\end{array}\right]
\end{equation*}
This expression has a unique property---every entry on the off diagonal is 0. This is because computing those entries would be equivalent to the \textit{laplace expansion 
for the determinant of a matrix with two linearly dependent or identical rows}. For instance consider the $(1,2)$ of $\bm{AC}^T$:
\begin{equation*}
(\bm{AC}^T)_{1,2}=-a\,\text{det}\left[\begin{array}{cc}b&c\\h&i\end{array}\right]+b\,
\text{det}\left[\begin{array}{cc}a&c\\g&i\end{array}\right]
-c\,\text{det}\left[\begin{array}{cc}a&b\\g&h\end{array}\right]
\end{equation*}
See that this is the same as taking the determinant of 	
\begin{equation*}
\left[\begin{array}{ccc}
-a&-b&-c\\
a&b&c\\
g&h&i
\end{array}\right]
\end{equation*}
Which is zero because two rows are linearly dependent. This property can be seen in any of the off-diagonal entries. Our final result is
\begin{equation*}
\bm{AC}^T=\left[\begin{array}{ccc}
\text{det}(\bm A)&0&0\\
0&\text{det}(\bm A)&0\\
0&0&\text{det}(\bm A)
\end{array}\right]
\end{equation*}
This pattern can be extrapolated to higher order matrices. We have the formula
\begin{equation*}
\bm{AC}^T=(\text{det}(\bm A))\bm I
\end{equation*}
This gives us a nice formula for the inverse matrix $\bm A^{-1}$:
\begin{equation*}
\bm A^{-1}=\frac{1}{\text{det}(\bm A)}\bm C^T
\end{equation*}
See that, as expected, the determinant of an invertible matrix cannot be zero.
\newpage

\section{Cramer's Rule to solve $\bm{Ax}=\bm b$}
Cramer's rule allows us to procedurally find the components $x_i$, of the solution vector in $\bm{Ax}=\bm b$. The idea here is 
to construct a matrix starting with $\bm I$; by replacing the first column of 
$\bm I$ by $\bm x$, \textit{this triangular matrix $\bm M_1$ has a determinant equal to $x_1$}:
\begin{equation*}
\text{det}\left[\begin{array}{ccc}
x_1&0&0\\
x_2&1&0\\
x_3&0&1
\end{array}\right]=x_1
\end{equation*}
(consider eliminating downward using row operations then taking the product down the diagonal). By multiplying by $\bm A$, 
\textit{the first column of the result becomes $\bm{Ax}$
which is $\bm b$}; the other columns are copied from $\bm A$. We denote this final matrix by $\bm B_1$:
\begin{center}
\includegraphics[width=10cm]{101}
\end{center}
See that if we had the determinants of $\bm B_1$ and $\bm A$, we can obtain $x_1$:
\begin{center}
\includegraphics[width=11cm]{102}
\end{center}
This allowed us to obtain the first component of $\bm x$. To obtain the other components we apply the same principle again; to find $x_2$ and $\bm B_2$, we replace the 
\textit{second column} of $\bm I$ with $\bm x$:
\begin{center}
\includegraphics[width=10cm]{103}
\end{center}
and take determinants to get
\begin{equation*}
(\text{det}\bm A)(x_2)=\text{det}\bm B_2\implies x_2=\frac{\text{det}\bm B_2}{\text{det}\bm A}
\end{equation*}
\begin{center}
\includegraphics[width=12cm]{104}
\end{center}
To solve an $n$ by $n$ system, Cramer's rule evaluates $n+1$ determinants (of $\bm A$ and $n$ different $\bm B_i$). Computing each determinant
would involve $n!$ terms (row permutations of $n$ by $n$ $\bm I$)---totalling $(n+1)!$ terms. This is a very inefficient method for solving equations (although it does give
an explicit formula).
\newpage
\subsection{Inverse formula from Cramer's rule}
Consider using Cramer's rule to solve for $\bm A^{-1}$. For the 2 by 2 case, $\bm A^{-1}$ has two columns; denoting them by $\bm x$ and $\bm y$, we solve 
\begin{equation*}
\bm{AA}^{-1}=\bm{A}\left[\begin{array}{cc}\bm x&\bm y\end{array}\right]=\bm I
\end{equation*}
solving each column at a time, 
\begin{center}
\includegraphics[width=10cm]{105}
\end{center}
Given $|\bm A|$ and the other required determinants
\begin{center}
\includegraphics[width=10cm]{106}
\end{center}
with that we compute the components as
\begin{center}
\includegraphics[width=12cm]{107}
\end{center}
When using Cramer's rule to solve $\bm{AA}^{-1}=\bm I$, \textit{the determinant of each $\bm B_j$ is a cofactor of $\bm A$}. Consider the 3 by 3 case; we want the inverse of 
\begin{equation*}
\bm A=\left[\begin{array}{ccc}
a_{11}&a_{12}&a_{13}\\
a_{21}&a_{22}&a_{23}\\
a_{31}&a_{32}&a_{33}
\end{array}\right]
\end{equation*}
solving for the first \textit{column} of $\bm A^{-1}$ requires the determinants
\begin{center}
\includegraphics[width=12cm]{108}
\end{center}
see that these determinants are the same as those on the first \textit{row} of the cofactor matrix 
(transposing and eliminating downward using row operations, then transposing back gives the exact cofactor). As such we have
\begin{center}
\includegraphics[width=11cm]{109}
\end{center}
Which leaves us with the inverse matrix formula as derived earlier.
\newpage

\section{Sign of a permutation and determinants}
\textbf{Notation}\\
We introduce the concept of a sign/signature of a permutation of a set of natural numbers. Consider a set of the first $n$ natural numbers
\begin{equation*}
1,2,\ldots,n
\end{equation*}
We will denote a permuation by
\begin{equation*}
\pi(1),\ldots,\pi(n)
\end{equation*}
where $\pi(1)$ is the first element of the permutation, $\pi(2)$ is the second, and so on. For instance, a possible permutation of the first 4 natural numbers is 2,1,4,3;
its elements can be denoted as 
\begin{equation*}
\pi(1)=2,\quad\pi(2)=1,\quad\pi(3)=4,\quad\pi(4)=3
\end{equation*}
\textbf{Inversions}\\
Two elements of a permutation $\pi(i)$ and $\pi(j)$ is said to be an inversion if and only if
\begin{equation*}
i<j\quad\text{but}\quad\pi(i)>\pi(j)
\end{equation*}
in other words, two elements are an inversion if their natural order is inverted. For instance, the permutation 4,1,3,2 has the inversions
\begin{equation*}
(4,1),\quad(4,3),\quad(4,2),\quad(3,2)
\end{equation*}
\textbf{Parity of a permutation}\\
A permutation is said to be \textit{even} if and only if the total number of inversions it contains is even. Otherwise it is said to be \textit{odd}.\\
\vspace{1mm}\\
In the previous example, the permutation 4,1,3,2 has 4 inversions. As such it is \textit{even}.\\
\vspace{1mm}\\
\textbf{Definition of sign}\\
The sign of a permutation $\pi$, denoted by sgn($\pi$) is a function defined as follows:
\begin{equation*}
\text{sgn}(\pi)=\begin{cases}
1&\text{if $\pi$ is even}\\
-1&\text{if $\pi$ is odd}\end{cases}
\end{equation*}
For instance, the permutation $\pi=\langle 4,1,3,2\rangle$ was even. Therefore sgn$(\pi)=1$.\\
(next page)\newpage
\noindent\textbf{Transpositions}\\
The operation of \textit{interchanging any two distinct elements of a permutation} is called a \textit{transposition}.\\
\vspace{1mm}\\
For instance, considering the following permutation of the first five natural numbers:
\begin{equation*}
4,3,2,1,5
\end{equation*}
The operation of interchanging its second and fourth element so as to obtain the new permuation
\begin{equation*}
4,1,2,3,5
\end{equation*}
is a transposition. Note the effect of transpositions on parity:
\begin{itemize}
\item If a permuation is odd, a transposition makes it even.
\item If a permutation is even, a transposition makes it odd.
\end{itemize}
Intuitively, a transposition either adds or removes an inversion. Either way it makes an odd number of inversions even or an even number of inversions odd.\\
\vspace{1mm}\\
\textbf{Definition of determinant}\\
Let $\bm A$ be a $K$ x $K$ matrix. Let $P$ be the set of all possible permutations of the first $K$ natural numbers.\\
\vspace{1mm}\\
The determinant of $\bm A$, denoted by det$(\bm A)$ or $|\bm A|$, is
\begin{equation*}
\text{det}=\sum_{\pi\in P}\text{sgn}(\pi)\prod^K_{k=1}A_{k,\pi(k)}
\end{equation*}
See how this formally describes the intuitive idea of the determinant as a discussed before. The product
\begin{equation*}
\prod^K_{k=1}A_{k,\pi(k)}
\end{equation*}
is over $K$ entries of $\bm A$. For each row $k=1,\ldots,K$, we choose the entry located in column $\pi(k)$; there is exactly one chosen entry $A_{k,\pi(k)}$ in each
column and row. The sign handles the negation (the row exchanges), and we sum over the set $P$ of all possible permuations $\pi$.
\newpage

\section{Determinants of block-triangular\\matrices and matrices with identity blocks}
A block matrix is of the form
\begin{equation*}
\left[\begin{array}{cc}
\bm A&\bm B\\
\bm C&\bm D
\end{array}\right]
\end{equation*}
where 
\begin{itemize}
\item $\bm A$ and $\bm B$ have the same number of rows;
\item $\bm C$ and $\bm D$ have the same number of rows;
\item $\bm A$ and $\bm C$ have the same number of columns;
\item $\bm B$ and $\bm D$ have the same number of columns.
\end{itemize}
An important fact about block matrices is that their multiplication can be carried out as per standard matrix multiplication
\begin{center}
\includegraphics[width=8cm]{120}
\end{center}
With the only caveat being that the shape of the blocks must be conformable (for instance the columns of $\bm A$ and the rows of $\bm E$ must coincide.\\
\vspace{1mm}\\
\textbf{Determinant of block-diagonal matrix with identity blocks}
The first result concerns block matrices of the form 
\begin{equation*}
\bm\Gamma=\left[\begin{array}{cc}\bm A&\bm 0\\\bm 0&\bm I\end{array}\right]\quad\text{or}\quad\bm\Gamma=\left[\begin{array}{cc}\bm I&\bm 0\\\bm 0&\bm A\end{array}\right]
\end{equation*}
Where $\bm I$ denotes the identity and $\bm A$ a square matrix. Block matrices whose off-diagonal blocks are all zero are called block-diagonal.\\
\vspace{1mm}\\
We can show that, given one of the above forms,
\begin{equation*}
\text{det}(\bm\Gamma)=\text{det}(\bm A)
\end{equation*}
(next page)\newpage
\noindent\textbf{Cont.}\\
We can show that, 
\begin{equation*}
\text{det}\left(\left[\begin{array}{cc}\bm A&\bm 0\\\bm 0&\bm I\end{array}\right]\right)=\text{det}(\bm A)=
\text{det}\left(\left[\begin{array}{cc}\bm I&\bm 0\\\bm 0&\bm A\end{array}\right]\right)
\end{equation*}
First consider the case where
\begin{equation*}
\bm\Gamma=\left[\begin{array}{cc}\bm A&\bm 0\\\bm 0&\bm I\end{array}\right]
\end{equation*}
Suppose $\bm A$ is $K$x$K$ and $\bm I$ is 1x1. Then $\bm\Gamma$ is $(K+1)$x$(K+1)$. We use the definition of the determinant:  
\begin{center}
\includegraphics[width=9cm]{121}
\end{center}
where $P$ is the set of all permutations of the first $K+1$ natural numbers. See that the term $\bm\Gamma_{K,\pi(K+1)}$ is only equal to 1 when $\pi(K+1)=K+1$ 
(since that row is zero everywhere except on the $(K+1)$th entry).\\
\vspace{1mm}\\
Also notice that the sign of the permutations would then only depend on $\pi(1),\ldots,\pi(K)$ (because $\pi(K+1)=K+1$ does not determine any inversion). As such we have
\begin{center}
\includegraphics[width=6cm]{122}
\end{center}
where $Q$ is the set of permutations of the first $K$ natural numbers.\\
(next page)\newpage
\noindent\textbf{Cont.}\\
The result for the case in which $\bm I$ is not 1x1 is proved recursively. For instance if $\bm I$ is 2x2, we have
\begin{center}
\includegraphics[width=5cm]{123}
\end{center}
and analagously for larger dimensions. The proof for the other case:
\begin{equation*}
\bm\Gamma=\left[\begin{array}{cc}\bm I&\bm 0\\\bm 0&\bm A\end{array}\right]
\end{equation*}
Is similar to the one just proved.\\
\vspace{1mm}\\
\textbf{Block-upper-triangular matrix}\\
An block-upper-triangular matrix is of the form
\begin{center}
\includegraphics[width=3cm]{124}
\end{center}
We can show that
\begin{equation*}
\text{det}(\bm\Gamma)=\text{det}(\bm A)\text{det}(\bm D)
\end{equation*}
Suppose that $\bm A$ is $K$x$K$, $\bm D$ is $L$x$L$, $\bm B$ is $K$x$L$ and $\bm 0$ is $L$x$K$. Using subscripts to denote shape, we can factor the matrix as
\begin{center}
\includegraphics[width=6cm]{125}
\end{center}
(next page)\newpage
\noindent\textbf{Cont.}\\
Thus,
\begin{center}
\includegraphics[width=9cm]{126}
\end{center}
In the last step we have 
\begin{center}
\includegraphics[width=4cm]{127}
\end{center}
(the determinant is just the product of pivots since we can eliminate upward without changing the determinant) The other two determinants come from
the earlier derived determinants of matrices with identity blocks.\\
\vspace{1mm}\\
\textbf{Block-lower-triangular matrix}\\
A block-lower-triangular matrix is of the form
\begin{center}
\includegraphics[width=3cm]{128}
\end{center}
Again we can show that
\begin{equation*}
\text{det}(\bm\Gamma)=\text{det}(\bm A)\text{det}(\bm D)
\end{equation*}
(next page)\newpage
\noindent\textbf{Cont.}\\
Suppose that $\bm A$ is $K$x$K$, $\bm D$ is $L$x$L$, $\bm C$ is $L$x$K$ and $\bm 0$ is $K$x$L$. Using subscripts to denote shape, we can factor the matrix as
\begin{center}
\includegraphics[width=7cm]{129}
\end{center}
Then, similar to the previous proof,
\begin{center}
\includegraphics[width=9cm]{130}
\end{center}
\newpage

\chapter{Eigenvalues and Eigenvectors}
\section{Computation}
$\bm x$ is an eigenvector if
\begin{equation*}
\bm{Ax}=\lambda\bm x
\end{equation*}
We have
\begin{align*}
&\bm{Ax}-\lambda\bm I\bm x=\bm 0\\
(&\bm{Ax}-\lambda\bm I)\bm x=\bm 0
\end{align*}
The matrix $(\bm A-\lambda\bm I)$ times the eigenvector $\bm x$ is the zero vector. \textit{the eigenvectors make up the nullspace of $\bm A-\lambda\bm I$.}\\
\vspace{1mm}\\
First we find the eigenvalues. If $(\bm{A}-\lambda\bm I)\bm x=\bm0$ has a nonzero solution, then $(\bm{A}-\lambda\bm I)$ is \textit{not invertible}---the determinant
of $(\bm{A}-\lambda\bm I)$ must be zero. This gives us a means to find the eigenvalues:
\begin{center}
\includegraphics[width=12cm]{110}
\end{center}
An $n$ by $n$ $(\bm{A}-\lambda\bm I)$ has $A_{ii}-\lambda$ down the main diagonal. This means that the \textit{characterstic polynomial} $\text{det}(\bm{A}-\lambda\bm I)$ 
has a degree of $n$---the determinant, after expansion into a polynomial, has $n$ solutions $\lambda_1$ to $\lambda_n$---and $\bm A$ has $n$ eigenvalues:
\begin{center}
\includegraphics[width=12cm]{111}
\end{center}
(next page)\newpage
\noindent\textbf{Cont.}\\
After obtaining the eigenvalues, we can find the eigenvectors by substituting the eigenvalues back into 
\begin{equation*}
(\bm{A}-\lambda\bm I)\bm x=\bm 0 
\end{equation*}
and solve for $\bm x$.\\
\vspace{1mm}\\
\textbf{Invertibility}\\
Consider again the intial equation
\begin{equation*}
\bm{Ax}=\lambda\bm x
\end{equation*}
If $\bm A$ were singular, it has a nontrivial nullspace and 0 is an eigenvalue, with the eigenvectors for $\lambda=0$ filling the nullspace.\\
\vspace{1mm}\\
If $\bm A$ were invertible, zero cannot be an eigenvalue. (the nullspace is just the zero vector, so any eigenvector cannot have a zero eigenvalue)
\begin{center}
\includegraphics[width=12cm]{112}
\end{center}
\newpage

\section{Eigenvalues, Determinant, and Trace}
Note that if we perform row operations on $\bm A$, the eigenvalues usually change. However, the determinant and trace of a matrix still harbour useful properties.\\
\vspace{1mm}\\
\textbf{The determinant of $\bm A$ is the product of its eigenvalues}\\
We can show that
\begin{equation*}
\text{det}(\bm A)=\lambda_1\lambda_2\cdots\lambda_n
\end{equation*}
Supposing that $\lambda_1,\ldots,\lambda_n$ are the eigenvalues of $\bm A$, then the $\lambda$ are also the roots of the characteristic polynomial:
\begin{align*}
\text{det}(\bm A-\lambda\bm I)=p(\lambda)&=(-1)^n(\lambda-\lambda_1)(\lambda-\lambda_2)\cdots(\lambda-\lambda_n)\\
&=(-1)(\lambda-\lambda_1)(-1)(\lambda-\lambda_2)\cdots(-1)(\lambda-\lambda_n)\\
&=(\lambda_1-\lambda)(\lambda_2-\lambda)\cdots(\lambda_n-\lambda)
\end{align*}
The leading coefficient $(-1)^n$ can be obtained by considering the contribution of the main diagonal to the determinant (the main diagonal is the only permutation that 
contributes to the coefficient of $\lambda^n$). Setting $\lambda$ to zero gives us our desired result.\\
\vspace{1mm}\\
\textbf{The trace of $\bm A$ is the sum of its eigenvalues}\\
Look again at the characteristic polynomial. See that it can be rewritten as
\begin{equation*}
p(\lambda)=(-1)^n(\lambda^n-(\lambda_1+\cdots+\lambda_n)\lambda^{n-1}+\cdots)
\end{equation*}
Looking at the determinant of interest $\text{det}(\bm A-\lambda\bm I)$, see that, in a similar manner to the previous part, the main diagonal is the only permutation that
contributes to the $\lambda^{n-1}$ coefficient in the result. Consider the laplace expansion along a row $i$, we see that any term involving an off diagonal element 
$[\bm A-\lambda\bm I]_{ij}$ eliminates $a_{ii}-\lambda$ and $a_{jj}-\lambda$ from the main diagonal (also see that any permutation with an off diagonal entry (most of them) 
would have to be missing at least two elements on the main diagonal).\\
\vspace{1mm}\\
Therefore the coefficient for $\lambda^{n-1}$ comes solely from the main diagonal:
\begin{equation*}
(a_{11}-\lambda)(a_{22}-\lambda)\cdots(a_{nn}-\lambda)=\cdots+(-1)^{n-1}(a_{11}+\cdots+a_{nn})\lambda^{n-1}+\cdots
\end{equation*}
Comparing coefficients with our characteristic polynomial we get the result
\begin{equation*}
a_{11}+\cdots+a_{nn}=\text{tr}(\bm A)=\lambda_1+\cdots+\lambda_n
\end{equation*}
\textbf{In short}
\begin{center}
\includegraphics[width=12cm]{113}
\end{center}
\newpage

\section{Commuting matrices share eigenvectors}
Say we have two matrices that commute:
\begin{equation*}
\bm{AB}=\bm{BA}
\end{equation*}
Starting from $\bm{Ax}=\lambda\bm x$, we have
\begin{equation*}
\bm{AB}\bm x=\bm{BA}\bm x=\bm B\lambda\bm x=\lambda\bm{Bx}
\end{equation*}
So $\bm x$ and $\bm{Bx}$ are both eigenvectors of $\bm A$ with the same eigenvalue.\\
\vspace{1mm}\\
If we make a further assumption that the eigenvalues of $\bm A$ are distinct---the eigenspaces of $\bm A$ are one dimensional---then $\bm{Bx}$ must also be a multiple of 
$\bm x$, making $\bm x$ also an eigenvector of $\bm B$.

\section{$\bm{AB}$ and $\bm{BA}$ have the same eigenvalues}
If $\bm v$ is an eigenvector of $\bm{AB}$ with some nonzero eigenvalue $\lambda$, then $\bm{Bv}\neq0$ and
\begin{equation*}
\lambda\bm{Bv}=\bm B(\bm{ABv})=(\bm{BA})\bm{Bv}
\end{equation*}
so $\bm{Bv}$ is an eigenvector for $\bm{BA}$ with the same eigenvalue.\\
\vspace{1mm}\\
For the case of a zero eigenvalue, if 0 is an eigenvalue of $\bm{AB}$, then the matrix is noninvertible and
\begin{equation*}
0=\text{det}(\bm{AB})=\text{det}(\bm{A})\text{det}(\bm{B})=\text{det}(\bm{BA})
\end{equation*}
so 0 will also be an eigenvalue of $\bm{BA}$.
\newpage

\section{Diagonalisation}
Suppose the $n$ by $n$ matrix $\bm A$ has $\bm n$ linearly independent eigenvectors $\bm x_1,\ldots,\bm x_n$. Placing those $\bm x_i$ into the columns of an invertible
\textit{eigenvector matrix} $\bm X$, then $\bm X^{-1}\bm{AX}$ is the diagonal \textit{eigenvalue matrix} $\bm\Lambda$:
\begin{center}
\includegraphics[width=10cm]{114}
\end{center}
To understand this, see that $\bm{AX}=\bm{X\Lambda}$:
\begin{center}
\includegraphics[width=11cm]{115}\\
\includegraphics[width=12cm]{116}
\end{center}
Assuming that $n$ by $n$ $\bm X$ consists of $n$ linearly independent eigenvectors, it then has an inverse, and so
\begin{center}
\includegraphics[width=9cm]{117}
\end{center}
One benefit of diagonalisation is that it becomes easy to compute matrix powers, $\bm A^k$:
\begin{center}
\includegraphics[width=9cm]{118}
\end{center}
(next page)\newpage
\noindent\textbf{Diagonalisability}\\
We have a condition that guarantees diagonalisability: \textit{Eigenvectors for $n$ different $\lambda$'s are independent}---if the eigenvectors are distinct then we can
diagonalise $\bm A$:
\begin{center}
\includegraphics[width=11cm]{119}
\end{center}
\textit{Proof:} Suppose $c_1\bm x_1+c_2\bm x_2=\bm 0$ (suppose two eigenvectors with different eigenvalues are dependent) 
Multiply the entire expression by $\bm A$:
\begin{equation*}
\bm A(c_1\bm x_1+c_2\bm x_2)=c_1\lambda_1\bm x_1+c_2\lambda_2\bm x_2=\bm 0
\end{equation*}
now consider the first expression multiplied by $\lambda_2$
\begin{equation*}
\lambda_2(c_1\bm x_1+c_2\bm x_2)=c_1\lambda_2\bm x_1+c_2\lambda_2\bm x_2=\bm 0
\end{equation*}
subtracting the two expressions gives us
\begin{equation*}
c_1\lambda_1\bm x_1-c_1\lambda_2\bm x_1=(\lambda_1-\lambda_2)c_1\bm x_1=\bm 0
\end{equation*}
Since $\lambda_1\neq\lambda_2$ and $\bm x_1\neq\bm0$, we must have
$c_1=0$. See that we can repeat the same process but multiply by $\lambda_1$ instead to show that we must also have $c_2=0$. Only the combination with $c_1=c_2=0$ gives
$c_1\bm x_1+c_2\bm x_2=\bm 0$---the eigenvectors $\bm x_1$ and $\bm x_2$ must be independent.\\
\vspace{1mm}\\
Generalising to $n$ eigenvectors: Suppose that
\begin{equation*}
c_1\bm x_1+\cdots+c_n\bm x_n=\bm 0
\end{equation*}
First multiply by $\bm A$:
\begin{equation*}
c_1\lambda_1\bm x_1+\cdots+c_n\lambda_n\bm x_n=\bm 0
\end{equation*}
then multiply that same first expression by $\lambda_n$:
\begin{equation*}
c_1\lambda_n\bm x_1+\cdots+c_n\lambda_n\bm x_n=\bm 0
\end{equation*}
and subtract the two expressions to get
\begin{align*}
&(\lambda_1-\lambda_n)c_1\bm x_1+\cdots+c_n(\lambda_n-\lambda_n)\bm x_n\\
=&(\lambda_1-\lambda_n)c_1\bm x_1+\cdots+c_{n-1}(\lambda_{n-1}-\lambda_n)\bm x_{n-1}=\bm 0
\end{align*}
Now take the result and multiply by $\bm A$ again. Subtract that same expression, but now multiplied by $\lambda_{n-1}$:
\begin{equation*}
(\lambda_1-\lambda_n)(\lambda_1-\lambda_{n-1})c_1\bm x_1+\cdots+c_{n-1}(\lambda_{n-1}-\lambda_n)(\lambda_{n-1}-\lambda_{n-1})\bm x_{n-1}=\bm 0
\end{equation*}
repeating this process, we eventually end up with
\begin{equation*}
(\lambda_1-\lambda_n)(\lambda_1-\lambda_{n-1})\cdots(\lambda_1-\lambda_2)c_1\bm x_1=\bm 0
\end{equation*}
(next page)\newpage
\noindent\textbf{Cont.}\\
We have
\begin{equation*}
(\lambda_1-\lambda_n)(\lambda_1-\lambda_{n-1})\cdots(\lambda_1-\lambda_2)c_1\bm x_1=\bm 0
\end{equation*}
Forcing $c_1=0$. We can repeat this same process to show that every $c_i=0$; when the $\lambda$ are different, $c_1\bm x_1+\ldots+c_n\bm x_n=\bm 0$ only when $c_i=0$, the 
eigenvectors are independent.\\
\vspace{1mm}\\
\textbf{Repeated eigenvalues don't imply non-diagonalisability}\\
Distinct eigenvalues guarantee diagonalisability, however this does not mean that repeated eigenvalues implies non-diagonalisability---if all the eigenvalues of a matrix are
distinct, then the matrix is automatically diagonalisable, but there are plenty of cases where a matrix is diagonalisable, but has repeated eigenvalues.
\newpage

\section{Similar Matrices}
Suppose the eigenvalue matrix $\bm\Lambda$ is fixed. As we change the eigenvector matrix $\bm X$, we get a whole family of different matrices 
$\bm A=\bm{X\Lambda X}^{-1}$---\textit{all with the same eigenvalues in} $\bm\Lambda$. All those matrices $\bm A$ (with the same 
$\bm\Lambda$) are called \textit{similar}.\\
\vspace{1mm}\\
This idea extends to matrices that \textit{can't be diagonalised}.
We choose one constant matrix $\bm C$ (not necessarily $\bm\Lambda$), and we look at the whole family of matrices 
$\bm A=\bm{BCB}^{-1}$, allowing all invertible matrices $\bm B$. It turns out that the matrices $\bm C$ and $\bm A$ are 
similar---they have the same $n$ eigenvalues.\\
\vspace{1mm}\\
We say $\bm C$ instead of $\bm\Lambda$ because $\bm C$ need not be diagonal. We only require invertible $\bm B$.
\begin{center}
\includegraphics[width=12cm]{131}
\end{center}
To show this, see that if $\bm{Cx}=\lambda\bm x$, then $\bm{BCB}^{-1}$ has the same eigenvalue $\lambda$ with new eigenvector $\bm{Bx}$:
\begin{center}
\includegraphics[width=11cm]{132}
\end{center}
A fixed matrix $\bm C$ produces a family of similar matrices $\bm{BCB}^{-1}$.
\newpage

\section{Matrix Powers}
The eigenvector matrix $\bm X$ allows us to diagonalise $\bm A$ as
\begin{equation*}
\bm A=\bm{X\Lambda X}^{-1}
\end{equation*}
This factorisation is particularly useful when computing powers:
\begin{center}
\includegraphics[width=11cm]{133}
\end{center}
We can intuitively understand $\bm{X\Lambda}^k\bm X^{-1}\bm u_0$. The expression can be constructed as follows:
\begin{center}
\includegraphics[width=12cm]{134}
\end{center}
We find the initial linear combination of eigenvectors that produces $\bm u_0$, whose coefficients we denote as $\bm c$. 
Each time we multiply by $\bm A$, 
we scale each eigenvector by its corresponding eigenvalue, so the result of $\bm{Au}_0$ is just that linear combination with coefficients 
$\bm\Lambda\bm c$. We scale each coefficient as many times as we like by its corresponding 
eigenvalue, $\bm\Lambda^k\bm c$; evaluating the final linear combination is precisely our factorisation. That is,
\begin{center}
\includegraphics[width=11cm]{135}
\end{center}
is the same as
\begin{equation*}
\bm A^k\bm u_0=\bm u_k=c_1(\lambda_1)^k\bm x_1+\cdots+c_n(\lambda_n)^k\bm x_n
\end{equation*}
which is a result of starting with $\bm u_0$
\begin{equation*}
u_0=c_1\bm x_1+\cdots+c_n\bm x_n
\end{equation*}
and applying $\bm A$ repeatedly
\begin{equation*}
\bm{Au}_0=u_1=c_1(\lambda_1)\bm x_1+\cdots+c_n(\lambda_n)\bm x_n
\end{equation*}
\newpage

\section{Inutition for spectral theorem}
\subsection{Symmetric matrices are always diagonalisable}
If $\bm A$ is an $n$x$n$ symmetric matrix with eigenvector $\bm v$, and $\bm u\in\mathbb{C}$ is such that $\bm u\perp\bm v$, then 
$\bm{Au}\perp\bm v$. We can show this:
\begin{equation*}
\bm v\cdot(\bm{Au})=\bm v^T\bm{Au}=\bm v^T\bm A^T\bm u=(\bm v^T\bm A^T)\bm u=(\bm A\bm v)^T\bm u
=\lambda\bm v^T\bm u=0
\end{equation*}
where $\lambda$ is the eigenvalue of $\bm v$.\\
\vspace{1mm}\\
Let $\{\bm v_1,\ldots,\bm v_k\}$ be any set of $k$ linearly independent eigenvectors of $\bm A$. Let $\bm V\subseteq\mathbb C^n$ be the vector
space spanned by these vectors, and $\bm V^\perp\subseteq\mathbb C^n$ be the orthogonal space, containing all vectors orthogonal to $\bm V$.\\
\vspace{1mm}\\
Every vector $\bm u\in\bm V^\perp$ is orthogonal to each $\bm v_i$. By ourfirst statement, so is $\bm{Au}$. Therefore, $\bm V^\perp$ is fixed under
$\bm A$; we can make a restricted linear transformation $\bm A|_{\bm V^\perp}:\bm V^\perp\to\bm V^\perp$.\\
\vspace{1mm}\\
Every linear transformation over a nonzero, finite dimensional space of an algebraically closed field has at least one eigenvector (this can be 
shown using the fundamental theorem of algebra). If $\bm V^\perp\neq\{\bm 0\}$, then there is another eigenvector $\bm v_{k+1}\in\bm V^\perp$ of
$\bm A$, \textit{orthogonal to} (and linearly independent from) $\{\bm v_1,\ldots\bm v_k\}$.\\
\vspace{1mm}\\
By induction, we can continue to extend $\bm V$ by more eigenvectors until $\bm V=\mathbb C^n$. At this point we then have
$n$ linearly independent eigenvectors of $\bm A$, providing a basis for its diagonalisation.

\subsection{Eigenvectors of distinct eigenvalues of a symmetric matrix are orthogonal}
For any real matrix $\bm A$ and any vectors $\bm x$ and $\bm y$, we have
\begin{equation*}
\langle\bm{Ax},\bm y\rangle=\langle\bm x,\bm A^T\bm y\rangle
\end{equation*}
Now assuming that $\bm A$ is symmetric, and that $\bm x$ and $\bm y$ are eigenvectors of $\bm A$ corresponding to distinct eigenvalues $\lambda$
and $\mu$. Then 
\begin{align*}
\lambda\langle\bm x,\bm y\rangle=\langle\lambda\bm x,\bm y\rangle
=\langle\bm A\bm x,\bm y\rangle&=\langle\bm x,\bm A^T\bm y\rangle
=\langle\bm x,\bm A\bm y\rangle=\langle\bm x,\mu\bm y\rangle\\
=&\mu\langle\bm x,\bm y\rangle
\end{align*}
Therefore,
\begin{equation*}
(\lambda-\mu)\langle\bm x,\bm y\rangle=0
\end{equation*}
since $\lambda-\mu\neq0$, then $\langle\bm x,\bm y\rangle=0$, and $\bm x\perp\bm y$.
\newpage

\section{Symmetric Positive Definite Matrices}
\subsection{Equivalence of definitions}
A symmetric matrix is \textit{positive definite} if the energy $\bm x^T\bm{Sx}$ is positive for all vectors $\bm x\neq\bm0$.\\
\vspace{1mm}\\
Another definition: A symmetric matrix is positive definite if all its eigenvalues are positive.\\
\vspace{1mm}\\
We can show that the two definitions are equivalent:
\begin{equation*}
\bm{Sx}=\lambda\bm x\implies\bm x^T\bm{Sx}=\lambda\bm x^T\bm x
\end{equation*}
So $\lambda>0$ leads to energy $\bm x^T\bm{Sx}>0$. If the eigenvalues are all positive, then the energy definition holds when $\bm x$ 
are eigenvectors.\\
\vspace{1mm}\\
We can show further that
if $\bm x^T\bm{Sx}>0$ for the eigenvectors of $\bm S$, then $x^T\bm{Sx}>0$ for every nonzero vector $\bm x$. Recall that symmetric matrices
are always diagonalisable, and that their eigenvectors can be chosen orthogonal. As such, every $\bm x$ is a combination 
$c_1\bm x_1+\cdots+c_n\bm x_n$ of the eigenvectors, and we can show
\begin{align*}
\bm x^T\bm{Sx}&=(c_1\bm x_1^T+\cdots+c_n\bm x_n^T)\,\bm S\,(c_1\bm x_1+\cdots+c_n\bm x_n)\\
&=(c_1\bm x_1^T+\cdots+c_n\bm x_n^T)(c_1\lambda_1\bm x_1+\cdots+c_n\lambda_n\bm x_n)\\
&=(c_1^2\lambda_1\bm x_1^T\bm x_1+\cdots+c_n^2\lambda_n\bm x_n^T\bm x_n)
\end{align*}
(In the last step we used the orthogonality of the eigenvectors of $\bm S$, so $\bm x_i^t\bm x_j=0$ for $i\neq j$) See that the entire final 
expression is $>0$ if every $\lambda_i>0$.

\subsection{If $\bm S_1$ and $\bm S_2$ are symmetric positive definite, so is $\bm S_1+\bm S_2$}
We can prove this simply adding energies:
\begin{equation*}
\bm x^T(\bm S_1+\bm S_2)\bm x=\bm x^T\bm S_1\bm x+\bm x^T\bm S_2\bm x
\end{equation*}
If both $\bm S_1$ and $\bm S_2$ are positive definite, so is their sum.
\newpage

\section{Cholesky decomposition}
Let $\bm A$ be a $K$ by $K$ matrix. We say that $\bm A$ possesses a cholesky decomposition if and only if there exists a lower triangular 
$K$ by $K$ matrix $\bm L$ such that its diagonal entries are strictly positive real numbers and 
\begin{equation*}
\bm A=\bm{LL}^*
\end{equation*}
Where $\bm L^*$ denotes the conjugate transpose of $\bm L$. When $\bm L$ is real, then the conjugate transpose $\bm L^*$ coincides with the 
transpose $\bm L^T$ and the cholesky factorisation is
\begin{equation*}
\bm A=\bm{LL}^T
\end{equation*}
When we restrict our attention to real vectors and matrices, then we say that a real matrix $\bm A$ is positive definite if and only if $\bm A$ 
is symmetric and 
\begin{equation*}
\bm x^T\bm{Ax}>0
\end{equation*}
for any non-zero real vector $\bm x$.\\
\vspace{1mm}\\
We can show that a square matrix possesses a cholesky factorisation if and only if it is positive definite. Since a positive definite matrix $\bm A$
is Hermitian ($\bm A=\bm A^*$), it can be diagonalised as
\begin{equation*}
\bm A=\bm P^*\bm{DP}
\end{equation*}
Where $\bm P$ is a unitary matrix (orthogonal but allowed to be complex) and $\bm D$ is a diagonal matrix with the eigenvalues of $\bm A$ on its 
main diagonal. Since $\bm A$ is positive definite, its eigenvalues are 
\textit{strictly positive numbers}. Thus we can write
\begin{equation*}
\bm D=\bm D^{1/2}\bm D^{1/2}
\end{equation*}
where $\bm D^{1/2}$ is a diagonal matrix such that its $(k,k)$-th entry satisfies
\begin{equation*}
(\bm D^{1/2})_{kk}=\sqrt{\bm D_{kk}}
\end{equation*}
for $\bm k=1,\ldots,K$. Therefore,
\begin{align*}
\bm A&=\bm P^*\bm{DP}\\
&=\bm P^*\bm D^{1/2}\bm D^{1/2}\bm P\\
&=(\bm D^{1/2}\bm P)^*\bm D^{1/2}\bm P
\end{align*}
The matrix $\bm P$ is unitary and therefore full rank. The matrix $\bm D^{1/2}$ is diagonal with strictly positive entries---and is therefore also
full-rank. The product of two full rank matrices is full rank, and so 
$\bm D^{1/2}\bm P$ is full-rank.\\
(next page)\newpage
\noindent\textbf{Cont.}\\
$\bm D^{1/2}\bm P$ is full-rank. It therefore has a $\bm{QR}$ decomposition (discussed earlier):
\begin{equation*}
\bm D^{1/2}\bm P=\bm{QR}
\end{equation*}
where $\bm Q$ is a unitary matrix and $\bm R$ is upper triangular \textit{with strictly positive entries on its main diagonal}. Thus we have
\begin{align*}
\bm A&=(\bm D^{1/2}\bm P)^*\bm D^{1/2}\bm P\\
&=(\bm{QR})^*\bm{QR}\\
&=\bm R^*\bm Q^*\bm{QR}\\
&=\bm R^*\bm R\\
&=\bm{LL}^*
\end{align*}
This shows that the all positive definite matrices have cholesky factorisations.\\
\vspace{1mm}\\
Say that some $\bm A$ has a cholesky decomposition. Since the diagonal entries of $\bm L$ are strictly positive, $\bm L$ and $\bm L^*$ are
full-rank. Therefore for any $\bm x\neq\bm 0$, we have
\begin{equation*}
\bm L^*\bm x\neq\bm0
\end{equation*}
which then satisfies
\begin{equation*}
\bm x^*\bm A\bm x=\bm x^*\bm L\bm L^*\bm x=||L^*\bm x||^2\geq0
\end{equation*}
for any $\bm x\neq\bm0$. This shows that any matrix with a cholesky factorisation is positive definite.\\
\vspace{1mm}\\
\textbf{Uniqueness}\\
We can show that the cholesky factorisation for any positive definite $\bm A$ is unique. Given that we have
\begin{equation*}
\bm A=\bm{LL}^*
\end{equation*}
Suppose that there exists another decomposition
\begin{equation*}
\bm{A}=\bm{MM}^*
\end{equation*}
Then we have
\begin{equation*}
\bm{LL}^*=\bm{MM}^*
\end{equation*}
and
\begin{equation*}
\bm M^{-1}\bm L=\bm M^*(\bm L^*)^{-1}
\end{equation*}
where the existence of the inverses $(\bm L^*)^{-1}$ and $\bm M^{-1}$ are guaranteed by the fact that $\bm L$ and $\bm M$ are triangular with 
strictly positive diagonal entries.\\
(next page)\newpage
\noindent\textbf{Cont.}\\
We had
\begin{equation*}
\bm M^{-1}\bm L=\bm M^*(\bm L^*)^{-1}
\end{equation*}
Since $\bm M$ and $\bm L$ are lower triangular, $\bm M^{-1}\bm L$ is lower triangular. Since $\bm M^*$ and $\bm L^*$ are upper triangular,
$\bm M^*(\bm L^*)^{-1}$ is upper triangular.\\
\vspace{1mm}\\
The lower triangular $\bm M^{-1}\bm L$ can be equal to the upper triangular $\bm M^*(\bm L^*)^{-1}$ only if both matrices are diagonal. 
Therefore,
\begin{equation*}
\bm M^{-1}\bm L=\bm D=\bm M^*(\bm L^*)^{-1}
\end{equation*}
where $\bm D$ is a diagonal matrix. Now see that
\begin{align*}
(\bm D^*)^{-1}&=((\bm M^*(\bm L^*)^{-1})^*)^{-1}\\
&=(\bm L^{-1}\bm M)^{-1}\\
&=\bm M^{-1}\bm L=\bm D
\end{align*}
and so we can then write
\begin{equation*}
\bm{DD}^*=(\bm D^*)^{-1}\bm D^*=\bm I
\end{equation*}
This means that any diagonal entry of $\bm D$, which we can denote as $\bm D_{kk}$, satisfies
\begin{equation*}
\bm D_{kk}\overline{\bm D_{kk}}=|\bm D_{kk}|^2=1
\end{equation*}
See that this means that the diagonal entries of $\bm D$ are all located on the unit circle (the one with the real and complex axes).\\
\vspace{1mm}\\
Moreover, $\bm D$ needs to satisfy the constraint
\begin{equation*}
\bm L=\bm{MD}
\end{equation*}
where the diagonal entries of both $\bm M$ and $\bm L$ are \textit{real and strictly positive}. The only way to satisfy this constraint 
while remaining on the unit circle is to have
\begin{equation*}
\bm D_{kk}=1
\end{equation*}
for all $k$. Therefore, $\bm D=\bm I$, and 
\begin{equation*}
\bm L=\bm M
\end{equation*}
\newpage

\section{Other tests for positive definiteness}
\subsection{$\bm S=\bm A^T\bm A$}
A symmetric matrix $\bm S$ is positive definite if
\begin{equation*}
\bm S=\bm A^T\bm A\text{ for some matrix $\bm A$ with independent columns}
\end{equation*}
See that if $\bm S=\bm A^T\bm A$, then
\begin{equation*}
\bm x^T\bm{Sx}=\bm x^T\bm A^T\bm A\bm x=(\bm A\bm x)^T(\bm A\bm x)
=||\bm{Ax}||^2
\end{equation*}
See that the energy is positive (for $\bm x\neq\bm 0$) provided that $\bm{Ax}$ is not the zero vector. To assure that $\bm{Ax}\neq\bm0$ when 
$\bm x\neq\bm0$, the columns of $\bm A$ must be independent.\\
\vspace{1mm}\\
If the columns are not independent, then there will be a nontrivial nullspace and $\bm{Ax}=\bm0$ for some $\bm x$, in which case the energy 
can then be zero, but \textit{will still never be negative}, meaning
\begin{equation*}
\bm x^T\bm{Sx}\geq0
\end{equation*}
As such, we can conclude that $\bm S=\bm A^T\bm A$ is at least semidefinite: $\bm x^T\bm{Sx}=||\bm{Ax}||^2$ will be $>0$ if the columns
of $\bm A$ are independent (positive definite), or it will be $\geq0$ otherwise (positive semidefinite).
\newpage

\subsection{$\bm{LDL}^T$ factorisation and positive pivots}
\textbf{$\bm{LDL}^T$ factorisation}\\
Assuming now row exchanges are needed, we have Gaussian elimination as $\bm A=\bm{LU}$ (instead of $\bm{PA}=\bm{LU}$). Looking at the upper triangular $\bm U$, see that
we can factor out the pivots of each row separately into a diagonal matrix $\bm D$, leaaving 1's along the diagonal of the factored $\bm U_1$:
\begin{equation*}
\bm{LU}=\bm L\bm{DU}_1
\end{equation*}
If $\bm A$ were symmetric, this implies
\begin{equation*}
\bm{LDU}_1=\bm U_1^T\bm D\bm L^T
\end{equation*}
assuming that $\bm U$ and $\bm L$ are invertible, then we have
\begin{equation*}
(\bm U_1^T)^{-1}\bm{LD}=\bm D\bm L^T\bm U_1^{-1}
\end{equation*}
Notice that the matrix on the left hand side is lower triangular, while that on the right hand side is upper triangular. The only way they can be equal is if both
sides are diagonal.\\
\vspace{1mm}\\
$\bm D$ is diagonal, both $(\bm U_1^T)^{-1}\bm{L}$ and $\bm L^T\bm U_1^{-1}$ must be diagonal. Finally, because there are only 1's on the diagonals of both expressions, they
must both be the identity. Therefore,
\begin{equation*}
\bm L=\bm U_1^T,\quad\bm L^T=\bm U_1
\end{equation*}
and our original factorisation can be written as
\begin{equation*}
\bm A=\bm{LDL}^T
\end{equation*}
\textbf{Positive pivots as a test for positive definiteness}\\
Every positive definite matrix $\bm S$ has a unique cholesky factorisation
\begin{equation*}
\bm S=\bm{AA}^T
\end{equation*}
Where $\bm A$ has real positive diagonal entries. Working backward, we can factor out the (positive) diagonal entries of $\bm A$, leaving 1's on the diagonals:
\begin{equation*}
\bm S=\bm{LD}'\bm D'\bm L^T=\bm{LD}\bm L^T
\end{equation*}
Where $\bm D=\bm D'\bm D'$. See that $\bm D$ (the pivots) must have positive entries.\\
\vspace{1mm}\\
Since the cholesky decomposition is unique, this factorisation is also unique. See that this means that a positive definite matrix will have \textit{positive pivots}, and
that elimination of a positive definite matrix \textit{will not require row exchanges}.
\newpage

\section{}




\appendix
\chapter{Misc. topics}
\section{Taylor series, Difference approximations}
\textbf{Taylor series}\\
Recall the idea of the taylor series, where $f(x)$ near some point $x=x_0$ can be approximated as
\begin{equation*}
f(x)\approx f(x_0)+f'(x_0)(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2+\frac{f'''(x_0)}{3!}(x-x_0)^3+\cdots
\end{equation*}
See that this comes from approximating $f(x)$ as a polynomial
\begin{equation*}
y(x)=a_0+a_1(x-x_0)+a_2(x-x_0)^2+a_3(x-x_0)^3+\cdots
\end{equation*}
where we have $a_0=y(x_0)$; now consider differentiating:
\begin{align*}
y'(x)&=a_1+2a_2(x-x_0)+3a_3(x-x_0)^2+\cdots\\
y''(x)&=2a_2+(3)(2)a_3(x-x_0)+(4)(3)a_4(x-x_0)^2\cdots\\
y^{(n)}(x)&=n!\,a_n+((n+1)\cdot(n-1)\cdots3\cdot2)a_{n+1}(x-x_0)+\cdots
\end{align*}
so we have
\begin{equation*}
a_1=y'(x_0),\quad a_2=\frac{y''(x_0)}{2!},\,\cdots,a_n=\frac{y^{(n)}(x_0)}{n!}
\end{equation*}
where substituting into our initial approximation gives us
\begin{equation*}
y(x)\approx y(x_0)+y'(x_0)(x-x_0)+\frac{y''(x_0)}{2!}(x-x_0)^2+\cdots+\frac{y^{(n)}(x_0)}{n!}(x-x_0)^n+\cdots
\end{equation*}
(next page)\newpage
\noindent\textbf{Cont.}\\
We had
\begin{equation*}
y(x)\approx y(x_0)+y'(x_0)(x-x_0)+\frac{y''(x_0)}{2!}(x-x_0)^2+\cdots+\frac{y^{(n)}(x_0)}{n!}(x-x_0)^n+\cdots
\end{equation*}
substituting $x-x_0=h$ gives us
\begin{equation*}
y(x_0+h)\approx y(x_0)+y'(x_0)(h)+\frac{y''(x_0)}{2!}(h)^2+\cdots+\frac{y^{(n)}(x_0)}{n!}(h)^n+\cdots
\end{equation*}
Which can be rewritten as
\begin{equation*}
y(x+h)\approx y(x)+hy'(x)+\frac{1}{2!}h^2y''(x)+\cdots+\frac{1}{n!}h^ny^{(n)}(x)+\cdots
\end{equation*}
\vspace{1mm}\\
\textbf{Difference formulas}\\
Considering the first few terms of the taylor series approximation, we have
\begin{align*}
y(x+h)-y(x)&\approx hy'(x)+\frac{1}{2}h^2y''(x)\\
y(x-h)-y(x)&\approx-hy'(x)+\frac{1}{2}h^2y''(x)
\end{align*}
This allows us to have an approximation to $dy/dx$:
\begin{center}
\includegraphics[width=10cm]{22}
\end{center}
and also to the second derivative
\begin{center}
\includegraphics[width=10cm]{23}
\end{center}
The individual formulas also yield approximations:
\begin{center}
\includegraphics[width=10cm]{24}
\end{center}
\newpage

\section{Conjugate/Hermitian transpose}
The conjugate transpose of a matrix $\bm A$ is the matrix $\bm A^*$
defined by
\begin{equation*}
\bm A^*=\bar{\bm{A}}^T=\bar{\bm{A}^T}
\end{equation*}
Where $T$ denotes transposition and the bar denotes complex conjugation---transpose, and complex conjugation of all entries. The order in which the
transposition and conjugation are performed is irrelevant.\\
\vspace{1mm}\\
\textbf{Properties}\\
The properties of conjugate transpostition are immediate consequences of the properties of transposition and conjugation. First, recall that
for two complex numbers $x=(a+ib)$ and $y=(c+id)$:
\begin{equation*}
\bar{(xy)}=\bar x \bar y
\end{equation*}
which can be shown:
\begin{equation*}
xy=(a+ib)(c+id)=ac-bd+i(ad+bc)
\end{equation*}
and
\begin{equation*}
\bar x\bar y=(a-ib)(c-id)=ac-bd-i(ad+bc)=\bar{(xy)}
\end{equation*}
with that we list some properties of conjugate transpositions:
\begin{itemize}
\item $(\bm A^*)^*=\bm A$
\item $(\bm A+\bm B)^*=\bm A^*+\bm B^*$
\item $(z\bm A)^*=\bar z\bm A^*$
\item $(\bm{AB})^*=\bm B^*\bm A^*$
\item $(\bm A^*)^{-1}=(\bm A^{-1})^*$ (provided $\bm A$ is square invertible)
\end{itemize}
The proof of the final point is analagous to the proof of $(\bm{A})^T)^{-1}=(\bm{A}^{-1})^T$.\\
(next page)\newpage
\noindent\textbf{Hermitian matrix}\\
A matrix that is equal to its conjugate transpose is called \textit{Hermitian} or \textit{self-adjoint}. In other words, $\bm A$ is Hermitian if 
and only if
\begin{equation*}
\bm A^*=\bm A
\end{equation*}
Denoting $\bm A_{kl}$ as the $(k,l)$-th entry of $\bm A$ and $(\bm A^*)_{kl}$ the $(k,l)$-th entry of $\bm A^*$, we have
\begin{equation*}
(\bm A^*)_{kl}=\overline{\bm A_{lk}}
\end{equation*}
Therefore, $\bm A$ is Hermitian if and only if
\begin{equation*}
\bm A_{kl}=\overline{\bm A_{lk}}
\end{equation*}
for all $k$ and $l$. See that this implies that the \textit{diagonal entries of $\bm A$ must be real} in order to satisfy
\begin{equation*}
\bm A_{kk}=\overline{\bm A_{kk}}
\end{equation*}
\newpage

\section{}



\end{document}
