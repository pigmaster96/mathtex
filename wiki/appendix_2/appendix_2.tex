\documentclass{report} 
\title{Appendix 2}
\date{Started 24 May 2024}
\author{Malcolm}
\usepackage{amsmath} %import math
\usepackage{mathtools} %more math
\usepackage{amssymb} %for QED symbol
\usepackage{amsthm} %
\usepackage{bm}%bold math
\usepackage{graphicx} %import imaging
\graphicspath{{./images/}} %set imaging path
\begin{document}
\maketitle

\tableofcontents

\appendix
\chapter{Differential Equations}
\section{First Order Differential Equations}
\subsection{Introduction to Ordinary Differential Equations\\(ODEs)} %030624
Here we introduce intuition for  Ordinary Differential Equations (ODEs) and introductory solving methods.\\
\vspace{1mm}\\
The simplest type of differential equation looks like:
\begin{equation*}
\frac{dy}{dx}=f(x)
\end{equation*}
which can be solved by the antiderivative $y=\int f(x)\,dx$. \\
\vspace{1mm}\\
\textbf{Intuition}\\
Now we consider a more interesting example: 
\begin{equation*}
\frac{dy}{dx}+xy=0
\end{equation*}
This equation can be solved by \textit{separation of variables}:
\begin{align*}
\frac{dy}{dx}+xy&=0\\
\frac{dy}{dx}&=-xy\\
\frac{dy}{y}&=-x\,dx
\end{align*}\\
(next page)\\
Since the problem is now set up in terms of differentials rather than ratios of differentials,
we can integrate both sides.
\begin{align*}
\int\frac{dy}{y}&=-\int x\,dx\\
\ln y+c_1&=-\frac{x^2}{2}+c_2\quad\text{(assume $y>0$)}
\end{align*}
We can combine the constants and simplify:
\begin{align*}
\ln y&=-\frac{x^2}{2}+c\\
e^{\ln y}&=e^{-x^2/2+c}\\
y&=e^ce^{-x^2/2}\\
y&=Ae^{-x^2/2},\quad\text{(where $A=e^c$})
\end{align*}
(The more apt $\ln|y|$ simplifies to $\pm Ae^{-x^2/2}$, which doesn't matter since $A$ is some
unspecific constant)\\
\vspace{1mm}\\
It turns out that our solution,
\begin{equation*}
y=Ae^{-x^2/2},\quad\text{(where $A=e^c$})
\end{equation*}
Works for any constant multiple $A$. We can check this 
solution:
\begin{align*}
y&=ae^{-x^2/2}\\
\frac{dy}{dx}&=\frac{d}{dx}ae^{-x^2/2}\\
&=a\cdot(-x)e^{-x^2/2}\\
&=-x\cdot ae^{-x^2/2}\\
\frac{dy}{dx}&=-xy
\end{align*}
$A$ is determined by an initial condition; for instance if $y(0)=1$, $A=1$.
\newpage

\subsection{Separation of Variables} %030524
Here we describe a rudimentary method for solving some differential equations---\\Separation of Variables.\\
\vspace{1mm}\\
In general, this method applies to differential equations of the form
\begin{equation*}
\frac{dy}{dx}=f(x)g(y)
\end{equation*}
Where we then \textit{separate} the variables and integrate:
\begin{align*}
\frac{dy}{dx}&=f(x)g(y)\\
\frac{dy}{g(y)}&=f(x)\,dx\\
h(y)\,dy&=f(x)\,dx\quad\text{where }h(y)=\frac{1}{g(y)}\\
\int h(y)\,dy&=\int f(x)\,dx\\
\end{align*}
Antidifferentiating both sides:
\begin{equation*}
H(y)=\int h(y)\,dy;\quad F(x)=\int f(x)\,dx
\end{equation*}
we now have
\begin{align*}
H(y)+c_1&=F(x)+c_2\\
H(y)&=F(x)+c
\end{align*}
\newpage

\subsection{Direction fields, Isoclines, and Integral curves}%131024
\textbf{Direction fields}\\
Given an equation $y'=f(x,y)$, we can construct a \textit{direction field}; imagine through each point $(x,y)$, we
draw a line segment whose slope is $f(x,y)$---consider $y'(x)=2x$:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{1}\\
\end{center}
(note that in this case $f(x,y)$ does not depend on $y$ (because of the equation)---it is invariant under vertical
translation)
\end{figure}\\
\textbf{Plotting direction fields---Isoclines}\\
In practice, computers are used to plot direction fields following the procedure:
\begin{enumerate}
\item Pick point $(x,y)$
\item Compute $y'=f(x,y)$
\item Plot line segment of slope at that point
\end{enumerate}
Notice how a new slope has to be computed for each specified point; when plotting direction fields by hand, 
it is much more practical to utilise \textit{isoclines}, which are, given the equation $y'=f(x,y)$, a one-parameter
family of curves given by the equations
\begin{equation*}
f(x,y)=m,\quad m\text{ constant}
\end{equation*}
Along a given isocline, all line segments have the same slope $m$.\\
(next page)
\newpage
\noindent\textbf{Example}\\
Consider plotting the direction field for the equation
$y'=x-y$; the isoclines are correspondingly the lines
$x-y=m$ (shown in dashed lines):
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{2}\\
\end{center}
The $m=0$ isocline marks the points where the slope of the solution is 0; it is therefore of special interest 
and is called the \textit{nullcline}.
\end{figure}\\
\textbf{Integral curves}\\
As also shown in the figure above, once the direction field has been sketched, curves which are \textit{at each
point tangent to the line segment at that point} can be drawn; 
such curves are called \textit{integral curves} or \textit{solution curves} for the direction field.
Their significance (this should be obvious) is that
\begin{equation*}
\text{\textit{The integral curves are the graphs of the solutions to }}y'=f(x,y)
\end{equation*}
Two integral curves have been drawn above (in solid lines).\\
\vspace{1mm}\\
\textbf{Intersection Principle}\\
Intuitively, see that at any point in the direction field it can only have one direction; therefore it is fairly
obvious that integral curves cannot cross at an angle.\\
\vspace{1mm}\\
Consider the existence and uniqueness theorem for ODEs: 
\begin{align*}
&\text{\textit{For any $(a,b)$ in the region where $f$ is defined, $y'=f(x,y)$ has}}\\&\text{\textit{exactly one solution such that 
$y(a)=b$.}}
\end{align*}
by the existence part of the theorem, 
there is an integral curve through any point where $f(x,y)$ is defined. Now supposing two integral curves
through the same point, by the uniqueness part of the theorem they must agree.\\
\vspace{1mm}\\
As a result, \textit{integral curves cannot intersect}; every point lies on exactly one integral curve.
\newpage

\subsection{Long term Behaviour: Fences, Funnels, and Separatrices}%140824
\textbf{Fences}\\
A \textit{lower fence} for the equation $y'=f(x,y)$ is a curve that `blocks' an integral curve from crossing from
\textit{above}; intuitively it is the curve whose direction field elements along the curve point up from it. Technically 
it can be described as a curve $y=L(x)$ such that $L'(x)<f(x,L(x))$ (the slope of the curve is always 
less than the slope of the direction field at that point).\\
\vspace{1mm}\\
Likewise an \textit{upper fence} is a curve that `blocks' integral curves from crossing from \textit{above}. 
Illustrated:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{3}\\
\end{center}
(The upper curve is the upper fence and the lower curve is the lower fence). Solutions will be `squeezed' between
upper and lower fences.
\end{figure}\\
Note that
\begin{itemize}
\item Note that fences aren't necessarily defined for all $x$; they could be defined only on an interval like
$x\geq c$ for some constant $c$.
\item Since integral curves can't cross an integral curve itself it is both an upper and lower fence.
\end{itemize}
(next page)
\newpage
\noindent\textbf{Example}\\
Consider the direction field for the equation
\begin{equation*}
y'=y^2-x
\end{equation*}
The isoclines for $m=0$ and $m=-1$ are plotted in yellow, with integral curves in blue:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{4}\\
\end{center}
Notice that the bottom hald of the isocline $m=0$ is a lower fence and for $x$ large enough the bottom 
half of the isocline $m=-1$ is an upper fence. (notice that the $m=-1$ isocline becomes an upper
fence only for $x$ large enough)
\end{figure}\\
(next page)
\newpage
\noindent\textbf{Funnels}\\
One use of fences is to construct funnels. A \textit{funnel} for the equation $y'=f(x,y)$ consists of a pair of 
fences; one lower fence $L(x)$ and one upper fence $U(x)$ with the properties
\begin{enumerate}
\item For $x$ large the lower fence is below the upper fence; $L(x)<U(x)$
\item The two fences come together asymptotically; $U(x)-L(x)$ is small for large $x$
\end{enumerate}
For instance, in the above example the bottom parts of the two isoclines $m=0$ and $m=-1$ act as a funnel
once $x$ is large enough. Given the equations of each isocline we have highly accurate estimates for solutions
between them as
\begin{equation*}
\underbrace{-\sqrt{x}}_{m=0}<y(x)<\underbrace{-\sqrt{x-1}}_{m=-1}
\end{equation*}
which is valid for large $x$.\\
\vspace{1mm}\\
Note that not all pairs of upper/lower fences form a funnel---they have to come together asymptotically as $x$ 
gets large.\\
\vspace{1mm}\\
\textbf{Separatrices}\\
A \textit{separatrix} is an integral curve such that the integral curves above it behave entirely differently from
integral curves below it as $x\to\infty$.
\newpage

\subsection{Runge-Kutta 2 (Numerical methods)}%161024
\textbf{General approach and Euler's method}\\
Euler's method (for numerical estimation) follows a more general procedure for stepping from 
$(x_n,y_n)$ to $(x_{n+1},y_{n+1})$:
\begin{equation*}
x_{n+1}=x_n+h,\quad y_{n+1}=y_n+m_nh
\end{equation*}
Where $h$ is the stepsize in the $x$ direction and $m$ is the slope of the line we step along. In Euler's
method $h$ is fixed ahead of time and $m_n=f(x_n,y_n)$.\\
\vspace{1mm}\\
\textbf{Runge-Kutta 2}\\
Naturally Euler's method is a fairly flawed method of numerical estimation. Other methods use other (and better)
ways of choosing $h$ and $m$. Here I describe the \textit{Runge-Kutta 2} (RK2) method, which is a
\textit{fixed stepsize} method; meaning $h$ is fixed and the added complexity comes from finding $m$.\\
\vspace{1mm}\\
Given an initial value problem $y'=f(x,y),y(x_0)=x_0$ and a step size $h$, one step of the RK2 method is as follows:
\begin{enumerate}
\item Compute the slope $k_1$ at $(x_0,y_0)$: $k_1=f(x_0,y_0)$
\item `Take' an Euler step from $(x_0,y_0)$ to $(a,b):$ $a=x_0+h,$  $b=y_0+k_1h$
\item Compute the slope $k_2$ at $(a,b):$ $k_2=f(a,b)$
\item Average $k_1$ and $k_2$ to get $m$: $m=(k_1+k_2)/2$
\item Now we use this averaged slope to take a step from 
$(x_n,y_n)$ to $(x_{n+1},y_{n+1})$: 
\begin{equation*}
x_{1}=x_0+h,\quad y_{1}=y_0+mh;\quad m=\frac{(k_1+k_2)}{2}
\end{equation*}
\end{enumerate}
Other methods such as RK4 or \textit{variable stepsize methods} may (probably) work better. Though one might want
to consider computational efficiency at the expense of accuracy.
\newpage

\subsection{First order Linear Differential Equations}%181024
\textbf{Definition}\\
The general \textit{First order linear ODE} in the unknown function $x=x(t)$ has the form
\begin{equation*}
A(t)\frac{dx}{dt}+B(t)x(t)=C(t)
\end{equation*}
If $A(t)\neq0$ we can simplify the equation by dividing by $A(t)$:
\begin{equation*}
\frac{dx}{dt}+p(t)x(t)=q(t)
\end{equation*}
This is called the \textit{standard form} for a first order linear ODE. Should the \textit{coefficients} $A(t),B(t)$
be constants (not dependent on $t$) we say the equation is a \textit{constant coefficient} DE.\\
\vspace{1mm}\\
If $C(t)=0$:
\begin{equation*}
A(t)\frac{dx}{dt}+B(t)x(t)=0
\end{equation*}
The DE is called \textit{homogeneous} (notice that conversion to standard form doesn't change this fact); otherwise
the equation is \textit{inhomogeneous}.\\
\vspace{1mm}\\
\textbf{Signals and Systems---Terminology}\\
Given a differential equation
\begin{equation*}
\frac{dx}{dt}+p(t)x(t)=q(t)
\end{equation*}
Notice that the right-hand side does not depend on $x$. The left-hand side represents the \textit{system}
(think of it as defining the behaviour of a system); the
right-hand side represents an outside influence on the system, which we can call the \textit{input}.\\
\vspace{1mm}\\
In general, a signal is a function of $t$. The system \textit{responds} to the input signal and yields
the function $x(t)$, which we call the \textit{output signal} or \textit{system response}. (these terms should just
be seen as convenient convention when describing an ODE)\\
\vspace{1mm}\\
\textit{Block diagrams} can be used to visually represent systems:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{5}\\
\end{center}
(next page)
\end{figure}
\newpage
\noindent\textbf{Example---RC circuits}\\
Suppose we have an electrical circuit as shown
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{6}\\
\end{center}
``Kirchhoff's Voltage Law'' states that the total voltage change around the loop is 0, meaning
\begin{equation*}
V(t)=V_R(t)+V_C(t)
\end{equation*}
The relationship between voltage drop and current are described as follows:
\begin{align*}
&\text{Resistor: }V_R(t)=RI(t)\text{ for a constant }R,\text{ the ``resistance''}\\
&\text{Capacitor: }V'_C(t)=\frac{1}{C}I(t)\text{ for a constant }C,\text{ the ``capacitance''}
\end{align*}
the voltage drop from the capacitance can be seen from
the equation defining capacitance
\begin{align*}
q&=CV\quad\text{(charge per unit voltage)}\\
I(t)=\frac{dq}{dt}&=\frac{d}{dt}(CV)\\
I(t)&=CV'\quad\text{($C$ constant)}\\
V'_C(t)&=\frac{1}{C}I(t)
\end{align*}
The voltage drop across the capacitor is proportional to the \textit{integral} of the current; it results from
a buildup of charge on two plates of the capacitor.\\
(next page)
\end{figure}
\newpage
\noindent We can differentiate Kirchhoff's Voltage Law 
\begin{align*}
V'(t)&=V_R'(t)+V_C'(t)\\
&=RI'(t)+\frac{1}{C}I(t)
\end{align*}
to obtain a first order linear differential equation
\begin{equation*}
RI'(t)+\frac{1}{C}I(t)=V'(t)
\end{equation*}
In this circuit we consider the voltage $V(t)$ to be the input signal, and the circuit with resistance $R$ and
capacitance $C$ to be the system. The current $I$ is the output signal/system response:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{7}\\
\end{center}
$I(0)$ represents the initial condition.
\end{figure}
\newpage

\subsection{Superposition (First order ODEs)}%191024
Considering the following the first order linear equation:
\begin{equation*}
\dot{y}+p(t)y=q(t)
\end{equation*}
If a given input $q(t)$ has the output $y(t)$ we write
\begin{equation*}
q\rightsquigarrow y
\end{equation*}
Here we show that if
\begin{equation*}
q_1\rightsquigarrow y_1\text{ and }q_2\rightsquigarrow y_2
\quad\text{then}\quad c_1q_1+c_2q_2\rightsquigarrow
c_1y_1+c_2y_2
\end{equation*}
\textbf{Proof}\\
First see that (since differentiation does't change the constant coefficient)
\begin{align*}
\frac{dy}{dt}+py&=q\\
c\frac{dy}{dt}+cpy&=cq\\
=\frac{d(cy)}{dt}+p(cy)&=cq;\quad cq\rightsquigarrow cy
\end{align*}
Now see that
\begin{align*}
\frac{d(c_1y_1+c_2y_2)}{dt}+p(c_1y_1+c_2y_2)
&=\underbrace{c_1\dot{y}_1+pc_1y_1}_{=c_1q_1}
+\underbrace{c_2\dot{y}_2+pc_2y_2}_{=c_2q_2}\\
&=c_1q_1+c_2q_2
\end{align*}
Essentially, any linear combination of solutions is also a solution.
\newpage

\subsection{Solution by Integrating Factor (inhomogenous first order ODEs)}%240824
Here we prove the general solution to the inhomogeneous first order linear ODE 
\begin{equation*}
\dot{x}+p(t)x=q(t)
\end{equation*}
is\begin{equation*}
x(t)=\frac{1}{u(t)}\left(\int u(t)q(t)dt+C\right),
\quad\text{where }u(t)=e^{\int p(t)dt}
\end{equation*}
the function $u$ is called an \textit{integrating factor}.\\\vspace{1mm}\\
\textbf{Proof}\\
We start with the product rule for differentiation:
\begin{equation*}
\frac{d}{dt}(ux)=u\dot{x}+\dot{u}x
\end{equation*}
Consider multiplying both sides of our inhomogenous first order ODE by some function $u(t)$:
\begin{equation*}
u\dot{x}+upx=uq
\end{equation*}
We want to choose a function $u(t)$ such that we can apply the product rule to the sum on the left hand side
of the equation. There may be many functions $u$ that could work, but in this case we only need one. See that
\begin{equation*}
\frac{d}{dt}(ux)=u\dot{x}+\dot{u}x\iff u\dot{x}+upx=u\dot{x}+\dot{u}x\iff
\dot{u}=up
\end{equation*}
so now by separation of equations
\begin{align*}
\frac{du}{u}&=p(t)dt\\
\ln|u|&=\int p(t)dt\\
u&=e^{\int p\,dt}
\end{align*}
By using $u$ to satisfy the product rule:
\begin{align*}
u\dot{x}+upx=\frac{d}{dt}(ux)&=uq\\
u(t)x(t)&=\int u(t)q(t)dt+c\\
x(t)&=\frac{1}{u(t)}\left(\int u(t)q(t)dt+c\right)
\end{align*}
which was what we wanted.\\
(next page)
\newpage
\noindent\textbf{Integrating factor and homogeneous equations}\\
Given the homogeneous first order ODE
\begin{equation*}
\dot{x}+p(t)x=0
\end{equation*}
Solving by separation of variables gives
\begin{equation*}
x_h(t)=Ae^{-\int p(t)dt}
\end{equation*}
Comparing this to the formula for the integrating factor
\begin{equation*}
u(t)=e^{\int p(t)dt}
\end{equation*}
see that
\begin{equation*}
x_h(t)=\frac{A}{u(t)}
\end{equation*}
\newpage

\subsection{General, Particular and Homogeneous solutions}%301024
Solving by method of Integrating factors allows us to come up with a solution for inhomogeneous first order linear
ODEs
\begin{equation*}
\dot{x}+p(t)x=q(t)
\end{equation*}
Which have the form
\begin{equation*}
x(t)=\frac{1}{u(t)}\left(\int u(t)q(t)dt+C\right),
\quad\text{where }u(t)=e^{\int p(t)dt}
\end{equation*}
Notice that the presence of the constant $C$ implies a family of solutions; by setting $C=0$ we get a
\textit{particular solution} $x_p$, which is simply one specific solution---we could have chosen any other:
\begin{align*}
x_p=\frac{1}{u(t)}\left(\int u(t)q(t)dt+0\right)\quad&\text{is a solution}\\
x_p=\frac{1}{u(t)}\left(\int u(t)q(t)dt+999\right)\quad&\text{is also a solution}
\end{align*}
The method of integrating factors naturally leaves us with a constant. But say we were to find a solution by 
\textit{inspection}---how would we know that the constant of integration exists in the form $\frac{C}{u(t)}$? (as
is in this case)\\
\vspace{1mm}\\
\textbf{General solution}\\
See that since
\begin{equation*}
x_h(t)=\frac{1}{u(t)}
\end{equation*}
We can write the solution by integrating factor as
\begin{align*}
x(t)&=\frac{1}{u(t)}\left(\int u(t)q(t)dt\right)+\frac{C}{u(t)}\\
&=x_p+Cx_h
\end{align*}
One way to fully solve the inhomogeneous equation is by first solving the \textit{homogeneous} equation, and
then finding any \textit{one} solution, a \textit{particular solution}, to the inhomogeneous equation $x_p$. 
(We can use any method to find $x_p$ since we the homogeneous solution handles the constant of integration):
\begin{equation*}
\text{General solution}=\text{Particular solution}+\text{Homogeneous solution}
\end{equation*}
(next page)
\newpage
\noindent\textbf{Intuition}\\
Given an inhomogeneous first order linear ODE and its associated homogeneous equation
\begin{align*}
\dot{x}+p(t)x=q(t)\quad&\text{(inhomogeneous)}\\
\dot{x}+p(t)x=0\quad&\text{(homogeneous)}
\end{align*}
Solving both equations by method of integrating factors gives
\begin{equation*}
x_p(t)=\frac{1}{u(t)}\left(\int u(t)q(t)dt\right)+\frac{A}{u(t)},\qquad
x_h(t)=\frac{B}{u(t)}
\end{equation*}
(where $A$ is any chosen constant, each constant giving a particular solution, and $B$ the constant of integration)
Now see that by adding the solutions together the constant for the inhomogeneous solution $A$ gets absorbed into
the homogeneous solution:
\begin{align*}
x_p(t)+x_h(t)&=\frac{1}{u(t)}\left(\int u(t)q(t)dt\right)+\frac{A+B}{u(t)}\\
&=\frac{1}{u(t)}\left(\int u(t)q(t)dt\right)+\frac{C}{u(t)}
\end{align*}
We can obtain the `ambiguous part' of the general solution by simply solving the homogeneous equation; this means
that when obtaining a particular solution we don't have to worry about the constant of integration.\\
\vspace{1mm}\\
\textbf{Superposition}\\
See that this also makes sense with respect to superposition of solutions, where since
\begin{equation*}
\underbrace{q(t)\rightsquigarrow x_p(t)}_{\text{inhomogeneous}}\quad\text{and}\quad
\underbrace{0\rightsquigarrow x_h(t)}_{\text{homogeneous}}
\end{equation*}
we can say
\begin{equation*}
q(t)+0=q(t)\rightsquigarrow x_p(t)+x_h(t)
\end{equation*}
\newpage

\subsection{Polar form and Euler Identity}%311024
\textbf{The Complex Plane, Polar Form}\\
Complex numbers can be represented geometrically by points in a plane, where the number $a+ib$ is represented by 
the point $(a,b)$; when points in a plane are thought of as representing complex numbers this way,
the plane is known as a \textit{Complex Plane}:
\begin{figure}[h]
\begin{center}
\includegraphics[width=9cm]{8}\\
\end{center}
See that the magnitude of the coordinates of a complex number $x+iy$ can be represented by
\begin{equation*}
x=r\cos(\theta),\quad y=r\sin(\theta)
\end{equation*}
where $r$ is the absolute value of the number:
\begin{equation*}
r=|x+iy|=\sqrt{x^2+y^2}
\end{equation*}
(its just the pythagorean theorem) thus the entire number
can be written as
\begin{equation*}
x+iy=r(\cos(\theta)+i\sin(\theta))
\end{equation*}
This is called the \textit{Polar Form}
of a non-zero complex number. We call $\theta$ the \textit{angle} or
\textit{argument} of $x+iy$:
\begin{equation*}
\theta=\arg(x+iy)
\end{equation*}
Notice that the angle can be increased by any integer multiple of $2\pi$ and will still represent the same thing. 
To simplify this one can specify the \textit{principal value} of the angle:
\begin{equation*}
0\leq\theta<2\pi
\end{equation*}
this can be indicated by $\text{Arg}(...)$; for instance
\begin{equation*}
\text{Arg}(-1)=\pi,\quad\arg(-1)=\pm\pi,\pm3\pi,\pm5\pi
\end{equation*}
\end{figure}\\
(next page)
\newpage
\noindent\textbf{Euler's Formula}\\
Complex numbers have another \textit{exponential} form called \textit{Euler's formula}:
\begin{equation*}
e^{i\theta}=\cos(\theta)+i\sin(\theta)
\end{equation*}
This should be regarded as a definition for the exponential of an imaginary power.\\
\vspace{1mm}\\
A good justification for Euler's formula can be found from
its Taylor approximation:
\begin{align*}
e^{i\theta}&=1+i\theta-\frac{\theta^2}{2!}-i\frac{\theta^3}{3!}+\frac{\theta^4}{4!}+i\frac{\theta^5}{5!}-\cdots\\
&=\left(1-\frac{\theta^2}{2!}+\frac{\theta^4}{4!}-\cdots\right)+i\left(\theta-\frac{\theta^3}{3!}
+\frac{\theta^5}{5!}-\cdots\right)\\
&=\cos(\theta)+i\sin(\theta)
\end{align*}
Note that the argument above is not a proof; rather it just shows that Euler's formula is formally compatible with
the series expansions for the exponential, sine, and cosine functions.\\
\vspace{1mm}\\
\textbf{Polar form again}\\
We can now write
\begin{equation*}
x+iy=r(\cos(\theta)+i\sin(\theta))=re^{i\theta}
\end{equation*}
Polar representation in exponential form allows for much simpler multiplication of complex numbers. Since one can
show that (using angle addition formulas)
\begin{align*}
e^{i\theta_1}e^{i\theta_2}&=(\cos(\theta_1)+i\sin(\theta_1))(\cos(\theta_2)+i\sin(\theta_2))\\
&=\cos(\theta_1)\cos(\theta_2)-\sin(\theta_1)\sin(\theta_2)\\&\quad+i(\sin(\theta_1)\cos(\theta_2)+\cos(\theta_1)\sin(\theta_2)\\
&=\cos(\theta_1+\theta_2)+i\sin(\theta_1+\theta_2)\\
&=e^{i(\theta_1+\theta_2)}
\end{align*}
(next page)
\newpage
\noindent\textbf{Complex Exponential properties}\\
We had
\begin{equation*}
e^{i\theta_1}e^{i\theta_2}=e^{i(\theta_1+\theta_2)}
\end{equation*}
This property can be extrapolated to further justify Euler's formula---the complex exponential follows
the same exponential addition rules as any typical exponential. See that we can now conclude:\\
\vspace{1mm}\\
\textit{Multiplication rule:}
\begin{equation*}
r_1e^{i\theta_1}\cdot r_2e^{i\theta_2}=r_1r_2e^{i(\theta_1+\theta_2)}
\end{equation*}
also see that since
\begin{equation*}
\frac{1}{r}e^{-i\theta}\cdot re^{i\theta}=1
\end{equation*}
\textit{Reciprocal Rule:} 
\begin{equation*}
\frac{1}{re^{i\theta}}=\frac{1}{r}e^{-i\theta}
\end{equation*}
\textbf{DeMoivre's Formula}\\
Since 
\begin{equation*}
(x+iy)^n=r^ne^{in\theta}
\end{equation*}
we can show \textit{DeMoivre's formula:}
\begin{equation*}
(\cos(\theta)+i\sin(\theta))^n=e^{in\theta}=\cos(n\theta)+i\sin(n\theta)
\end{equation*}
\textbf{Combining pure oscillations of the same frequency}\\
We can also show that
\begin{equation*}
a\cos(\lambda t)+b\sin(\lambda t)=A\cos(\lambda t-\phi)
\end{equation*}
where
\begin{equation*}
A=\sqrt{a^2+b^2},\quad\phi=\tan^{-1}\left(\frac{b}{a}\right)
\end{equation*}
See that
\begin{align*}
a\cos(\lambda t)+b\sin(\lambda t)&=\text{Re}((a-bi)(\cos(\lambda t)+i\sin(\lambda t))\\
&=\text{Re}(Ae^{-i\phi}\cdot e^{i\lambda t})\\
&=\text{Re}(Ae^{i(\lambda t-\phi)})\\
&=A\cos(\lambda t-\phi)
\end{align*}
\newpage

\subsection{More on Complex Exponentials}%031124
\textbf{Notable properties}\\
We know that (as proven)
\begin{equation*}
e^{a+ib}=e^ae^{ib}=e^a(\cos(b)+i\sin(b))
\end{equation*}
So see that
\begin{equation*}
\text{Re}(e^{a+ib})=e^a\cos(b),\quad
\text{Im}(e^{a+ib})=e^a\sin(b)
\end{equation*}
this can be extrapolated further to show
\begin{align*}
\cos(x)&=\text{Re}(e^{ix}),&\sin(x)&=\text{Im}(e^{ix})\\
\cos(x)&=\frac{1}{2}(e^{ix}+e^{-ix}),&\sin(x)&=
\frac{1}{2i}(e^{ix}-e^{-ix})
\end{align*}
\textbf{Derivatives and integrals}\\
Note that a function like
\begin{equation*}
e^{ix}=\cos(x)+i\sin(x)
\end{equation*}
is a \textit{complex-valued function of the real variable} $x$. Such a function may be written as
\begin{equation*}
u(x)+iv(x),\quad u,v\text{ real-valued}
\end{equation*}
with its derivative and integral with respect to $x$ defined to be
\begin{equation*}
\text{a) }D(u+iv)=Du+iDv,\quad\text{b) }\int(u+iv)dx=\int udx+i\int vdx
\end{equation*}
It follows easily that
\begin{equation*}
D(e^{(a+ib)x})=(a+ib)e^{(a+ib)x}
\end{equation*}
since
\begin{align*}
D(e^{(a+ib)x})&=D(e^{ax}\cos(bx)+ie^{ax}\sin(bx))\\
&=ae^{ax}\cos(bx)-be^{ax}\sin(bx)+i(ae^{ax}\sin(bx)+be^{ax}\cos(bx))\\
&=e^{ax}\cos(bx)(a+ib)+e^{ax}\sin(bx)(ia+i^2b)\\
&=(a+ib)e^{ax}(\cos(bx)+i\sin(bx))\\
&=(a+ib)e^{(a+ib)x}
\end{align*}
Therefore we can also write the down the integral as
\begin{equation*}
\int e^{(a+ib)x}dx=\frac{1}{a+ib}e^{(a+ib)x}
\end{equation*}
\newpage

\subsection{Finding $n$-th roots}
To solve linear DEs with constant coefficients, we need to be able to find the real and complex roots of polynomial
equations. Though a lot of this is done today with calculators and computers, one
still has to know how to do an important special case by hand: finding the roots of 
\begin{equation*}
z^n=\alpha
\end{equation*}
where $\alpha$ is a complex number---finding the $n$-th roots of $\alpha$.\\
\vspace{1mm}\\
\textbf{$n$-th roots of unity}\\
Consider first a special case; we want the solutions to
\begin{equation*}
z^n=1
\end{equation*}
We use polar representation for both sides, setting $z=re^{i\theta}$ on the left. See that
\begin{equation*}
\underbrace{r^ne^{in\theta}}_{(re^{i\theta})^n}
=\underbrace{1\cdot e^{(2k\pi i)},\quad k=0,\pm1,\pm2,\ldots}_{=1}
\end{equation*}
Equating the absolute values and the arguments of each side:
\begin{equation*}
r^n=1,\quad n\theta=2k\pi,\quad k=0,\pm1,\pm2,\ldots
\end{equation*}
(Notice the arguments for $k=a$ and $k=-a$, where $a$ is an integer, are the same.
Also see that $r$ can only be 1 it is defined to be \textit{real and non-negative} 
so it can't be anything else) we can conclude that
\begin{equation*}
r=1,\quad\theta=\frac{2k\pi}{n},\quad k=0,1,\ldots,n-1
\end{equation*}
we don't need any integer values of $k$ other than $0,\ldots,n-1$---they would not produce a complex number
that isn't already among the above $n$ numbers. See that if we add $an$, an integer multiple of $n$,
to any $k$ we get the same complex number:
\begin{equation*}
\theta'=\frac{2(k+an)\pi}{n}=\theta+2a\pi\end{equation*}
(this is the same as having $k=n,n+1,n+2\ldots$) so
\begin{equation*}
e^{i\theta'}=e^{i\theta}e^{2a\pi i}=e^{i\theta}
\end{equation*}
We can conclude therefore that \textit{the $n$-th roots of 1 are the numbers}
\begin{equation*}
e^{2k\pi i/n},\quad k=0,\ldots,n-1
\end{equation*}
(next page)
\newpage
\noindent\textbf{Roots of unity visualised}\\
There are $n$ complex $n$-th roots of unity. Since they all have absolute value 1 ($r=1$) they all lie on the unit
circle in the complex plane. They are evenly spaced around the unit circle; the angle between two consecuitive
roots is $2\pi/n$.
\begin{figure}[h]
Illustrated here is the case for $n=6$:
\begin{center}
\includegraphics[width=10cm]{9}\\
\end{center}
The six solutions to $z^6=1$ lie on the unit circle in the complex plane. 
See that we can express the roots of unity in
a different notation:
\begin{equation*}
\text{\textit{the $n$-th roots of 1 are} }1,\zeta,\zeta^2,\ldots,\zeta^{n-1},\text{ where }\zeta=e^{2\pi i/n}
\end{equation*}
\end{figure}\\
\textbf{General case}\\
Now we generalise to find the $n$-th roots of an arbitrary complex number $w$. We start by writing $w$ in
polar form:
\begin{equation*}
w=re^{i\theta};\quad\theta=\text{Arg}(w),0\leq\theta<2\pi
\end{equation*}
Here $\theta$ is the principal value of the polar angle of $w$. Following the same reasoning as before, see that
\begin{equation*}
z^n=re^{i(\theta+2\pi k)};\quad k=0,\pm1,\pm2,\ldots
\end{equation*}
where removing the redundant $k$  (this can be shown using the same methods as above) and solving gives us
\begin{equation*}
z=\sqrt[n]{r}e^{i(\theta+2\pi k)/n},\quad k=0,1,\ldots,n-1
\end{equation*}
See that these $n$ roots can be expressed with the roots of unity as
\begin{equation*}
\sqrt[n]{w}=z_0,z_0\zeta,z_0\zeta^2,\ldots,z_0\zeta^{n-1},\quad\text{where }z_0=\sqrt[n]{r}e^{i\theta/n}
\end{equation*}
($z_0$ is just the case where $k=0$) See that all of the $n$ roots satisfy $z^n=w$.
\newpage

\subsection{Sinusoidal functions}
\textbf{Definition and properties}\\
A \textit{sinusoidal function/oscillation/signal} is one
that can be written in the from
\begin{equation*}
f(t)=A\cos(\omega t-\phi)
\end{equation*}
The function $f(t)$ is a cosine function which has been 
\textit{amplified} by $A$, \textit{shifted} by $\phi/\omega$, and \textit{compressed} by $\omega$.
\begin{itemize}
\item$A>0$ is its \textit{amplitude}: how high the graph of $f(t)$ rises above the $t$-axis at its maximum values
\item$\phi$ is its \textit{phase lag}: the value of $\omega t$ for which the graph has its maximum 
(a positive phase lag shifts the sinusoid \textit{forward}; consider a maximum at $\cos(a)$, 
without phase lag it is reached at $\omega t=a$, with phase lag its now $\omega t=a+\phi$.)
\item$\tau=\phi/\omega$ is its \textit{time delay/lag}: how far along the $t$-axis the graph of $\cos(wt)$ has been 
shifted due to phase lag. ($\tau$ and $\phi$ have the same sign; consider a maximum at $\cos(0)$, without
phase lag it is reached at $\omega t=0\implies t=0$, with phase lag its now 
$\omega t-\phi=0\implies t=\phi/\omega$.)
\item$\omega$ is its \textit{angular frequency}: the number of complete oscillations $f(t)$ makes per time interval
of $2\pi$; that is, the \textit{number of radians per unit time} 
(1 radian in 1 second means 1 oscillation in $2\pi$ seconds---1 radian is the angle subtended at the centre of 
a circle by an arc equal in length to the radius).
\item$v=\omega/2\pi$ is the \textit{frequency} of $f(t)$: the number of complete oscillations made in a
time interval of 1; that is, the number of cycles per unit time.
\item$P=2\pi/\omega=1/v$ is its \textit{period}: the $t$-interval required for one complete oscillation.
\end{itemize}
See that one can also write the sinusoidal function using
the time lag $\tau=\phi/\omega$:
\begin{equation*}
f(t)=A\cos(\omega(t-\tau))
\end{equation*}
(next page)
\newpage
\noindent\textbf{Example}\\
\begin{figure}[h]
In the figure below the dotted curve is $\cos(t)$ and the solid curve is $2.5\cos(\pi t-\pi/2)$. The solid
curve has
\begin{equation*}
A=2.5,\quad\omega=\pi,\quad\phi=\pi/2,\quad\tau=1/2
\end{equation*}
\begin{center}
\includegraphics[width=10cm]{10}\\
\end{center}
\end{figure}
\newpage

\subsection{Solution to the Constant Coefficient First Order Equation}
\textbf{Solution}\\
Considering the constant coefficient equation (constant coefficient meaning $k$ is a constant)
\begin{equation*}
\dot{y}+ky=q(t)
\end{equation*}
This is easily solvable by integrating factor:
\begin{align*}
y&=e^{-kt}\left(\int e^{kt}q(t)dt+c\right)\\
&=e^{-kt}\int e^{kt}q(t)dt+ce^{-kt}
\end{align*}
(integrating factor gives us a way of finding the particular solution, but see that it also gives us the 
homogeneous solution) We have the \textit{particular} solution and \textit{homogeneous} solution respectively
\begin{equation*}
y_p(t)=e^{-kt}\int e^{kt}q(t)dt\quad\text{and}\quad
y_h(t)=e^{-kt}
\end{equation*}
The general solution is then
\begin{equation*}
y(t)=y_p(t)+cy_h(t)
\end{equation*}
\textbf{Behaviour for $k>0$:}\\
For $k>0$ the system models \textit{exponential decay}. When the input is 0 the system response is $y(t)=ce^{-kt}$, 
which decays exponentially to 0 as $t$ goes to $\infty$.\\
\vspace{1mm}\\
In the general solution we call $ce^{-kt}$ the \textit{transient} because it goes to 0. The other term 
$e^{-kt}\int e^{kt}q(t)dt$ is called the \textit{steady-state/long-term} solution. That is, $cy_h$ is the transient
and $y_p$ is the steady-state solution.\\
\vspace{1mm}\\
The value of $c$ is determined by the initial value $y(0)$. See that this initial value only affects the transient
and not the long-term behaviour of the solution---no matter what the initial condition, every solution goes
asymptotically to the steady-state---all solution curves approach the steady-state as $t\to\infty$.\\
\vspace{1mm}\\
(next page)
\newpage
\noindent\textbf{Behaviour for $k>0$ illustrated}\\
In the case $k>0$ all solutions go asymptotically to the 
steady-state:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{11}\\
\end{center}
Since all the solutions approach each other, there is no precise way to choose the one to we call the 
steady-state---we can \textit{choose any one} to be the steady-state solution. 
Generally we just choose the simplest looking solution.
\end{figure}\\
\textbf{The case $k\leq0$:}\\
When $k\leq0$ the homogeneous solution $e^-{kt}$ does not decay asymptotically to 0---it is not transient. In this
case it does not make sense to talk about the steady-state solution.
\newpage

\subsection{First Order Response to Sinusoidal/Exponential Input---complex replacement}
\textbf{Context}\\
Consider solving the first order constant coefficient DE with sinusoidal input
\begin{equation*}
\dot{x}+kx=B\cos(\omega t)
\end{equation*}
The idea here is to replace $\cos(\omega t)$ by the complex exponenial $e^{i\omega t}$; this is called
\textit{complex replacement}.\\
\vspace{1mm}\\
\textbf{Complex Replacement}\\
Consider introducing a new variable $y$ with its own related ODE:
\begin{equation*}
\dot{y}+ky=B\sin(\omega t)
\end{equation*}
Combining $x$ and $y$ to make a complex variable $z=x+iy$, see that we get
\begin{equation*}
\dot{z}+kz=B(\cos(\omega t)+i\sin(\omega t))=Be^{i\omega t}
\end{equation*}
where
\begin{equation*}
\cos(\omega t)=\text{Re}(e^{i\omega t})\quad\text{and}\quad x=\text{Re}(z)
\end{equation*}
\textbf{Exponential input}\\
Using complex replacement we now have the same problem but with exponential input
\begin{equation*}
\dot{z}+kz=Be^{i\omega t}
\end{equation*}
This can be solved using integrating factors, but we present a simpler solution: consider a particular solution of 
the form $z_p(t)=Ae^{i\omega t}$ (this is a reasonable choice given that differentiation reproduces exponentials); 
this gives us
\begin{equation*}
\dot{z}_p+kz_p=i\omega Ae^{i\omega t}+kAe^{i\omega t}
=(k+i\omega)Ae^{i\omega t}
\end{equation*}
so we have
\begin{equation*}
(k+i\omega)Ae^{i\omega t}=Be^{i\omega t}\implies A=B/(k+i\omega)
\end{equation*}
As such we have the particular solution
\begin{equation*}
z_p(t)=Be^{i\omega t}/(k+i\omega)
\end{equation*}
simplifying with polar coordinates:
\begin{equation*}
z_p(t)=\frac{Be^{i\omega t}}{\sqrt{k^2+\omega^2}e^{i\phi}}
=\frac{Be^{i(\omega t-\phi)}}{\sqrt{k^2+\omega^2}}
\end{equation*}
since
\begin{equation*}
k+i\omega=\sqrt{k^2+\omega^2}e^{i\phi},\quad\text{where }\phi=\tan^{-1}(\omega/k)\text{ in the first quadrant}
\end{equation*}
Since $\tan^{-1}$ is ambiguous ($\tan(\pi/4)=\tan(5\pi/4)=1$), we clarify by saying which quadrant the
complex number is in. In this case since $k,\omega>0$ its the first quadrant.\\
(next page)
\newpage
\noindent\textbf{Solving for sinusoidal input}\\
We have 
\begin{equation*}
z_p(t)=\frac{Be^{i(\omega t-\phi)}}{\sqrt{k^2+\omega^2}}
\end{equation*}
we wanted $x_p$, where $x_p=\text{Re}(z_p)$:
\begin{equation*}
x_p(t)=\frac{B}{\sqrt{k^2+\omega^2}}\cos(\omega t-\phi)
\end{equation*}
To get the general solution we add the homogeneous solution:
\begin{equation*}
x(t)=x_p(t)+Ce^{-kt}=\frac{B}{\sqrt{k^2+\omega^2}}\cos(\omega t-\phi)+Ce^{-kt}
\end{equation*}
\newpage

\subsection{Amplitude, Phase, Gain, and Bode Plots\\---Terminology and introduction}
\textbf{Terminology}\\
We found that the ODE
\begin{equation*}
\dot{x}+kx=kB\cos(\omega t)
\end{equation*}
has a particular solution
\begin{equation*}
x(t)=\frac{kB}{\sqrt{k^2+\omega^2}}\cos(\omega t-\phi)
\end{equation*}
where $\phi=\tan^{-1}(\omega/k)$. If we consider the input to be $B\cos(\omega t)$ then the 
gain $g$ (output amplitude/input amplitude) is $g=k/\sqrt{k^2+\omega^2}$:
\begin{equation*}
x(t)=gB\cos(\omega t-\phi)
\end{equation*}
We define the terminology as follows:
\begin{itemize}
\item $B\cos(\omega t)$ is the input/input signal.
\item $B$ is the input amplitude and $\omega$ is the input angular/circular frequency.
\item $x(t)$ is the output or response.
\item $g=k/\sqrt{k^2+\omega^2}$ is called the gain/amplitude response. 
See that the input amplitude is scaled by the gain to give the output amplitude.
\item $\phi$ is called the phase lag.
\end{itemize}
(next page)
\newpage
\noindent\textbf{Bode plots}\\
Since $g$ and $\phi$ vary with 
$\omega$, we can regard them as functions of $\omega$---$g(\omega)$ and $\phi(\omega)$.
$k$ is called the \textit{coupling constant}. Consider the graphs of $g(\omega)$ and $-\phi(\omega)$ for 
the values of coupling constant $k=.25,.5,.75,1,1.25,1.5$:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{12}\\
\end{center}
\end{figure}\\
These graphs are essentially \textit{Bode plots}. (Bode plots display $\log g(\omega)$ and $-\phi(\omega)$ against 
$\log\omega$).
\newpage

\subsection{Autonomous equations, Logistic Model,\\Stable/Unstable equilibria}
Here we consider \textit{autonomous first order differential equations}. These are (in general) nonlinear equations
of the form
\begin{equation*}
\dot{x}=f(x)
\end{equation*}
(compare this with the general first order ODE $\dot{x}=f(x,t)$.) The word autonomous means self governing---the
rate of change of $x$ is governed by $x$ itself and is not dependent on time.\\
\vspace{1mm}\\
\textbf{Example: Logistic Population Model}\\
Suppose we have a model for a population $y$ with variable growth rate $k(y)$ which depends on the current
population but \textit{not on time}:
\begin{equation*}
\dot{y}=k(y)\cdot y
\end{equation*}
Say we model $k(y)$ as
\begin{figure}[h]
\begin{equation*}
k(y)=k_0\left(1-\frac{y}{M}\right)
\end{equation*}
\begin{center}
\includegraphics[width=10cm]{13}\\
\end{center}
\end{figure}\\
The idea here is that population growth is positive until the population reaches some $M$, after which it becomes 
negative and declines until it becomes lower than $M$.
In the simplest version of this we model $k(y)$ as a straight line as above. The final equation is known as the 
\textit{logistic population model}:
\begin{equation*}
\dot{y}=k_0(1-(y/M))y=f(y)
\end{equation*}
The equation is nonlinear and autonomous. 
Autonomous equations are always separable; in this case partial fractions could be used to compute an
integral, but here we consider a qualitative approach.\\
(next page)
\newpage
\noindent\textbf{Qualitative perspective}\\
We start by looking for \textit{constant} solutions
$y(t)=y_0$. We do this by considering $\dot{y}=0$; see that this occurs in two situations, either
\begin{equation*}
y(t)=0,\quad\text{or}\quad y(t)=M
\end{equation*}
Because a system at equilibrium is unchanging, we call these solutions \textit{equilibrium solutions}. 
Since equilibrium is achieved when $y$ is $0$ or $M$ we call $0$ and $M$ the \textit{critical points} of the DE. 
To summarise, these statements all mean the same thing:
\begin{enumerate}
\item $f(y_0)=0$.
\item $y(t)=y_0$ is an equilibrium solution.
\item $y=y_0$ is a critical point.
\end{enumerate}
Consider the direction field for these solutions; (recall each isocline represents $f(y)=c$) they correspond to the
\textit{nullclines}, where $f(y)=0$:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{14}\\
\end{center}
\end{figure}\\
(Note that the nullclines are also solution curves, if $y$ starts at $M$ it will never change since its
derivative will be 0 forever.) For a clear picture of the other isoclines consider
a graph of $f(y)$ against $y$:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{15}\\
\end{center}
\end{figure}\\
See that 
\begin{align*}
\text{for }y<0\quad&\dot{y}=f(y)\text{ is negative,}\\
\text{for }0<y<M\quad&\dot{y}=f(y)\text{ is positive,}\\
\text{for }M<y\quad&\dot{y}=f(y)\text{ is negative}
\end{align*}
These are indicated on the graph by the arrows on the horizontal axis.\\
(next page)
\newpage
\noindent\textbf{Direction field}\\
We sketch the direction field and some solution curves:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{16}\\
\end{center}
See that
\begin{enumerate}
\item Since the isoclines are constant in the $t$ direction any solution curve can be translated left or right and 
still be a solution---time invariance.
\item Since the lines $y=0$ and $y=M$ are solutions the other curves can't cross them.
\item The solutions that start above $y=0$ must increase, and since they can't cross $y=M$ they 
tend toward it asymptotically. These bounded solutions
are called \textit{logistic curves}. They represent small populations increasing to $M$.
\item If the population exceeds $M$, they tend back towards it. This represents overpopulation. M is called the 
\textit{carrying capacity} of the environment.
\item Although it doesn't make sense to model a negative numbered population $y<0$, mathematically the solution
curves that start below $y=0$ decrease without bound.
\end{enumerate}
\end{figure}\\
\textbf{Stable and Unstable Equilibria}\\
See that solution curves near the equilibrium $y=M$ tend asymptotically towards it; this is called a 
\textit{stable equilibrium}. Solution curves near the other equilibrium $y=0$ tend away from it; this is called
an \textit{unstable equilibrium}.
\newpage

\subsection{Phase lines, Semistable equilibria}
\textit{Phase lines} allows for the essential content of an autonomous DE:
\begin{equation*}
\dot{y}=f(y)
\end{equation*}
to be conveyed more efficiently. A phase line is drawn using the following steps:
\begin{enumerate}
\item Draw the $y$-axis as a vertical line and mark on it the equilibria---where $f(y)=0$.
\item In each of the intervals delimited by the equilibria draw an upward pointing arrow if $f(y)>0$ and a downward arrow if $f(y)<0$.
\end{enumerate}
Phase lines tells us roughly how the system behaves, capturing the information of a qualitative sketch. Consider 
the following examples.\\
\vspace{1mm}\\
\textbf{Example 1:}\\
Consider the simple autonomous equation
\begin{equation*}
\dot{y}=3y
\end{equation*}
1) First we find the critical points; see that only one critical point exists: $y=0$. 
2) Next we plot the graph of $f(y)$ (in this case a straight line); see that $\dot{y}>0$ for $y>0$ and $\dot{y}<0$ for $y<0$:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{17}\\
\end{center}
\end{figure}\\
(next page)
\newpage
\noindent\textbf{Example 1 (cont.)}\\
3) With this we can draw the phase line as outlined above. Since the arrows on the phase line point
away from the critical point, the equilibrium is \textit{unstable}:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{18}\\
\end{center}
4) See how the phase line conveys qualitiative information. The equilibrium solution corresponds to the critical
point.
\end{figure}\\
\textbf{Example 2: Logistic equation}\\
Now consider the same solution for the logistic equation:
\begin{figure}[h]
\begin{equation*}
\dot{y}=k_0(1-y/M)y
\end{equation*}
With critical points $y=0,y=M$:
\begin{center}
\includegraphics[width=9cm]{19}\\
\end{center}
\end{figure}\\
(next page)
\newpage
\noindent\textbf{Semistable Equilibria}\\
Some equilibria are stable on one side and unstable on the other. We call them \textit{semistable}. 
Consider the DE
\begin{equation*}
\dot{y}=y^2
\end{equation*}
With only one critical point $y=0$:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{20}\\
\end{center}
\end{figure}
\newpage

\section{Second Order Constant Coefficient Linear Equations}
\subsection{Second Order Physical systems---\\Spring-Mass-Dashpot}
\textbf{Spring and Mass}\\
Here we model a second order differential equation. Consider a spring attached to a wall and a cart:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{21}\\
\end{center}
Consider the coordinate system set in a way that at $x=0$ the spring doesn't exert any force---the equilibrium 
position. Now also consider an external force acting on the mass, the system can be modelled as
\begin{equation*}
m\ddot{x}=F_\text{spr}+F_\text{ext}
\end{equation*}
The spring's behaviour can be characterised by the fact that it depends on the deviation from equilibrium position,
meaning
\begin{align*}
\text{if }x>0,\quad&F_\text{spr}(x)<0\\
\text{if }x=0,\quad&F_\text{spr}(x)=0\\
\text{if }x<0,\quad&F_\text{spr}(x)>0
\end{align*}
The simplest way to model the force exerted by the spring (which is valid in general for small $x$) is
\begin{equation*}
F_\text{spr}(x)=-kx,\text{ where }k>0
\end{equation*}
This is called \textit{Hooke's law}, and $k$ is called the \textit{spring constant}.
\end{figure}\\
Replacing $F_\text{spr}$ by $-kx$ we get
\begin{equation*}
m\ddot{x}+kx=F_\text{ext}
\end{equation*}
(next page)
\newpage
\noindent\textbf{Dashpot}\\
Any real mechanical system has friction, which can take many forms; it is characterised by the fact that it
depends on the motion of the mass. We will suppose that
it depends only on the velocity of the mass and not on its
position.\\
\vspace{1mm}\\
Often dampening is controlled by a device called
the \textit{dashpot} (its a cylinder filled with oil that a piston moves through):
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{22}\\
\end{center}
We write $F_\text{dash}(\dot{x})$ for the force exerted by the dashpot. It opposes the velocity:
\begin{align*}
\text{if }\dot{x}>0,\quad&F_\text{dash}(\dot{x})<0\\
\text{if }\dot{x}=0,\quad&F_\text{dash}(\dot{x})=0\\
\text{if }\dot{x}<0,\quad&F_\text{dash}(\dot{x})>0
\end{align*}
The simplest way to model this (which is also valid for small $\dot{x}$) is
\begin{equation*}
F_\text{dash}(\dot{x})=-b\dot{x},\text{ where }b>0
\end{equation*}
This is called \textit{linear damping}, and $b$ is called the \textit{damping constant}.
\end{figure}\\
Putting this together
\begin{equation*}
m\ddot{x}=F_\text{spr}+F_\text{dash}+F_\text{ext}
\end{equation*}
we get the differential equation for the displacement $x$ of the mass from equilibrium as
\begin{equation*}
m\ddot{x}+b\dot{x}+kx=F_\text{ext}
\end{equation*}
\newpage

\subsection{Linear DEs---Notation}
A \textit{linear differential equation} is of the following form:
\begin{equation*}
a_nx^{(n)}+a_{n-1}x^{(n-1)}+\cdots+a_1\dot{x}+a_0x=q(t)
\end{equation*}
The $a_k$ are the \textit{coefficients}; they may depend on $t$. If $a_n$ is not
zero then the differential equation is said to be of order $n$.\\
\vspace{1mm}\\
If the $a_k$ are constant then the equation is said to be a \textit{constant coefficient linear equation}.

\subsection{Second order homogeneous constant coefficient\\linear equations---Spring system, 
Simple harmonic oscillator}
Consider the spring system in the case where $F_\text{ext}=0$:
\begin{equation*}
m\ddot{x}+b\dot{x}+kx=0
\end{equation*}
With no external force the equation is \textit{homogeneous}.\\
\vspace{1mm}\\
\textbf{Undamped case: Simple harmonic oscillator}\\
The special case where $b=0$ (no dashpot) is called \textit{undamped}. This is called the \textit{simple harmonic
oscillator}.  We can write its ODE as
\begin{equation*}
\ddot{x}+\frac{k}{m}x=0
\end{equation*}
If we let $\omega=\sqrt{k/m}$ our equation becomes
\begin{equation*}
\ddot{x}+\omega^2x=0
\end{equation*}
See that $x_1(t)=\cos(\omega t)$ and $x_2(t)=\sin(\omega t)$ are solutions to this equation. Since the equation is
linear we can use superposition of solutions to get
the solution
\begin{equation*}
x(t)=a\cos(\omega t)+b\sin(\omega t)=A\cos(\omega t-\phi)
\end{equation*}
This is the \textit{general solution}. We know it gives every solution because
$x(0)=a$ and $\dot{x}(0)=\omega b$---by solving (uniquely) for $a$ and $b$ we can get any desired initial condition.
\newpage

\subsection{Characteristic polynomial}
\textbf{2nd Order Case}\\
For $m,b,k$ constant, consider the homogeneous equation
\begin{equation*}
m\ddot{x}+b\dot{x}+kx=0
\end{equation*}
Consider solutions of the form $x=e^{rt}$. We have
\begin{equation*}
m\ddot{x}+b\dot{x}+kx=(mr^2+br+k)e^{rt}=0
\end{equation*}
Since an exponential is never zero, $e^{rt}$ is therefore a solution exactly when $r$ satisfies the
\textit{characteristic equation} (the left hand side is the \textit{characteristic polynomial}):
\begin{equation*}
mr^2+br+k=0
\end{equation*}
\textbf{Example}\\
Consider the DE
\begin{equation*}
\ddot{x}+8\dot{x}+7x=0
\end{equation*}
The characteristic polynomial here is $r^2+8r+7$. Solving for $r$ by factorisation we have $(r+1)(r+7)$ and 
the roots $r=-1$ and $r=-7$. Therefore the corresponding exponential solutions are
$x_1(t)=e^{-t}$ and $x_2(t)=e^{-7t}$.\\
\vspace{1mm}\\
By superposition, the linear combination of independent solutions gives the general solution:
\begin{equation*}
x(t)=c_1e^{-t}+c_2e^{-7t}
\end{equation*}
Where given initial conditions for $x$ and $\dot{x}$ we can solve for $c_1$ and $c_2$.\\
\vspace{1mm}\\
\textbf{General $n$th Order Case}\\
See that using the same principle we can take the homogeneous constant coefficient linear equation of degree $n$:
\begin{equation*}
a_nx^{(n)}+\cdots+a_1\dot{x}+a_0x=0
\end{equation*}
and get its characteristic polynomial
\begin{equation*}
p(r)=a_nr^n+\cdots+a_1r+a_0
\end{equation*}
In which the exponential $x(t)=e^{rt}$ is a solution of the homogeneous DE if and only if $r$ is a root
of $p(r)$ (meaning $p(r)=0$). By superposition, any linear combination of these exponentials is also a solution.
\newpage

\subsection{Modes and Roots (real and complex)\\(homogeneous constant coefficient linear equations)}
\textbf{Modes}\\
A solution of the form $x(t)=ce^{rt}$ to the homogeneous constant coefficient linear equation:
\begin{equation*}
a_nx^{(n)}+a_{n-1}x^{(n-1)}+\cdots+a_1\dot{x}+a_0x=0
\end{equation*}
is called a \textit{modal solution} and $ce^{rt}$ the \textit{mode} of the system. Recall that
$e^{rt}$ is a solution exactly when $r$ is a root of the characteristic polynomial
\begin{equation*}
p(s)=a_ns^n+a_{n-1}s^{n-1}+\cdots+a_1s+a_0
\end{equation*}
(note this only works for \textit{homogeneous constant coefficient linear equations}; it won't apply to
non-constant coefficient or inhomogeneous or nonlinear equations.)\\
\vspace{1mm}\\
\textbf{Real roots}\\
The roots of these polynomials can be real or complex. Roots can also be repeated. First consider the real case for
a second order homogeneous constant coefficient DE:
if the characteristic polynomial has real roots $r_1$ and $r_2$ then the \textit{modal} solutions are 
$x_1(t)=e^{r_1t}$ and $x_2(t)=e^{r_2t}$. The general solution can be found by superposition:
\begin{equation*}
x(t)=c_1x_1(t)+c_2x_2(t)=c_1e^{r_1t}+c_2e^{r_2t}
\end{equation*}
\textbf{Example}\\
Solving $\ddot{x}+5\dot{x}+4x=0$: The characteristic equation is
\begin{equation*}
s^2+5s+4=(s+1)(s+4)=0
\end{equation*}
Which has roots -1 and -4. The modal solutions are $x_1(t)=e^{-t}$ and $x_2(t)=e^{-4t}$. Therefore
the general solution is
\begin{equation*}
x(t)=c_1e^{-t}+c_2e^{-4t}
\end{equation*}
(next page)
\newpage
\noindent\textbf{Complex roots---illustrative example}\\
Consider now the equation $\ddot{x}+4\dot{x}+5x=0$. The characterisic polynomial is $s^2+4s+5$. 
Using the quadratic formula the roots are
\begin{equation*}
s=\frac{-4\sqrt{16-20}}{2}=-2\pm\sqrt{-1}=-2\pm i
\end{equation*}
So our exponential solutions are (using the letter $z$ to indicate they are complex valued):
\begin{equation*}
z_1(t)=e^{(-2+i)t}\quad\text{and}\quad z_2(t)=e^{(-2-i)t}
\end{equation*}
The DE has real coefficients, we expect \textit{real solutions}. To get them, consider the following theorem:\\
\vspace{1mm}\\
\textbf{Real Solution Theorem:}\\
\textit{Theorem:} If $z(t)$ is a complex-values solution to $m\ddot{z}+b\dot{z}+kz=0$, where $m,b,k$ are real, 
then the real and imaginary parts of $z$ are also solutions.\\
\vspace{1mm}\\
\textit{Proof:} Letting $u(t)$ be the real part of $z$ and $v(t)$ the imaginary part, so that
$z(t)=u(t)+iv(t)$, see that the DE can be written as
\begin{equation*}
(m\ddot{u}+b\dot{u}+ku)+i(m\ddot{v}+b\dot{v}+kv)=0
\end{equation*}
Both expressions in parentheses are real. The only way for the sum to be 0 is if both expressions are 0. 
That is, both $u$ and $v$ are solutions.\\
\vspace{1mm}\\
\textbf{Illustrative example cont.}\\
We had $z_1(t)=e^{(-2+i)t}$ and $z_2(t)=e^{(-2-i)t}$. Using Euler's formula:
\begin{equation*}
z_1(t)=e^{(-2+i)t}=e^{-2t}\cos t+ie^{-2t}\sin t
\end{equation*}
Both the real part $e^{-2t}\cos t$ and imaginary part $e^{-2t}\sin t$ are solutions. We now have two \textit{basic}
solutions and can use superposition to obtain the general
\textit{real valued} solution
\begin{equation*}
x(t)=c_1e^{-2t}\cos(t)+c_2e^{-2t}\sin(t)
\end{equation*}
See that choosing the other exponential solution
\begin{equation*}
z_2(t)=e^{(-2+i)t}=e^{-2t}\cos(-t)+ie^{-2t}\sin(-t)
\end{equation*}
would give the basic real solutions
\begin{equation*}
e^{-2t}\cos(t)\quad\text{and}\quad-e^{-2t}\sin(t)
\end{equation*}
Which would give the same general solution.\\
(next page)
\newpage
\noindent\textbf{More on complex roots}\\
We had the general solution
\begin{equation*}
c_1e^{-2t}\cos(t)+c_2e^{-2t}\sin(t)
\end{equation*}
See that the solution can be written in a different form:
\begin{equation*}
x(t)=e^{-2t}(c_1\cos(t)+c_2\sin(t))=Ae^{-2t}\cos(t-\phi)
\end{equation*}
This is a \textit{damped sinusoid} with \textit{circular pseudo-frequency} 1.\\
\vspace{1mm}\\
\textbf{Example}\\
Solving $\ddot{x}+\dot{x}+x=0$, the characteristic equation is $s^2+s+1=0$ and the roots
\begin{equation*}
\frac{-1\pm\sqrt{1-4}}{2}=\frac{-1}{2}\pm i\frac{\sqrt{3}}{2}
\end{equation*}
We have the complex exponential solutions
\begin{equation*}
z_1(t)=e^{(-1+i\sqrt{3})t/2},\quad z_2(t)=e^{(-1-i\sqrt{3})t/2} 
\end{equation*}
From this we obtain the basic real solutions
\begin{equation*}
\text{Re}(z_1(t))=e^{-t/2}\cos(\sqrt{3}t/2),\quad
\text{Im}(z_1(t))=e^{-t/2}\sin(\sqrt{3}t/2)
\end{equation*}
and therefore the general real solution
\begin{equation*}
e^{-t/2}(c_1\cos(\sqrt{3}t/2)+c_2\sin(\sqrt{3}t/2))
=Ae^{-t/2}\cos(\sqrt{3}t/2-\phi)
\end{equation*}
\textbf{In general}\\
In general, supposing the equation $m\ddot{x}+b\dot{x}+kx=0$ has the characteristic roots
$a\pm ib$, two real solutions are
\begin{equation*}
e^{at}\cos(bt)\quad\text{and}\quad e^{at}\sin(bt)
\end{equation*}
and the general real solution is
\begin{equation*}
c_1e^{at}\cos(bt)+c_2e^{at}\sin(bt)=Ae^{at}\cos(bt-\phi)
\end{equation*}
\newpage

\subsection{Repeated roots\\(homogeneous constant coefficient linear equations)}
\textbf{Illustrative example}\\
Consider $\ddot{x}+4\dot{x}+4x=0$. In this case the
characteristic equation:
\begin{equation*}
P(s)=s^2+4s+4=(s+2)^2
\end{equation*}
has $r=-2$ as a repeated root. The only exponential solution is $e^{-2}t$. 
To get the second basic exponential solution, see that
$te^{-2t}$ is also a solution. Our general solution is 
therefore
\begin{equation*}
x(t)=c_1e^{-2t}+c_2te^{-2t}
\end{equation*}
\textbf{Deriving solution for second order repeated roots}\\
Considering the DE $a\ddot{x}+b\dot{x}+cx=0$, from the characteristic equation we know
\begin{equation*}
r_{1,2}=\frac{-b\pm\sqrt{b^2-4ac}}{2a}
\end{equation*}
Should we only have one solution, as per the quadratic formula, we must have
\begin{equation*}
b^2-4ac=0,\quad\text{and}\quad r_{1,2}=-\frac{b}{2a}
\end{equation*}
We know one solution is $x_1=e^{-b/(2a)t}$. Consider a second solution of the form 
\begin{equation*}
x_2=v(t)x_1
\end{equation*}
we want to plug $x_2$ into the DE. For that we require its derivatives:
\begin{align*}
x_2'&=v'e^{-b/(2a)t}-\frac{b}{2a}ve^{-b/(2a)t}\\
x_2''&=v''e^{-b/(2a)t}-\frac{b}{2a}v'e^{-b/(2a)t}
-\frac{b}{2a}v'e^{-b/(2a)t}+\frac{b^2}{4a^2}ve^{-b/(2a)t}
\\&=v''e^{-b/(2a)t}-\frac{b}{a}v'e^{-b/(2a)t}+\frac{b^2}{4a^2}ve^{-b/(2a)t}
\end{align*}
Evaluating the DE with our proposed solution $x_2$:
\begin{align*}
a\left(v''e^{-b/(2a)t}-\frac{b}{a}v'e^{-b/(2a)t}+\frac{b^2}{4a^2}ve^{-b/(2a)t}\right)&+\\
b\left(v'e^{-b/(2a)t}-\frac{b}{2a}ve^{-b/(2a)t}\right)
&+c\left(ve^{-b/(2a)t}\right)=0
\end{align*}
Factoring out the exponential we get
\begin{align*}
&e^{-b/(2a)t}\left(av''-bv'+\frac{b^2}{4a}v+bv'-\frac{b^2}{2a}v+cv\right)\\
&=e^{-b/(2a)t}\left(av''+\left(-\frac{b^2}{4a}+c\right)v\right)\\
&=e^{-b/(2a)t}\left(av''-\frac{1}{4a}\left(b^2-4ac\right)v\right)=0
\end{align*}
(next page)
\newpage
\noindent\textbf{Derivation continued}\\ 
Evaluating the DE with our proposed solution $x_2=ve^{-b/(2a)t}$ gave us
\begin{equation*}
e^{-b/(2a)t}\left(av''-\frac{1}{4a}\left(b^2-4ac\right)v\right)=0
\end{equation*}
We know that $b^2-4ac=0$ (since the quadratic equation only has one solution as mentioned before). Thus since
exponentials cannot be zero, we have 
\begin{equation*}
av''=0\implies v''=0
\end{equation*}
(since $a\neq0$.) We can then determine $v(t)$:
\begin{equation*}
v'=\int v''dt=k\implies v=\int v'dt=k_1t+k_2
\end{equation*}
We therefore have our proposed basic solutions as
\begin{equation*}
x_1=e^{-b/(2a)t},\quad x_2=vx_1=(k_1t+k_2)e^{-b/(2a)t}
\end{equation*}
see that combining our solutions into a general solution via superposition gives
\begin{equation*}
x(t)=c_1e^{-b/(2a)t}+c_2(k_1t+k_2)e^{-b/(2a)t}
\end{equation*}
See that this can be simplified since $c_1,c_2,k_1,k_2$ are all unknown constants
\begin{equation*}
x(t)=(c_1+c_2k_2)e^{-b/(2a)t}+c_2k_1te^{-b/(2a)t}
\end{equation*}
We have
\begin{equation*}
x(t)=C_1e^{-b/(2a)t}+C_2te^{-b/(2a)t}
\end{equation*}
See that since we are dealing with a homogeneous equation,
$te^{-b/(2a)t}$ by itself satisfies the DE (because of superposition).
\newpage

\subsection{Damped harmonic oscillators}
\textbf{Spring-mass-dashpot}\\
Recall the model spring-mass-dashpot system with the constant coefficient linear DE:
\begin{figure}[h]
\begin{equation*}
m\ddot{x}+b\dot{x}+kx=F_\text{ext}
\end{equation*}
where $m$ is the mass, $b$ the damping constant, $k$ the spring constant, and $x(t)$ the displacement
of the mass from its equilibrium position.
\begin{center}
\includegraphics[width=10cm]{23}\\
\end{center}
Assuming zero external force ($F_\text{ext}=0$), we have
the homogeneous equation
\begin{equation*}
m\ddot{x}+b\dot{x}+kx=0
\end{equation*}
The algebra doesn't require any restrictions on $m,b,k$, except $m\neq0$ (for the equation to be second order
in the first place). But in this physical model we require $m>0,b\geq0$, and $k>0$.
\end{figure}\\
(next page)
\newpage
\noindent\textbf{Damped harmonic oscillator}
\begin{figure}[h]
The \textit{undamped} ($b=0$) system has the equation
\begin{equation*}
m\ddot{x}+kx=0
\end{equation*}
We have its solution as
\begin{equation*}
x(t)=c_1\cos(\omega t)+c_2\sin(\omega t)=A\cos(\omega t-\phi)
\end{equation*}
Here $\omega=\sqrt{k/m}$. The solution is always a sinusoid, thus we call this a 
\textit{simple harmonic oscillator}:
\begin{center}
\includegraphics[width=10cm]{24}\\
\end{center}
When we add damping ($b>0$) we then call the system a \textit{damped harmonic oscillator}.\\
\vspace{1mm}\\
This emphasises an important fact about uding DEs to model physical systems:
Any system modeled by the same equation will respond just like the spring-mass-dashpot (regardless of what $m,d,k,x$
represent). That is, all damped harmonic oscillators exhibit similar behaviour.
\end{figure}
\newpage

\subsection{Under, Over and Critical damping}
\textbf{Response to damping}\\
As we saw, the unforced damped harmonic oscillator has equation
\begin{equation*}
m\ddot{x}+b\dot{x}+kx=0
\end{equation*}
with $m>0,b\geq0$ and $k>0$. It has characteristic equation
\begin{equation*}
ms^2+bs+k=0
\end{equation*}
with characteristic roots
\begin{equation*}
\frac{-b\pm\sqrt{b^2-4mk}}{2m}
\end{equation*}
There are three cases depending on the sign of the expression under the square root:
\begin{enumerate}
\item $b^2<4mk$---\textit{Underdamping}
\item $b^2>4mk$---\textit{Overdamping}
\item $b^2=4mk$---\textit{Critical damping}
\end{enumerate}
\textbf{First case: Underdamping}\\
If $b^2<4mk$ the square root is negative and the characteristic roots are complex. See that the roots are given by
\begin{equation*}
-\frac{b}{2m}\pm i\omega_d,\quad\text{where }\omega_d=\frac{\sqrt{|b^2-4mk|}}{2m}
\end{equation*}
With that we have the complex exponential solutions
\begin{equation*}
e^{(-b/(2m)+i\omega_d)t},\quad e^{(-b/(2m)-i\omega_d)t}
\end{equation*}
With the basic real solutions
\begin{equation*}
e^{-bt/(2m)}\cos(\omega_dt),\quad e^{-bt/(2m)}\sin(\omega_dt)
\end{equation*}
The general real solution is found by taking linear combinations of two basic solutions:
\begin{equation*}
x(t)=c_1e^{-bt/(2m)}\cos(\omega_dt)+c_2e^{-bt/(2m)}\sin(\omega_dt)
\end{equation*}
This can also be written as
\begin{equation*}
x(t)=e^{-bt/(2m)}(c_1\cos(\omega_dt)+c_2\sin(\omega_dt))
=Ae^{-bt/(2m)}\cos(\omega_dt-\phi)
\end{equation*}
(next page)
\newpage
\noindent\textbf{Underdamping---Intuition}\\
We had the behaviour of an underdamped system as
\begin{equation*}
Ae^{-bt/(2m)}\cos(\omega_dt-\phi)
\end{equation*}
Intuitively, when $b=0$ the response is a sinusoid. In terms of the spring-mass-dashpot setup this corresponds to 
zero damping/no friction. When the damping constant $b$ is small (underdamped), we expect the system to still
oscillate, but with decreasing amplitude as its energy is lost to friction.\\
\vspace{1mm}\\
This can be seen from the response of the underdamped system. The factor $\cos(\omega_dt-\phi)$ represents
the oscillation while the exponential factor $e^{-bt/(2m)}$ has a negative exponent---thus representing 
a decaying amplitude. As $t\to\infty$, $x(t)\to0$:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{25}\\
\end{center}
We call $\omega_d$ the \textit{damped angular/circular frequency} of the system, or the \textit{psuedo-frequency}
of $x(t)$ (`psuedo' because $x(t)$ is not periodic and only periodic functions have a frequency).
\end{figure}\\
(next page)
\newpage
\noindent\textbf{Underdamping---Example}\\
Consider the system
\begin{equation*}
\ddot{x}+\dot{x}+3x=0
\end{equation*}
(See that $b^2<4mk$) We have the characteristic equation 
$s^2+s+3=0$ and the characteristic roots $-1/2\pm\sqrt{11}/2$. We have the basic real solutions as
\begin{equation*}
e^{-t/2}\cos(\sqrt{11}t/2),\quad e^{-t/2}\sin(\sqrt{11}t/2)
\end{equation*}
and the general solution
\begin{equation*}
x(t)=e^{-t/2}(c_1\cos(\sqrt{11}t/2)+c_2\sin(\sqrt{11}t/2))
=Ae^{-t/2}\cos(\sqrt{11}t/2-\phi)
\end{equation*}
The damped angular frequency, $\omega_d$, in this case is $\sqrt{11}/2$. Say we have the initial conditions 
$x(0)=1,\dot{x}(0)=0$, this gives us constants $c_1=1,c_2=1/\sqrt{11}$. So
\begin{align*}
x(t)&=e^{-t/2}\left(\cos(\sqrt{11}t/2)+\frac{1}{\sqrt{11}}\sin(\sqrt{11}t/2)\right)\\
&=\frac{\sqrt{12}}{\sqrt{11}}e^{-t/2}\cos(\sqrt{11}t/2-\phi)
\end{align*}
Where $\phi=\tan^{-1}(1/\sqrt{11})$. Plotted:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{26}\\
\end{center}
\end{figure}\\
(next page)
\newpage
\noindent\textbf{Overdamping}\\
Overdamping corresponds to the case where $b^2>4mk$ (distinct real roots). In this case \textit{both characteristic roots are negative}---see that it is always the case that
$b^2-4mk<b^2$ and therefore that $-b+\sqrt{b^2-4mk}<0$, while the other root is clearly negative. Thus we have
the real roots
\begin{equation*}
r_1=\frac{-b+\sqrt{b^2-4mk}}{2m},r_2=\frac{-b-\sqrt{b^2-4mk}}{2m}
\end{equation*}
(both $r$ are \textit{real and negative} and are \textit{different}) We have the general solution
\begin{equation*}
x(t)=c_1e^{r_1t}+c_2e^{r_2t}
\end{equation*}
Intuitively, this corresponds to the frictional force being so great that the system can't oscillate---the system
just goes asymptotically to the equilibrium $x=0$:
\begin{figure}[h]
\begin{center}
\includegraphics[width=7cm]{27}\\
\end{center}
\textbf{Example}\\
For instance, consider the system $\ddot{x}+4\dot{x}+3x=0$ with inital conditions $x(0)=1,\dot{x}(0)=0$. This has
characteristic equation $s^2+4s+3=0$ and characteristic roots $-1,-3$. Its general solution is
\begin{equation*}
x(t)=c_1e^{-t}+c_2e^{-3t}
\end{equation*}
Because the roots are real and different, this system is classified as overdamped. To satisfy the initial conditions
$c_1=3/2,c_2=-1/2$. So 
\begin{equation*}
x(t)=3/2e^{-t}-1/2e^{-3t}
\end{equation*}
\begin{center}
\includegraphics[width=10cm]{28}\\
\end{center}
Because $e^{-t}$ goes to 0 \textit{more slowly} than $e^{-3t/2}$ it controls the rate at which
$x$ goes to 0---the term that goes to zero slowest controls the rate.
\end{figure}\\
(next page)
\newpage
\noindent\textbf{Critical damping}\\
Critical damping corresponds to the case where $b^2=4mk$ (repeated real roots)\\---the characteristic polynomial 
has the roots $-b/2m,-b/2m$.
With repeated roots we have the solutions
\begin{equation*}
e^{-bt/(2m)},\quad te^{-bt/(2m)}
\end{equation*}
and the general solution
\begin{equation*}
e^{-bt/(2m)}(c_1+c_2t)
\end{equation*}
As in the overdamped case, this does not oscillate. It is worth noting that for a fixed $m$ and $k$, choosing $b$
to be the critically damping value gives the fastest return of the system to equilibrium:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{29}\\
\end{center}
\textbf{Intuition for rate of decay}\\
This can be seen from the roots. Considering fixed $m,k$ and only varying the damping constant $b$, 
in the overdamped case notice that one characteristic root is less negative than that of the 
critically damped case; since the less negative root controls the decay rate this leads to the rate of decay always
being smaller. See that $f=b$ has a higher derivative than
$g=\sqrt{b^2-4mk}$:
\begin{equation*}
\frac{df}{db}=1\quad\frac{dg}{db}=\frac{b}{\sqrt{b^2-4mk}}
>\frac{b}{\sqrt{b}}=1
\end{equation*}
For $m,k>0$. So $-b+\sqrt{b^2-4mk}$ only becomes less negative (slower rate of decay).\\
\vspace{1mm}\\
For the underdamped case to exist we must have $b<4mk$. See that this $b$ is always smaller than in the critically
damped case, which corresponds to $b=4mk$.
\end{figure}
\newpage

\subsection{Superposition (Second order ODEs)}%100524--\underbrace, \quad, align, \left[\right]
The Principle of Superposition for Second Order Differential Equations; if
\begin{equation*}
\frac{d^2y}{dt^2}+p(t)\frac{dy}{dt}+q(t)y=0
\end{equation*}
is a second order linear differential equation and $y=y_1(t)$ and $y=y_2(t)$ are both
solutions to this differential equation, then for $C$ and $D$ as constants,
\begin{equation*}
y=Cy_1(t)+Dy_2(t)\quad\text{is also a solution}
\end{equation*}
Essentially, any linear combination of solutions is also a solution.\\
\vspace{1mm}\\
\textit{Proof}: Consider $y=y_1$ and $y=y_2$ are solutions to the second
order linear differential equation $\frac{d^2y}{dt^2}+p(t)\frac{dy}{dt}+q(t)y=0$. Then we have
that:
\begin{equation*}
\frac{d^2y_1}{dt^2}+p(t)\frac{dy_1}{dt}+q(t)y_1=0\quad\text{and}\quad\frac{d^2y_2}{dt^2}+p(t)\frac{dy_2}{dt}+q(t)y_2=0
\end{equation*}
If $C$ and $D$ are constants, plugging in $y=Cy_1(t)+Dy_2(t)$:
\begin{align*}
&\frac{d^2}{dt^2}(Cy_1(t)+Dy_2(t))+p(t)\frac{d}{dt}(Cy_1(t)+Dy_2(t))
+q(t)(Cy_1(t)+Dy_2(t))\\
&=C\frac{d^2y_1}{dt^2}+D\frac{d^2y_2}{dt^2}+p(t)C\frac{dy_1}{dt}+p(t)D\frac{dy_2}{dt}
+q(t)Cy_1+q(t)Dy_2\\
&=C\underbrace{\left[\frac{d^2y_1}{dt^2}+p(t)\frac{dy_1}{dt}+q(t)y_1\right]}_{=0}
+D\underbrace{\left[\frac{d^2y_2}{dt^2}+p(t)\frac{dy_2}{dt}+q(t)y_2\right]}_{=0}\\
&=0
\end{align*}
Therefore, $y=Cy_1(t)+Dy_2(t)$ is also a solution. Note that the superposition principle
\textbf{does not} work for nonlinear differential equations.\\
(next page)
\newpage
\noindent\textbf{In context of inhomogenous differential equations}\\ %120524 \textit{}, \textbf{}
In addition, if $y_1$ is a solution to:
\begin{equation*}
\frac{d^2y}{dt^2}+p(t)\frac{dy}{dt}+q(t)y=f_1(t)
\end{equation*}
and $y_2$ is a solution to:
\begin{equation*}
\frac{d^2y}{dt^2}+p(t)\frac{dy}{dt}+q(t)y=f_2(t)
\end{equation*}
then for constants $C$ and $D$, $Cy_1+Dy_2$ is a solution to:
\begin{equation*}
\frac{d^2y}{dt^2}+p(t)\frac{dy}{dt}+q(t)y=Cf_1(t)+Df_2(t)
\end{equation*}
\textit{Proof}: Plugging in $y=Cy_1+Dy_2$:
\begin{align*}
&\frac{d^2}{dt^2}(Cy_1+Dy_2)+p(t)\frac{d}{dt}(Cy_1+Dy_2)+q(t)(Cy_1+Dy_2) \\
&=C\frac{d^2y_1}{dt^2}+D\frac{d^2y_2}{dt^2}+p(t)C\frac{dy_1}{dt}+p(t)D\frac{dy_2}{dt}
+q(t)Cy_1+q(t)Dy_2\\
&=C\underbrace{\left[\frac{d^2y_1}{dt^2}+p(t)\frac{dy_1}{dt}+q(t)y_1\right]}_{=f_1(t)}
+D\underbrace{\left[\frac{d^2y_2}{dt^2}+p(t)\frac{dy_2}{dt}+q(t)y_2\right]}_{=f_2(t)}\\
&=Cf_1(t)+Df_2(t)
\end{align*}
Superposition is therefore \textit{not} limited to homogenous equations.

\subsection{General solution for inhomogenous linear ODEs} %120524
Therefore, to get the general solution $y(t)$ to an inhomogenous linear ODE:
\begin{align*}
\text{inhomogenous:}\quad\frac{d^2y}{dt^2}+p(t)\frac{dy}{dt}+q(t)y=f(t)
\end{align*}
1. Find the general solution $y_h$ to the associated \textbf{homogenous} equation:
\begin{align*}
\text{homogenous:}\quad\frac{d^2y_h}{dt^2}+p(t)\frac{dy_h}{dt}+q(t)y_h=0
\end{align*}
2. Find (in some way) any \textbf{one particular solution} $y_p$ to the \textbf{inhomogenous} ODE.
\\
3. Add $y_p$ to $y_h$ to get the general solution to the inhomogenous ODE:
\begin{align*}
\underbrace{y}_{\text{general \textbf{inhomogenous} solution}}=
\underbrace{y_p}_{\text{\textbf{any particular solution}}}+
\underbrace{y_h}_{\text{general \textbf{homogenous} solution}}
\end{align*}
Note that the superposition principle \textbf{does not} work for nonlinear differential equations.
\newpage

\subsection{Superposition of homogeneous equations only requires linearity}
Consider the illustrative example. Given the ODE
\begin{equation*}
t^2y''+ty'-4y=0
\end{equation*}
One can check by substitution that $y_1(t)=t^2$ and $y_2(t)=1/t^2$ are both solutions. Thus
\begin{equation*}
y(t)=c_1t^2+c_2/t^2
\end{equation*}
Is a solution for any $c_1,c_2$.
Point here is that \textit{we didn't need the differential equation to have constant coefficients}: linearity and 
homogeneity is sufficient.

\subsection{Existence and uniqueness} %130524 \vspace{}
Solving a first-order linear ODE leads to a 1-parameter family of solutions (a general solution).
To derive a specific solution, we need an initial condition, such as $y(0)$. One may wonder if
there are other solutions. Here is a general result which says that there aren't and confirms 
that our methods find all solutions:\\
\vspace{2mm}\\
\textbf{Existence and uniqueness theorem for a linear ODE:}\\
Let $p(t)$ and $q(t)$ be continuous functions on an open interval $I$. Let $a\in I$, and let $b$
be a given number. Then there \textbf{exists} a \textbf{unique} solution defined on the entire 
interval $I$ to the first order linear ODE
\begin{equation*}
\dot{y}+p(t)y=q(t)
\end{equation*}
satisfying the initial condition
\begin{equation*}
y(a)=b
\end{equation*}
\textbf{Existence} means there is \textbf{at least one} solution.\\
\textbf{Uniqueness} means that there is \textbf{only one} solution.
\newpage

\subsection{Exponential input (2nd order ODEs)\\---exponential response}
\textbf{Context}\\
Consider a case where the driving function (the external force in a spring-mass-dashpot system) is
an exponential $Be^{at}$:
\begin{equation*}
mx''+bx'+kx=Be^{at}
\end{equation*}
where $B,a$ are constants. We obtain a \textit{particular} solution by considering a solution of the form $Ae^{at}$
(same as the first order case). Substituting gives us
\begin{align*}
mx''+bx'+kx&=ma^2Ae^{at}+baAe^{at}+kAe^{at}\\
&=(ma^2+ba+k)Ae^{at}
\end{align*}
Thus we get (by comparing coefficients)
\begin{equation*}
A=\frac{B}{ma^2+ba+k}
\end{equation*}
See that the denominator is just the characteristic polymonial, which we will denote as $p(a)$ (this is not
suprising, since the characteristic polynomial is derived
using the same principles).\\
\vspace{1mm}\\
This concept isn't new, the same idea was used in the context of exponential/sinusoidal input to first order ODEs.\\
\vspace{1mm}\\
\textbf{Exponential response formula (ERF)}\\
Considering the second order equation
\begin{equation*}
mx''+bx'+kx=Be^{at}
\end{equation*}
and letting $p(r)=mr^2+br+k$ be its characteristic polynomial, then
\begin{equation*}
x(t)=\frac{B}{p(a)}e^{at}
\end{equation*}
is a \textit{particular solution}, as long as $p(a)\neq0$.\\
(next page)
\newpage
\noindent\textbf{Example}\\
Consider finding the \textit{general} solution to
\begin{equation*}
x''+8x'+7x=9e^{2t}
\end{equation*}
considering solutions of the form $x(t)=Ae^{2t}$, substituting gives us
\begin{equation*}
x''+8x'+7x=Ae^{2t}\underbrace{(4+16+7)}_{p(2)}=27Ae^{2t}
\end{equation*}
Comparing coefficients gives us
\begin{equation*}
A=\frac{\overbrace{9}^{B}}{\underbrace{27}_{p(2)}},\quad x_p=\frac{1}{3}e^{2t}
\end{equation*}
This is our \textit{particular} solution (see that the ERF is just a shortcut of this reasoning). To obtain 
a general solution we need to solve the homogeneous equation:
\begin{equation*}
x''+8x'+7x=0
\end{equation*}
Where we get the homogeneous solution
\begin{equation*}
x_h=c_1e^{-7t}+c_2e^{-t}
\end{equation*}
Thus the general solution to the original equation is
\begin{equation*}
x=x_h+x_p=c_1e^{-7t}+c_2e^{-t}+\frac{1}{3}e^{2t}
\end{equation*}
\newpage

\subsection{Sinusoidal input---complex replacement}
\textbf{General case, Gain}\\
The ERF can be applied using complex replacement to solve equations with sinusoidal driving. Consider the
general case:
\begin{equation*}
mx''+bx'+kx=B\cos(\omega t)
\end{equation*}
can be \textit{complexified} as
\begin{equation*}
mz''+bz'+kz=Be^{i\omega t}
\end{equation*}
where $z=x+iy$, $y$ defined as
\begin{equation*}
my''+by'+ky=B\sin(\omega t)
\end{equation*}
Now we solve for $z_p$ using the ERF:
\begin{equation*}
z_p=\frac{B}{p(i\omega)}e^{i\omega t}
\end{equation*}
We get $x_p$ by taking Re$(z)$:
\begin{equation*}
x_p=\frac{B}{|p(i\omega)|}\cos(\omega t-\phi)
\end{equation*}
where $|p(i\omega)|$ refers to the magnitude of $p(i\omega)$, and $\phi=$Arg$(p(i\omega))$.\\
\vspace{1mm}\\
See that the output looks similar to the input---apart from the phase lag, the output is a
multiple of the input by a factor of $1/|p(i\omega)|$. This factor is called the \textit{gain} of the system:
\begin{equation*}
\text{output amplitude}=\text{gain}\times\text{input amplitude}
\end{equation*}
(next page)
\newpage
\noindent\textbf{Example}\\
Say we want the general solution to 
\begin{equation*}
x''+8x'+7x=9\cos(2t)
\end{equation*}
using complex replacement we instead solve
\begin{equation*}
z''+8z'+7z=9e^{2it}
\end{equation*}
Now we can apply the ERF:
\begin{align*}
z_p(t)&=\frac{9}{p(2i)}e^{2it}\\
&=\frac{9}{(2i)^2+16i+7}e^{2it}\\
&=\frac{9}{3+16i}e^{2it}
\end{align*}
Simplify by expressing $p(2i)$ in polar form:
\begin{align*}
\frac{9}{3+16i}e^{2it}&=9\left(\frac{1}{\sqrt{265}}e^{-i\phi}\right)e^{2it}\\
&=\frac{9}{\sqrt{265}}e^{i(2t-\phi)}
\end{align*}
where $\phi=$Arg$(p(2i))=\tan^{-1}(16/3)$. We can then obtain $x_p$ as
\begin{equation*}
x_p(t)=\text{Re}(z_p(t))=\frac{9}{\sqrt{265}}\cos(2t-\phi)
\end{equation*}
To get the general solution we add the homogeneous solution:
\begin{equation*}
x_h(t)=c_1e^{-7t}+c_2e^{-t}
\end{equation*}
To obtain
\begin{equation*}
x(t)=x_p+x_h=\frac{9}{\sqrt{265}}\cos(2t-\phi)+c_1e^{-7t}+c_2e^{-t}
\end{equation*}
In this case the gain is
\begin{equation*}
1/|p(2i)|=1/\sqrt{265}
\end{equation*}
\newpage

\subsection{Simple harmonic oscillator---\\intuition for resonance}
\textbf{Natural frequency}\\
We consider the undamped harmonic oscillator $(b=0)$
\begin{equation*}
mx''+kx=F_{\text{ext}}(t)
\end{equation*}
we define the \textit{natural frequency} $\omega_n$ as
\begin{equation*}
\omega_n=\sqrt{k/m}
\end{equation*}
this comes from considering the system with no driving force $F_{\text{ext}}(t)=0$, in which
case the characteristic equation $p(r)=mr^2+k$ has the roots $\pm i\sqrt{k/m}$. This leads to the general solution
\begin{equation*}
x_h=c_1\cos((\sqrt{k/m})t)+c_2\sin((\sqrt{k/m})t)
\end{equation*}
With the natural frequency defined the unforced solution is
\begin{equation*}
x_h=c_1\cos(\omega_nt)+c_2\sin(\omega_nt)
\end{equation*}
and the original equation looks like
\begin{equation*}
m(x''+\omega_n^2x)=F_{\text{ext}}(t)
\end{equation*}
\textbf{Intuition for resonance}\\
Now consider adding sinusoidal input $F_{\text{ext}}(t)=B\cos(\omega t)$:
\begin{equation*}
m(x''+\omega_n^2x)=B\cos(\omega t)
\end{equation*}
Using complex replacement, we seek to obtain the $z_p$ in
\begin{equation*}
m(x''+\omega_n^2x)=Be^{i\omega t}
\end{equation*}
Applying the ERF with $a=i\omega$ we get
\begin{equation*}
z_p=\frac{B}{p(i\omega)}e^{i\omega t}=\frac{B}{m(\omega_n^2-\omega^2)}e^{i\omega t}
\end{equation*}
(Notice the lack of a damping makes the denominator non-complex.) Taking the real part to obtain $x_p$:
\begin{equation*}
x_p=\text{Re}(z_p)=\frac{B}{m(\omega_n^2-\omega^2)}\cos(\omega t)
\end{equation*}
With the homogeneous solution, our general solution is
\begin{equation*}
x_h=c_1\cos(\omega_nt)+c_2\sin(\omega_nt)+\frac{B}{m(\omega_n^2-\omega^2)}\cos(\omega t)
\end{equation*}
(next page)
\newpage
\noindent\textbf{Intuition for resonance (cont.)}\\
We had the particular solution
\begin{equation*}
\frac{B}{m(\omega_n^2-\omega^2)}\cos(\omega t)
\end{equation*}
See that the gain is $1/|p(a)|=1/m(\omega_n^2-\omega^2)$---the closer $\omega$ is to $\omega_n$. 
Setting $m=1,B=1$ and $\omega_n=2$ while varying $\omega$ (the frequency of the driving):
\begin{center}
\includegraphics[width=10cm]{30}\\
\end{center}
See that the solution breaks down when $\omega=\omega_n$. This is called \textbf{pure resonance}; notice it corresponds to the case $p(a)=0$.\\
(next page)
\newpage
\noindent\textbf{The workaround solution---resonant response formula}\\
When $p(a)=0$ the initial solution we found breaks down. It can be checked that the following is a solution 
\textit{to the complexified equation}:
\begin{equation*}
z_p(t)=\frac{B}{p'(a)}te^{at}
\end{equation*}
where taking the real part gives a valid $x_p$ (which can also be checked):
\begin{equation*}
x_p(t)=\frac{B}{2m\omega}t\sin(\omega t)
\end{equation*}
See the extra factor of $t$ before the sine term---the amplitude of the response in the case of pure resonance
grows with time. 
\begin{center}
\includegraphics[width=10cm]{31}\\
\end{center}
The following is a counterpart to the ERF in the pure resonance case, when $p(a)=0$. It is called the 
\textit{Resonant Response Formula (RRF)}; essentially, in the second order equation
\begin{equation*}
mx''+bx'+kx=Be^{at}
\end{equation*}
with characteristic polynomial $p$, if $p(a)=0$ and $p'(a)\neq0$, then 
\begin{equation*}
x(t)=\frac{B}{p'(a)}te^{at}
\end{equation*}
is a \textit{particular} solution.
\newpage

\subsection{Stability}
\textbf{The notion of Stability}\\
A system is called \textit{stable} if its long-term behaviour does not depend significantly 
on the initial conditions.\\
\vspace{1mm}\\
\textbf{In this context}\\
In terms of DEs, the simplest spring-mass system or RLC-circuit is represented as
\begin{equation*}
a_0y''+a_1y'+a_2y=r(t),\quad a_i\text{ constants, }t=\text{time.}
\end{equation*}
By the theory of inhomogeneous equations, the general solution to this is of the form
\begin{equation*}
y=c_1y_1+c_2y_2+y_p,\quad\text{$c_1,c_2$ arbitrary constants}
\end{equation*}
where $y_p$ is a \textit{particular solution}, and $c_1y_1+c_2y_2$ is the \textit{complementary function} (the
general solution to the associated homogeneous equation).\\
\vspace{1mm}\\
See that the initial conditions determine the exact values of $c_1$ and $c_2$. (the same initial conditions
might lead to different constants for different particular solutions, but considering a
fixed particular solution, they are controlled by the initial conditions) See that
\begin{equation*}
\text{\textit{the system is stable}}\iff\forall c_1,c_2\quad\lim_{t\to\infty}(c_1y_1+c_2y_2)=0
\end{equation*}
If the ODE is stable, the two parts of the solution can be classified as
\begin{equation*}
y_p=\text{\textit{steady-state solution}},\quad c_1y_1+c_2y_2=\text{\textit{transient}}
\end{equation*}
The effects of transient term, which depends on the initial conditions, disappear over time. 
The steady state term therefore more and more closely represents the response of the system as time goes to $\infty$, regardless of initial conditions.\\
(next page)
\newpage
\noindent\textbf{Conditions for stability (second order case)}\\
First we consider the second order case. See that stability concerns just the behaviour of the solutions to
the associated homogeneous equation
\begin{equation*}
a_0y''+a_1y'+a_2y=0
\end{equation*}
(the forcing term only determines the particular solution. what matters for stability is that the homogeneous
solution decays to 0---so the response resembles the particular solution regardless of constants of integration) 
See that the conditions can be split into three cases, summarised as follows:
\begin{table}[h]
\begin{center}
\begin{tabular}{c c c}
\hline
roots & (homogeneous) solution & condition for stability\\
\hline
$r_1\neq r_2$&$c_1e^{r_1t}+c_2e^{r_2t}$&$r_1<0,r_2<0$\\
$r_1=r_2$&$e^{r_1t}(c_1+c_2t)$&$r_1<0$\\
$a\pm ib$&$e^{at}(c_1\cos bt+c_2\sin bt)$&$a<0$\\
\hline
\end{tabular}
\end{center}
\end{table}\\
\textbf{Root, coefficient form for second order case}\\
The three cases can be summarised as (root form)
\begin{equation*}
a_0y''+a_1y'+a_2y=r(t)\text{ is stable}\iff\parbox{13em}{all roots of $a_0r^2+a_1r+a_2=0$\\
have negative real part.}
\end{equation*}
it can be further shown that (coefficient form) \textit{assuming $a_0>0$:}
\begin{equation*}
a_0y''+a_1y'+a_2y=r(t)\text{ is stable}\iff a_0,a_1,a_2>0
\end{equation*}
See that the characteristic equation can be written as
\begin{equation*}
r^2+\frac{a_1}{a_0}r+\frac{a_2}{a_0}=r^2+br+c=0
\end{equation*}
by the quadratic formula
\begin{equation*}
(-b\pm\sqrt{b^2-4c})/2
\end{equation*}
First considering the case where both roots are complex, then the \textit{real} part of each of them is $-b/2$.
The condition for stability is that the real part is negative, therefore for stability we must have $b>0$. 
A similar principle applies for the case of repeated roots, where the real part is again given by $-b/2$, 
so $b>0$ for stability.\\
\vspace{1mm}\\
Now consider the case of two real roots. See that $(-b+\sqrt{b^2-4c})/2>0$ if $b<\sqrt{b^2-4c}$, which only occurs
when $c<0$ (assuming the square root gives a real result; also the other root is always less than 0).
Therefore to have both real roots be negative we must have $c>0$.\\
\vspace{1mm}\\
Assuming $a_0>0$, we must have $a_1,a_2>0$.\\
(next page)
\newpage
\noindent\textbf{Stability of higher order ODEs (root form only)}\\
The stability criterion in the root form can also be applied to higher-order ODEs with constant coefficients:
\begin{equation*}
(a_0D^n+a_1D^{n-1}+\ldots+a_{n-1}D+a_n)y=f(t)
\end{equation*}
where the characteristic equation looks like
\begin{equation*}
a_0r^n+a_1r^{n-1}+\ldots+a_{n-1}r+a_n=0
\end{equation*}
The real and complex roots of the characteristic equation give rise to solutions to the associated homogeneous equation just as they do in the second order case (for a $k$-fold repeated root, one gets additional solutions by 
multiplying $1,t,t^2,\ldots,t^{k-1}$).\\
\vspace{1mm}\\
Thus the stability criterion for the second order case can be extrapolated to the higher order case:
\begin{equation*}
n\text{-th order ODE is stable}\iff\text{all roots have negative real parts}
\end{equation*}
\newpage

\subsection{$p(D)$ Notation, Gain, Phase lag, Complex Gain}
\textbf{$p(D)$ Notation}\\
Consider the typical linear constant coefficient DE
\begin{equation*}
a_nx^{(n)}+a_{n-1}x^{(n-1)}+\ldots+a_1x'+a_0x=q(t)
\end{equation*}
as usual, this has the characteristic polynomial
\begin{equation*}
p(s)=a_ns^n+a_{n-1}s^{n-1}+\ldots+a_1s+a_0
\end{equation*}
We write $D=\frac{d}{dt}$ to represent the operation of differentiation applied to functions of $t$, 
where for instance $Dx=\frac{dx}{dt}$. Similarly
we have the representation $D^2=\frac{d^2}{dt^2}$ for differentiation twice. See that this notation can be fit
into the characteristic polynomial:
\begin{equation*}
p(D)=a_nD^n+a_{n-1}D^{n-1}+\ldots+a_1D+a_0
\end{equation*}
which allows us to represent the initial DE as
\begin{equation*}
p(D)x=q
\end{equation*}
giving us an efficient representation.\\
\vspace{1mm}\\
\textbf{Context for Gain, Phase lag}\\
We have the \textit{transience theorem}: All solutions $x=x(t)$ to the linear homogeneous constant coefficient DE
\begin{equation*}
p(D)x=0
\end{equation*}
decay to 0 as $t\to\infty$ exactly when the roots $r$ of the characteristic polynomial $p(s)$ have negative
real part.\\
\vspace{1mm}\\
In this case the homogeneous solutions are called \textit{transients}; by superposition, 
should we fix a driving term, and therefore a particular solution, all solutions then converge to the 
\textit{same} solution as $t$ gets large, and we say that
the DE is \textit{stable}. Therefore if we have a system modelled by a stable equation, and are only interested in
what happens after the transients have died down, we can ignore the initial condition:
\begin{center}
\includegraphics[width=10cm]{32}\\
\end{center}
The solutions converge to the particular solution $x_p$. When the input signal is sinusoidal, recall 
that the particular solution will also be sinusoidal. Here we review this.\\
(next page)
\newpage
\noindent\textbf{Review of Complex replacement}\\
We find the particular solution using the ERF, where the DE
\begin{equation*}
p(D)x=Be^{at}
\end{equation*}
has the particular solution
\begin{equation*}
x_p=\frac{Be^{at}}{p(a)}
\end{equation*}
provided $p(a)\neq0$ (this corresponds to the resonant case).
Given sinusoidal input:
\begin{equation*}
p(D)x=B\cos(\omega t)
\end{equation*}
We consider the complex equation since $B\cos(\omega t)=\text{Re}(Be^{i\omega t})$:
\begin{equation*}
p(D)z=Be^{i\omega t},\quad x=\text{Re}(z)
\end{equation*}
The ERF gives
\begin{equation*}
z_p=\frac{B}{p(i\omega)}e^{i\omega t}\implies x_p=B\,\text{Re}\left(\frac{e^{i\omega t}}{p(i\omega)}\right)
\end{equation*}
Thus
\begin{equation*}
x_p=\frac{B}{|p(i\omega)|}\cos(\omega t-\phi)
\end{equation*}
where $\phi=\text{Arg}(p(i\omega))$. This is the particular, and therefore steady-state (periodic) solution.\\
\vspace{1mm}\\
\textbf{Gain, Phase lag}\\
Compare the periodic input $q(t)=B\cos(\omega t)$ and its
periodic output $x_p(t)=\frac{B}{|p(i\omega)|}\cos(\omega t-\phi)$. See that the amplitude is scaled by 
$1/|p(i\omega)|$ and the output sinusoid is shifted by an angle $\phi=\text{Arg}(p(i\omega))$ relative
to the input sinusoid. This motivates the following definitions: for a constant coefficient linear DE
\begin{equation*}
P(D)x=q(t)\quad\text{with sinusoidal input } q(t)
\end{equation*}
\begin{enumerate}
\item The \textit{gain} is defined to be the \textit{ratio of the amplitude of the output sinusoid to the 
amplitude of the input sinusoid}.
\item The \textit{phase lag} is defined to be the \textit{angle by which the output
sinusoid is shifted relative to the input sinusoid}.
\end{enumerate}
\textbf{Complex Gain}\\
When solving by complex replacement we had $x_p=\text{Re}(z_p)$ where $z_p(t)$ is the complex solution
to $p(D)z=Be^{i\omega t}$:
\begin{equation*}
z_p=\frac{B}{p(i\omega)}e^{i\omega t}
\end{equation*}
see that the complexified exponential input can also be compared to the complex output. We therefore define the 
\textit{complex gain}, in this case it is $1/p(i\omega)$.
\newpage

\subsection{Polynomial Input: Undetermined Coefficients}
\textbf{Definition}\\
A \textit{polynomial} is a function of the form
\begin{equation*}
q(x)=a_nx^n+a_{n-1}x^{n-1}+\cdots+a_0
\end{equation*}
where the largest $k$ for which $a_k\neq0$ is the \textit{degree} of $q(x)$ (0 is also a polynomial with no degree).
\\
\vspace{1mm}\\
\textbf{Theorem. (Undetermined coefficients)}: If $p(0)\neq
0$, and $q(x)$ is a polynomial of degree $n$, then
\begin{equation*}
p(D)y=q(x)
\end{equation*}
has exactly one solution which is a polynomial, and is of degree $n$.\\
\vspace{1mm}\\
\textbf{Instructive example}\\
We now show this with an instructive example. Consider
finding a particular solution $y_p$ to
\begin{equation*}
y''+3y'+4y=4x^2-2x
\end{equation*}
Consider a polynomial solution with the same degree as the input---of the form $y_p=Ax^2+Bx+C$. See that
calculating its derivatives gives us
\begin{align*}
y_p&=Ax^2+Bx+C\\
y'_p&=2Ax+B\\
y''_p&=2A
\end{align*}
Substituting into the DE gives us
\begin{align*}
&2A+3(2Ax+B)a+4(Ax^2+Bx+C)\\
=&4Ax^2+(4B+6A)x+(4C+3B+2A)=4x^2-2x
\end{align*}
Comparing coefficients we have $A=1,B=-2,C=1$. So
we obtain the particular solution $y_p=x^2-2x+1$.\\
(next page)
\newpage
\noindent\textbf{Another example}\\
Consider
\begin{equation*}
y''+5y'+4y=2x+3
\end{equation*}
We have a \textit{trial solution} of the form $y_p=Ax+B$ (same degree as input). Substituting gives us
\begin{equation*}
0+5A+4(Ax+B)=2x+3
\end{equation*}
Comparing coefficients and solving gives us $A=1/2,B=1/8$.
We obtain the particular solution
\begin{equation*}
y_p=\frac{1}{2}x+\frac{1}{8}
\end{equation*}
Should we want a general solution we add the homogeneous solution.\\
\vspace{1mm}\\
\textbf{Case where $p(0)=0$ or homogeneous DE has polynomial solutions}\\
Should $p(0)=0$, then coefficient of $y$ in the equation is 0, this looks like
\begin{equation*}
a_2y''+a_1y'=q
\end{equation*}
See that in this case the homogeneous DE $p(D)$ has polynomial solutions (constants are polynomials, and any
constant $y_p$ would satisfy the homogeneous differential equation.\\
\vspace{1mm}\\
In this case, the polynomial solution to the inhomogeneous DE $p(D)y=q$ would be of \textit{higher degree} than 
that of $q(x)$; consider the example
\begin{equation*}
y''+y'=x+1
\end{equation*}
A particular solution of the form $y_p=Ax+B$ (same degree as input) doesn't work---substituting gives
$0+A=x+1$. We fix this by considering a solution one degree higher $y_p=Ax^2+Bx$, where substituting gives us
\begin{equation*}
2Ax+(2A+B)=x+1\implies A=1/2,B=0\implies y_p=\frac{1}{2}x^2
\end{equation*}
\textbf{Another example}\\
Consider
\begin{equation*}
y'''+3y''=x^2+x
\end{equation*}
Lowest order derivative is 2, so we try increasing the degree of the proposed solution by 2: $y_p=Ax^4+Bx^3+Cx^2$.
Substituting, we have
\begin{equation*}
(24Ax+6B)+3(12Ax^2+6Bx+2C)=x^2+x
\end{equation*}
so $A=1/36,B=1/27,C=-1/27$.
\newpage

\subsection{Linear Operators}
\textbf{Polynomial differential operator}\\
The general linear ODE with constant coefficients of order $n$ for a function $y=y(t)$:
\begin{equation*}
y^{(n)}+a_1y^{(n-1)}+\ldots+a_ny=q(t)
\end{equation*}
can be written compactly using the differentiation operator $D=\frac{d}{dt}$:
\begin{equation*}
p(D)y=q(t)
\end{equation*}
where
\begin{equation*}
p(D)=D^n+a_1D^{n-1}+\ldots+a_n
\end{equation*}
we call $p(D)$ a \textit{polynomial differential operator with constant coefficients}.\\
\vspace{1mm}\\
\textbf{Properties}\\
We now state a few rules surrounding these operators. We will assume that the functions involved are sufficiently
differentiable so that the operators can be applied.\\
\vspace{1mm}\\
\textbf{Sum rule}: If $p(D)$ and $q(D)$ are polynomial operators, then for any (sufficiently differentiable) 
function $u$,
\begin{equation*}
[p(D)+q(D)]u=p(D)u+q(D)u
\end{equation*}
\textbf{Linearity rule}: If $f,g$ are functions and $c_1,c_2$ constants,
\begin{equation*}
p(D)(c_1f+c_2g)=c_1p(D)f+c_2p(D)g
\end{equation*}
\textbf{Proof of linearity rule}: This follows from the linearity of differentiaton, see that
\begin{equation*}
D(c_1f+c_2g)=(c_1f+c_2g)'=c_1f'+c_2g'=c_1Df+c_2Dg
\end{equation*}
also see that taking the second or higher derivative also follows the linearity rule. That is,
\begin{equation*}
D^n(c_1f+c_2g)=\frac{d^n}{dt}(c_1f+c_2g)=c_1f^{(n)}+c_2g^{(n)}=c_1D^nf+c_2D^ng
\end{equation*}
We can scale the linear operator $D^n$ by $a$ (a constant/function/or whatever independent variable) and it stays
linear:
\begin{equation*}
aD^n(c_1f+c_2g)=a\frac{d^n}{dt}(c_1f+c_2g)=c_1af^{(n)}+c_2ag^{(n)}=c_1aD^nf+c_2aD^ng
\end{equation*}
Finally we can combine these operators into a polynomial operator
\begin{equation*}
D^n+a_1D^{n-1}+\ldots+a_n
\end{equation*}
which clearly still obeys the linearity rule (consider $c_1f+c_2g$ a single function $u$ and apply
sum rule to each term in the differential polynomial operator).\\
(next page)
\newpage
\noindent\textbf{Multiplication rule}: If $p(D)=g(D)h(D)$ as polynomials in $D$, then
\begin{equation*}
p(D)u=g(D)(h(D)u)
\end{equation*}
First consider the simple operator $aD^k$ and see that
\begin{equation*}
D^m(aD^ku)=aD^{m+k}u
\end{equation*}
(Note that in this case $a$ must be a constant.) This then extends to general polynomial operators $h(D)$ by 
the linearity rule.\\
\vspace{1mm}\\
An important corollary of the multiplication property is that \textit{polynomial operators with constant
coefficients commute}:
\begin{equation*}
g(D)(h(D)u)=h(D)(g(D)u)
\end{equation*}
This can be shown since $g(D)h(D)=h(D)g(D)=p(D)$, so both sides of the equation are equal to $p(D)u$.\\
\vspace{1mm}\\
\textbf{Substitution rule}
\begin{equation*}
p(D)e^{at}=p(a)e^{at}
\end{equation*}
See that by repeated differentiation
\begin{equation*}
De^{at}=ae^{at},D^2e^{at}=a^2e^{at},\ldots,D^ke^{at}=a^ke^{at}
\end{equation*}
therefore;
\begin{equation*}
(D^n+c_1D^{n-1}+\ldots+c_n)e^{at}=(a^n+c_1a^{n-1}+\ldots+c_n)e^{at}
\end{equation*}
This is the substitution rule.\\
(next page)
\newpage
\noindent\textbf{Exponential-shift rule}: This handles expressions such as $t^ke^{at}$ and $t^k\sin(at)$. 
Let $u=u(t)$. Then
\begin{equation*}
p(D)e^{at}u=e^{at}p(D+a)u
\end{equation*}
\textbf{Proof}: First see that when $p(D)=D$, by product rule
\begin{equation*}
De^{at}u(t)=e^{at}Du(t)+ae^{at}u(t)=e^{at}(D+a)u(t)
\end{equation*}
To show this rule is true for $D^k$, we apply the differentiation repeatedly:
\begin{align*}
D^2e^{at}u=D(De^{at}u)&=D(e^{at}(D+a)u)\\
&=e^{at}(D+a)((D+a)u)\\
&=e^{at}(D+a)^2u
\end{align*}
in the same way,
\begin{align*}
D^3e^{at}u=D(D^2e^{at}u)&=D(e^{at}(D+a)^2u)\\
&=e^{at}(D+a)((D+a)^2u)\\
&=e^{at}(D+a)^3u
\end{align*}
and so on; see therefore that 
\begin{equation*}
D^ke^{at}u=e^{at}(D+a)^ku
\end{equation*}
and 
\begin{equation*}
(D^n+c_1D^{n-1}+\ldots+c_n)e^{at}u(t)=((D+a)^n+c_1(D+a)^{n-1}+\ldots+c_n)e^{at}u(t)
\end{equation*}
\newpage

\subsection{Time Invariance}
\textbf{Definition}\\
In the case of \textit{constant coefficient} operators $p(D)$, there is an important 
relationship between solutions of to $p(D)x=q(t)$ for input signals $q(t)$. The following result shows why these
operators are called `Linear Time Invariant' (or LTI).\\
\vspace{1mm}\\
\textbf{Translation invariance}: If $p(D)$ is a constant-coefficient differential operator and $p(D)x=q(t)$,
then $p(D)y=q(t-c)$, where $y(t)=x(t-c)$.\\
\vspace{1mm}\\
This is the `time invariance' of $p(D)$.\\
\vspace{1mm}\\
\textbf{Example}\\
Consider two systems:
\begin{align*}
\text{System A: }&\frac{1}{t}y(t)=x(t)\implies y(t)=tx(t)\\
\text{System B: }&\frac{1}{10}y(t)=x(t)\implies y(t)=10x(t)
\end{align*}
System A is not time-invariant, System B is. First consider system A with a delayed input $x_d(t)=x(t+\delta)$.
We have the corresponding response $y_1$ as
\begin{equation*}
y_1(t)=tx_d(t)=tx(t+\delta)
\end{equation*}
Now consider delaying the output by $\delta$
\begin{equation*}
y_2(t)=y(t+\delta)=(t+\delta)x(t+\delta)
\end{equation*}
Clearly $y_1(t)\neq y_2(t)$, therefore the system is not time-invariant. Now consider the same for system $B$;
for the delayed input:
\begin{equation*}
y_1(t)=10x(t+\delta)
\end{equation*}
Now with the delayed output
\begin{equation*}
y_2(t)=y(t+\delta)=10x(t+\delta)
\end{equation*}
$y_1(t)=y_2(t)$, therefore the system is time-invariant.\\
\vspace{1mm}\\
\textbf{Example 2}\\
Suppose that we know that $x_p(t)=\sqrt{2}\sin(t/2-\pi/4)$ is a solution to the DE
\begin{equation*}
2\ddot{x}+\dot{x}+x=\sin(t/2)
\end{equation*}
Should we want to find a solution $y_p$ to 
\begin{equation*}
2\ddot{x}+\dot{x}+x=\sin(t/2-\pi/3)
\end{equation*}
by translation-invariance we immediately have
\begin{equation*}
y_p=\sqrt{2}\sin(t/2-\pi/4-\pi/3)=\sqrt{2}\sin(t/2-7\pi/12)
\end{equation*}
\newpage
\noindent\textbf{Intuitively}:\\
A linear map is a function between two vector spaces where addition and scalar multiplication are preserved;
a function $T:V\to W$ is linear if 
\begin{align*}
T(u+v)&=T(u)+T(v)\\
T(av)&=aT(v)
\end{align*}
\textit{Linearity} in the context of a system means that the relation between the input $x(t)$ and the output $y(t)$, both being regarded as functions, is a linear mapping: if $a$ is a constant then the system output to $ax(t)$ is $ay(t)$ (so $T(ax)=aT(x)=ay$).\\
\vspace{1mm}\\
If $x'(t)$ is a further input with system output $y'(t)$ then the output
of the system to an input $x(t)+x'(t)$ is $y(t)+y'(t)$ (so $T(u+v)=T(u)+T(v)$). (this is also superposition)\\
\vspace{1mm}\\
\textit{Time invariance} means that whether we apply an input to the system now or $T$ seconds from now, the output would be identical except for a time delay of $T$ seconds. That is, if the output due to input $x(t)$ is $y(t)$, then the output due to $x(t-T)$ is $y(t-T)$. The
system is time invariant because the output does not depend on the particular time the input is applied.
\newpage

\subsection{Generalised ERF with proof}
We can solve a LTI DE $p(D)x=q(t)$ with exponential input 
$q(t)=Be^{at}$ even when $p(a)=0$ using the \textit{generalised exponential response} formula:\\
\vspace{1mm}\\
\textbf{Definition}\\
\textit{Generalised ERF:} Let $p(D)$ be a polynomial operator with constant coefficients, and $p^{(s)}$
its $s$-th derivative. Then
\begin{equation*}
p(D)x=Be^{at},\quad\text{where $a$ is real or complex}
\end{equation*}
has the particular solution 
\begin{equation*}
x_p=\begin{cases}
\frac{Be^{at}}{p(a)}&\text{if }p(a)\neq0\\
\frac{Bte^{at}}{p'(a)}&\text{if }p(a)=0\text{ and }p'(a)\neq0\\
\frac{Bt^2e^{at}}{p''(a)}&\text{if }p(a)=p'(a)=0\text{ and }p''(a)\neq0\\
&\vdots\\
\frac{Bt^se^{at}}{p^{(s)}(a)}&\text{if $a$ is an $s$-fold zero}
\end{cases}
\end{equation*}
\textbf{Proof}\\
The first case can be shown fairly easily:
\begin{equation*}
p(D)x_p=p(D)\frac{e^{at}}{p(a)}=\frac{1}{p(a)}p(D)e^{at}
=\frac{p(a)e^{at}}{p(a)}=e^{at}
\end{equation*}
Now we prove the last general case (since the other cases are just instances of it). First understand
that saying the polynomial $p(D)$ has the number $a$ as an $s$-fold zero is the same as saying $p(D)$ has 
a factorization
\begin{equation*}
p(D)=q(D)(D-a)^s
\end{equation*}
(both $p(D)$ and $q(D)$ are polynomials; this just says $(-a)$ can be factored out $s$ times). We will first prove
that the above implies
\begin{equation*}
p^{(s)}(a)=q(a)s!
\end{equation*}
(next page)
\newpage
\noindent\textbf{Proof cont.}\\
We show
\begin{equation*}
p(D)=q(D)(D-a)^s
\end{equation*}
implies
\begin{equation*}
p^{(s)}(a)=q(a)s!
\end{equation*}
Say that we factor out $(D-a)$ as many times as possible. We know that $q(D)$ is still a polynomial. 
Say of degree $k$:
\begin{equation*}
C_0+C_1D+\ldots+C_kD^k
\end{equation*}
we know that $(D-a)$ can no longer be factored out. Now see that we can write $q(D)$ \textit{in terms of} $(D-a)$:
\begin{equation*}
q(D)=c_0+c_1(D-a)+\ldots+c_k(D-a)^k
\end{equation*}
which is still a polynomial. See that $q(a)=c_0$ so
\begin{equation*}
q(D)=q(a)+c_1(D-a)+\ldots+c_k(D-a)^k
\end{equation*}
We can then write $p(D)$ as
\begin{equation*}
p(D)=q(a)(D-a)^s+c_1(D-a)^{s+1}+\ldots+c_k(D-a)^{s+k}
\end{equation*}
taking the derivative $s$ times, we obtain
\begin{equation*}
p^{(s)}(D)=q(a)s!+(\text{positive powers of $D-a$})
\end{equation*}
where substituting $a$ gives us
\begin{equation*}
p^{(s)}(D)=q(a)s!
\end{equation*}
(next page)
\newpage
\noindent\textbf{Proof cont.}\\
Using the result
\begin{equation*}
p^{(s)}(D)=q(a)s!
\end{equation*}
we can use the exponential-shift rule to prove the resonant response formulas:
\begin{align*}
p(D)\frac{t^se^{at}}{p^{(s)}(a)}&=\frac{e^{at}}{p^{(s)}(a)}p(D+a)t^s\\
&=\frac{e^{at}}{p^{(s)}(a)}q(D+a)D^st^s\\
&=\frac{e^{at}}{q(a)s!}q(D+a)s!\\
&=\frac{e^{at}}{q(a)s!}q(a)s!=e^{at}
\end{align*}
where the last line follows from 
\begin{equation*}
q(D+a)s!=(q(a)+c_1D+\ldots+c_kD^k)s!=q(a)s!
\end{equation*}
Note that by linearity we could have stated the formula with a factor of $B$ in the input and a 
corresponding factor of $B$ to the output. That is, the DE
\begin{equation*}
p(D)x=Be^{at}
\end{equation*}
has a particular solution
\begin{equation*}
x_p=\frac{Be^{at}}{p(a)},\quad\text{if }p(a)\neq0\text{ etc.}
\end{equation*}
\newpage

\subsection{Resonant response formula, example}
Recall the usual case of the ERF, where a solution to
\begin{equation*}
p(D)x=Be^{at}
\end{equation*}
is given by 
\begin{equation*}
x_p=\frac{Be^{at}}{p(a)}\quad\text{provided that }p(a)\neq0
\end{equation*}
We generalised this formula for the case when $p(a)=0$. Say $p'(a)\neq0$, a particular solution is given by
\begin{equation*}
x_p=\frac{Bte^{at}}{p'(a)}\quad\text{if }p(a)=0\text{ and }p'(a)\neq0
\end{equation*}
we call this the \textit{Resonant Response Formula}.\\
\vspace{1mm}\\
\textbf{Example}\\
Consider $x''+4x=2\cos 2t$. We use complex replacement and the ERF to get a complex particular solution, then
take the real part. We have
\begin{equation*}
z''+4z=2e^{2it},\quad x_p=\text{Re}(z_p)
\end{equation*}
and characteristic polynomial $p(s)=s^2+4$. See that $p(2i)=0$ (resonant case). So we use
the resonant response formula; we have $p'(2i)=4i\neq0$:
\begin{equation*}
z_p=\frac{2te^{2it}}{4i}
\end{equation*}
Taking the real part gives us the desired particular solution
\begin{equation*}
x_p=\frac{1}{2}t\sin2t
\end{equation*}
\newpage

\subsection{Pure resonance}
Here we look at the pure resonant case for a second-order LTI DE. We use the spring-mass model for a physical
interpretation of the system, but obviously the same behaviour applies to other systems with the same DE:
\begin{equation*}
x''+w_0^2x=F_0\cos\omega t
\end{equation*}
We solve a system like this with complex replacement:
\begin{equation*}
z''+w_0^2z=F_0e^{i\omega t},\quad x=\text{Re}(z)
\end{equation*}
Characteristic polynomial:
\begin{equation*}
p(r)=r^2+\omega_0^2\implies p(i\omega)=\omega_0^2-\omega^2
\end{equation*}
ERF:
\begin{equation*}
z_p=\begin{cases}
\frac{F_0e^{i\omega t}}{p(i\omega)}=\frac{F_0e^{i\omega t}}{\omega_0^2-\omega^2}&\text{if $\omega\neq\omega_0$}
\\
\frac{F_0te^{i\omega t}}{p'(i\omega)}=\frac{F_0te^{i\omega t}}{2i\omega}&\text{if $\omega=\omega_0$}
\end{cases}
\end{equation*}
Taking real part
\begin{equation*}
x_p=\begin{cases}
\frac{F_0\cos\omega t}{\omega_0^2-\omega^2}&\text{if $\omega\neq\omega_0$}\\
\frac{F_0t\sin\omega_0t}{2\omega_0}&\text{if $\omega=\omega_0$}
\end{cases}
\end{equation*}
See that the amplitude $A(\omega)$ is given by
\begin{equation*}
A(\omega)=\left|\frac{F_0}{\omega_0^2-\omega}\right|
\end{equation*}
See that as $\omega$ gets closer to $\omega_0$ the amplitude increases (it is similar to the damped amplitude
response except that the peak is infinitely high). 
\begin{center}
\includegraphics[width=10cm]{33}\\
\end{center}
When $\omega=\omega_0$ we have $x_p=\frac{F_0t\sin\omega_0t}{2\omega_0}$. This is called \textit{pure resonance}.
The frequency $\omega_0$ is called the \textit{resonant} or \textit{natural} frequency of the system.
Notice that the response is \textit{oscillatory but not periodic}---the amplitude keeps growing in time.
\newpage

\subsection{Recap and Practical resonance}
\textbf{Recap: Gain, Phase lag, Complex gain}\\
Given a second order linear CC DE with a \textit{sinusoidal} driving force $B\cos\omega t$:
\begin{equation*}
mx''+bx'+kx=B\cos\omega t
\end{equation*}
An equation like this is solved using complex replacement:
\begin{equation*}
mz''+bz'+kz=Be^{i\omega t},\quad x=\text{Re}(z)
\end{equation*}
characteristic equation:
\begin{equation*}
p(s)=ms^2+bs+k
\end{equation*}
ERF:
\begin{equation*}
z_p=\frac{Be^{i\omega t}}{p(i\omega)}=\frac{Be^{i\omega t}}{k-m\omega^2+ib\omega}
\end{equation*}
So
\begin{equation*}
x_p=\text{Re}(z_p)=\frac{B}{\sqrt{(k-m\omega^2)^2+b^2\omega^2}}\cos(\omega t-\phi)
\end{equation*}
where $\phi=\text{Arg}(p(i\omega))=\tan^{-1}\left(\frac{b\omega}{k-m\omega^2}\right)$. See that the numerator of
the inverse tangent is positive---$\phi$ must be between 0 and $\pi$ (in first or second quadrants).\\
\vspace{1mm}\\
Recall that the \textit{complex gain} is defined as the ratio between the output and input amplitudes in the 
\textit{complexified} equation:
\begin{equation*}
\tilde{g}(\omega)=\frac{1}{p(i\omega)}=\frac{1}{k-m\omega^2+ib\omega}
\end{equation*}
The \textit{gain} is defined as the ratio between the output and input amplitudes in the \textit{real} equation:
\begin{equation*}
g=g(\omega)=\frac{1}{|p(i\omega)|}=\frac{1}{\sqrt{(k-m\omega^2)^2+b^2\omega^2}}
\end{equation*}
The \textit{phase lag} is
\begin{equation*}
\phi=\phi(\omega)=\text{Arg}(p(i\omega))=\tan^{-1}\left(\frac{b\omega}{k-m\omega^2}\right)
\end{equation*}
from this we also have the \textit{time lag} as $\phi/\omega$.\\
\vspace{1mm}\\
We call the gain $g(\omega)$ the \textit{amplitude response} of the system, the phase lag $\phi(\omega)$
the \textit{phase response} of the system, and 
both of them collectively as the \textit{frequency response} of the system.\\
(next page)
\newpage
\noindent\textbf{Frequency response and Practical Resonance}\\
We have a periodic solution to the equation
\begin{equation*}
mx''+bx'+kx=B\cos(\omega t)
\end{equation*}
given by $x_p=gB\cos(\omega t-\phi)$, where $g$ is the gain
\begin{equation*}
g=g(\omega)=\frac{1}{\sqrt{(k-m\omega^2)^2+b^2\omega^2}}
\end{equation*}
and $\phi$ is the phase lag
\begin{equation*}
\phi=\phi(\omega)=\text{Arg}(p(i\omega))=\tan^{-1}\left(\frac{b\omega}{k-m\omega^2}\right)
\end{equation*}
The gain/amplitude response is a function of $\omega$. It tells us the size of the system's response to the
given input frequency. If the amplitude has a peak at $\omega_r$ we call this the \textit{practical
resonance frequency}:
\begin{center}
\includegraphics[width=10cm]{34}\\
\end{center}
If the damping $b$ gets too large, there will be no peak and therefore no practical resonance. 
(note this is different from overdamping and critical damping, where the response is not sinusoidal and therefore
does not have a measureable gain)\\
\vspace{1mm}\\
\textbf{Finding the practical resonant frequency}\\
Practical resonance occurs at the frequency $\omega_r$ where $g(\omega)$ has a maximum. Accordingly we look for the
\textit{minimum} of 
\begin{equation*}
f(\omega)=(k-m\omega^2)^2+b^2\omega^2
\end{equation*}
Setting $f'(\omega)=0$ and solving gives
\begin{align*}
f'(\omega)&=-4m\omega(k-m\omega^2)+2b^2\omega\\
&=2\omega(b^2-2mk-2m^2\omega^2)=0
\end{align*}
so we have $\omega=0$ or $(b^2-2mk-2m^2\omega^2)=0$:
\begin{align*}
m^2\omega^2&=mk-b^2/2\\
\omega_r&=\sqrt{\frac{k}{m}-\frac{b^2}{2m^2}}
\end{align*}
See that there is a practical resonant frequency when $mk-b^2/2>0$.\\
(next page)
\newpage
\noindent\textbf{Phase lag visualised}\\
The dotted line is the input and the solid line is the response:
\begin{center}
\includegraphics[width=10cm]{35}\\
\end{center}
The damping causes a lag between the input and output. In radians, the angle $\phi$ is called the
\textit{phase lag} and the time $\phi/\omega$ the \textit{time lag}.
\newpage

\section{Fourier series and Laplace Transform}
\subsection{Periodic functions---Definitions}
\textbf{Definition}\\
A function $f(t)$ is \textit{periodic} with \textit{period} $P>0$ if
\begin{equation*}
f(t+P)=f(t)\quad\text{for all }t
\end{equation*}
\textbf{Example}\\
$f(t)=\sin(2t)$ is periodic with period $P=\pi$. This can be seen because for all $t$,
\begin{equation*}
f(t+\pi)=\sin(2(t+\pi))=\sin(2t+2\pi)=\sin(2t)=f(t)
\end{equation*}
Notice however that in the this example $f(t)=\sin(2t)$ also has period $P=n\pi$ for any integer $n=1,2,3\ldots$.\\
\vspace{1mm}\\
\textbf{Base period}\\
Most periodic functions have a \textit{minimal period}---which the smallest value that satisfies the definition
of the period. This is also called the \textit{base period}. An exception to this is the constant function,
where every value of $P>0$ is a period.\\
\vspace{1mm}\\
\textbf{Windows}\\
To fully describe a periodic function we only need to specify the period and the value of the function over
one full period. We call an interval containing one full
period a \textit{window}. Typical choices are $[-P/2,P/2)$ and $[0,P)$, but any interval of length $P$ would work.
\newpage

\subsection{Fourier series---Definition and Coefficients}
If the $f(t)$ is periodic (for now considering period $2\pi$)
,we can express the function (where it is continuous) as an
infinite sum of sines and cosines which all have period $2\pi$.\\
\vspace{1mm}\\
\textbf{Theorem} (Fourier):\\
Suppose $f(t)$ has period $2\pi$ then we have
\begin{align*}
f(t)\approx&\frac{a_0}{2}+a_1\cos(t)+a_2\cos(2t)+a_3\cos(3t)+\ldots\\
&+b_1\sin(t)+b_2\sin(2t)+b_3\sin(3t)+\ldots\\
=&\frac{a_0}{2}+\sum^{\infty}_{n=1}[a_n\cos(nt)+b_n\sin(nt)]
\end{align*}
where the coefficients $a_0,a_1,\ldots$ and $b_1,b_2,\ldots$ are computed by
\begin{align*}
a_0&=\frac{1}{\pi}\int^\pi_{-\pi}f(t)\,dt\\
a_n&=\frac{1}{\pi}\int^\pi_{-\pi}f(t)\cos(nt)\,dt\\
b_n&=\frac{1}{\pi}\int^\pi_{-\pi}f(t)\sin(nt)\,dt
\end{align*}
See that all the sinusoids have $2\pi$ as a period. The series is called a \textit{Fourier series}; and the
coefficients $a_0,a_1,\ldots,b_1,b_2\ldots$ are called the \textit{Fourier coefficients} of $f(t)$.\\
\vspace{1mm}\\
We used an approximation instead of an equality in the first statement because the two sides may differ 
for discontinuities in $f(t)$. We use an equal sign from now on.
\newpage

\subsection{Example: Square wave}
\textbf{Definition}\\
Say we have $f(t)$ as the \textit{square wave} with period $2\pi$, defined as
\begin{equation*}
f(t)=\begin{cases}
-1&\text{for }-\pi\leq t<0\\
1&\text{for }0\leq t<\pi
\end{cases}
\end{equation*}
Plotted:
\begin{center}
\includegraphics[width=10cm]{36}\\
\end{center}
\textbf{Fourier series}\\
Consider computing its Fourier series. Recall
\begin{equation*}
f(t)=\frac{a_0}{2}+\sum^{\infty}_{n=1}[a_n\cos(nt)+b_n\sin(nt)]
\end{equation*}
where
\begin{equation*}
a_0=\frac{1}{\pi}\int^\pi_{-\pi}f(t)\,dt,\quad
a_n=\frac{1}{\pi}\int^\pi_{-\pi}f(t)\cos(nt)\,dt,\quad
b_n=\frac{1}{\pi}\int^\pi_{-\pi}f(t)\sin(nt)\,dt
\end{equation*}
First see that
\begin{equation*}
a_0=\frac{1}{\pi}\int^\pi_{-\pi}f(t)\,dt=0
\end{equation*}
now for the other coefficients, see that we can split the
integral to get around the discontinuity in the 
square wave function
\begin{equation*}
a_n=\frac{1}{\pi}\int^\pi_{-\pi}f(t)\cos(nt)\,dt
=\frac{1}{\pi}\int^0_{-\pi}(-1)\cos(nt)\,dt+\frac{1}{\pi}\int^\pi_{0}(1)\cos(nt)\,dt
\end{equation*}
and so
\begin{equation*}
a_n=\left.-\frac{\sin(nt)}{n\pi}\right|_{-\pi}^0+
\left.\frac{\sin(nt)}{n\pi}\right|^{\pi}_0=0\quad\text{for $n\neq0$}
\end{equation*}
(or see visually that both integrals are 0)\\
(next page)
\newpage
\noindent\textbf{Fourier series cont.}\\
Likewise
\begin{align*}
b_n&=\frac{1}{\pi}\int^\pi_{-\pi}f(t)\sin(nt)\,dt=
\frac{1}{\pi}\int^0_{-\pi}(-1)\sin(nt)\,dt+\frac{1}{\pi}\int^\pi_{0}(1)\sin(nt)\,dt\\
&=\left.\frac{\cos(nt)}{n\pi}\right|_{-\pi}^0-\left.\frac{\cos(nt)}{n\pi}\right|^{\pi}_0
=\frac{1-\cos(-n\pi)}{n\pi}-\frac{\cos(n\pi)-1}{n\pi}\\
&=\frac{2}{n\pi}(1-\cos(-n\pi))=\frac{2}{n\pi}(1-(-1)^n)
=\begin{cases}
\frac{4}{n\pi}&\text{for $n$ odd}\\
0&\text{for $n$ even}
\end{cases}
\end{align*}
The last step comes from the the simplification $\cos(n\pi)=(-1)^n$.
With that we have the fourier series for $f(t)$ as
\begin{equation*}
f(t)=\sum^\infty_{n=1}b_n\sin(nt)=\frac{4}{\pi}\left(
\sin t+\frac{1}{3}\sin(3t)+\frac{1}{5}\sin(5t)+\cdots\right)
\end{equation*}
\textbf{Illustrated convergence of Fourier series}\\
Here we plot the sums of the first $N$ terms of the series for $N=1,3,9,33$:
\begin{center}
\includegraphics[width=10cm]{37}\\
\end{center}
See that the more terms considered, the better the series approximates the candidate function.\\
\vspace{1mm}\\
\textbf{Intuition for odd and even functions}\\
It turns out that for \textit{odd} $f(t)$, meaning $f(-t)=-f(t)$, such functions only have sines (which
are also odd functions) in their Fourier series, it resembles
\begin{equation*}
f(t)=\sum^\infty_{n=1}b_n\sin(nt)
\end{equation*}
Similarly for even functions, where $f(-t)=f(t)$, their Fourier series consist of cosines:
\begin{equation*}
f(t)=\frac{a_0}{2}+\sum^\infty_{n=1}a_n\cos(nt)
\end{equation*}
For instance see that in the case of the square wave (an odd function), the Fourier series consists of just sines.
\newpage

\subsection{Fourier series for functions with period 2L}
\textbf{Generalising}\\
Suppose that we have a periodic function $f(t)$ with arbitrary period $P=2L$; generalising the special case
$P=2\pi$ by a re-scaling of the interval $(-\pi,\pi)$ to
$(-L,L)$ allows us to write down the general Fourier series and Fourier coefficient formulas:
\begin{equation*}
f(t)=\frac{a_0}{2}+\sum^\infty_{n=1}\left[a_n\cos\left(n\frac{\pi}{L}t\right)
+b_n\sin\left(n\frac{\pi}{L}t\right)\right]
\end{equation*}
with Fourier coefficients given by
\begin{align*}
a_0&=\frac{1}{L}\int^L_{-L}f(t)\,dt\\
a_n&=\frac{1}{L}\int^L_{-L}f(t)\cos\left(n\frac{\pi}{L}t\right)dt\\
b_n&=\frac{1}{L}\int^L_{-L}f(t)\sin\left(n\frac{\pi}{L}t\right)dt
\end{align*}
The number $L=P/2$ is called the \textit{half-period}.\\
\vspace{1mm}\\
\textbf{Example: Triangle wave}\\
Let $f(t)$ be a period 2 function, defined on the window $[-1,1)$ by $f(t)=|t|$; this function is called a 
\textit{triangle wave} or \textit{continuous sawtooth function}:
\begin{center}
\includegraphics[width=10cm]{38}\\
\end{center}
Consider computing its Fourier series. In this case the period is $P=2$, and therefore the half-period is $L=1$. 
Computing $a_0$:
\begin{equation*}
a_0=\frac{1}{L}\int^L_{-L}f(t)\,dt=\int^1_{-1}|t|\,dt
=2\int^1_{0}t\,dt=1
\end{equation*}
Now for $a_n$ for $n\neq0$ (and integrating by parts):
\begin{align*}
a_n&=\frac{1}{L}\int^L_{-L}f(t)\cos\left(n\frac{\pi}{L}t\right)dt\\
&=\frac{1}{1}\int^1_{-1}|t|\cos(n\pi t)\,dt=2\int^1_0t\cos(n\pi t)\,dt\\
&=2\left(\frac{t\sin(n\pi t)}{n\pi}+\left.\frac{\cos(n\pi t)}{n^2\pi^2}\right)\right|^1_0
=\frac{2}{n^2\pi^2}((-1)^n-1)=\begin{cases}
-\frac{4}{n^2\pi^2}&\text{for $n$ odd}\\
0&\text{for $n$ even}\end{cases}
\end{align*}
(next page)
\newpage
\noindent\textbf{Triangle wave (cont.)}\\
Now for the sine coefficients.
\begin{align*}
b_n&=\frac{1}{L}\int^L_{-L}f(t)\sin\left(n\frac{\pi}{L}t\right)dt=\frac{1}{1}\int^1_{-1}|t|\sin(n\pi t)\,dt\\
&=\int^0_{-1}-t\sin(n\pi t)\,dt+\int^1_{0}t\sin(n\pi t)\,dt\\
&=-\left(\left.-\frac{t\cos(n\pi t)}{n\pi}+\frac{\sin(n\pi t)}{n^2\pi^2}\right)\right|^0_{-1}+
\left(\left.-\frac{t\cos(n\pi t)}{n\pi}+\frac{\sin(n\pi t)}{n^2\pi^2}\right)\right|^1_{0}\\
&=\frac{\cos(n\pi)}{n\pi}-\frac{\cos(n\pi)}{n\pi}=0
\end{align*}
Thus the Fourier series for $f(t)$ is 
\begin{align*}
f(t)&=\frac{1}{2}-\frac{4}{\pi^2}\left(\cos(\pi t)+\frac{\cos(3\pi t)}{3^2}+\frac{\cos(5\pi t)}{5^2}+\cdots\right)\\
&=\frac{1}{2}-\frac{4}{\pi^2}\sum_{n\text{ odd}}\frac{\cos(n\pi t)}{n^2}
\end{align*}
\newpage

\subsection{Orthogonality Rules Justification}
Here we justify a few integral formulas, called the orthogonality relations (note the integral can be viewed
as a continuous dot product between two sinusoids, thus the link to the name). $n,m$ are integers:
\begin{align*}
\frac{1}{L}\int^L_{-L}\cos\left(n\frac{\pi}{L}t\right)\cos\left(m\frac{\pi}{L}t\right)\,dt&=\begin{cases}
1&n=m\neq0\\
0&n\neq m\\
2&n=m=0
\end{cases}\\
\frac{1}{L}\int^L_{-L}\cos\left(n\frac{\pi}{L}t\right)\sin\left(m\frac{\pi}{L}t\right)\,dt&=0\\
\frac{1}{L}\int^L_{-L}\sin\left(n\frac{\pi}{L}t\right)\sin\left(m\frac{\pi}{L}t\right)\,dt&=\begin{cases}
1&n=m\neq0\\
0&n\neq m\\
0&n=m=0
\end{cases}
\end{align*}
\textbf{First relation}\\
First see that, denoting $a=n\pi/L$, $b=m\pi/L$, $n,m$ integers:
\begin{align*}
\int^L_{-L}e^{i(a+b)t}+e^{-i(a+b)t}\,dt&=\left[\frac{e^{i(a+b)t}}{i(a+b)}-\frac{e^{-i(a+b)t}}{i(a+b)}\right]^L_{-L}\\
&=\left.\frac{2\sin((a+b)t)}{i(a+b)}\right|^L_{-L}\\
&=\left.\frac{2L\sin((n\pi+m\pi)t/L)}{i(n\pi+m\pi)}\right|^L_{-L}\\
&=\frac{4L\sin(n\pi+m\pi)}{i(n\pi+m\pi)}=\begin{cases}
0&n=m\neq0\\
0&n\neq m
\end{cases}
\end{align*}
And for $n=m=0$:
\begin{equation*}
\int^L_{-L}e^{i(a+b)t}+e^{-i(a+b)t}\,dt=\int^L_{-L}e^{0}+e^{0}\,dt=\int^L_{-L}2\,dt=4L
\end{equation*}
so
\begin{equation*}
\int^L_{-L}e^{i(a+b)t}+e^{-i(a+b)t}\,dt=\begin{cases}
0&n=m\neq0\\
0&n\neq m\\
4L&n=m=0
\end{cases}
\end{equation*}
(next page)
\newpage
\noindent\textbf{First relation (cont.)}\\
Then see that 
\begin{align*}
\int^L_{-L}e^{i(a-b)t}+e^{-i(a-b)t}\,dt&=\left[\frac{e^{i(a-b)t}}{i(a-b)}-\frac{e^{-i(a-b)t}}{i(a-b)}\right]^L_{-L}\\
&=\left.\frac{2\sin((a-b)t)}{i(a-b)}\right|^L_{-L}\\
&=\left.\frac{2L\sin((n\pi-m\pi)t/L)}{i(n\pi-m\pi)}\right|^L_{-L}\\
&=\frac{4L\sin(n\pi-m\pi)}{i(n\pi-m\pi)}=0\quad\text{for $n\neq m$}
\end{align*}
and for $a=b\neq0$ or $a=b=0$
\begin{equation*}
\int^L_{-L}e^{i(a-b)t}+e^{-i(a-b)t}\,dt=\int^L_{-L}e^{0}+e^{0}\,dt=\int^L_{-L}2\,dt=4L
\end{equation*}
so
\begin{equation*}
\int^L_{-L}e^{i(a-b)t}+e^{-i(a-b)t}\,dt=\begin{cases}
4L&n=m\neq0\\
0&n\neq m\\
4L&n=m=0
\end{cases}
\end{equation*}
Finally we prove the first relation:
\begin{align*}
\frac{1}{L}\int^L_{-L}\cos\left(n\frac{\pi}{L}t\right)\cos\left(m\frac{\pi}{L}t\right)\,dt=
\frac{1}{L}\int^L_{-L}\cos(at)\cos(bt)\,dt
\end{align*}
using the fact that $\cos(x)=(e^{ix}+e^{-ix})/2$
\begin{align*}
\frac{1}{L}\int^L_{-L}\cos(at)\cos(bt)\,dt&=
\frac{1}{L}\int^L_{-L}\frac{e^{iat}+e^{-iat}}{2}\cdot
\frac{e^{ibt}+e^{-ibt}}{2}\,dt\\
&=\frac{1}{4L}\int^L_{-L}e^{i(a+b)t}+e^{-i(a+b)t}
+e^{i(a-b)t}+e^{-i(a-b)t}\,dt
\end{align*}
\begin{equation*}
=\underbrace{\frac{1}{4L}\int^L_{-L}e^{i(a+b)t}+e^{-i(a+b)t}\,dt}_{=\begin{cases}0&n=m\neq0\\
0&n\neq m\\
1&n=m=0
\end{cases}}
+\underbrace{\frac{1}{4L}\int^L_{-L}e^{i(a-b)t}+e^{-i(a-b)t}\,dt}_{=\begin{cases}
1&n=m\neq0\\
0&n\neq m\\
1&n=m=0
\end{cases}}
\end{equation*}
so
\begin{equation*}
\frac{1}{L}\int^L_{-L}\cos\left(n\frac{\pi}{L}t\right)\cos\left(m\frac{\pi}{L}t\right)\,dt=\begin{cases}
1&n=m\neq0\\
0&n\neq m\\
2&n=m=0
\end{cases}
\end{equation*}
(next page)
\newpage
\noindent\textbf{Second relation}\\
See that for $a=n\pi/L$, $b=m\pi/L$, $n,m$ integers:
\begin{align*}
\int^L_{-L}e^{i(a+b)t}-e^{-i(a+b)t}\,dt&=
\left[\frac{e^{i(a+b)t}}{i(a+b)}+\frac{e^{-i(a+b)t}}{i(a+b)}\right]^L_{-L}\\
&=\left.\frac{2\cos((a+b)t)}{i(a+b)}\right|^L_{-L}\\
&=\frac{2\cos((a+b)L)}{i(a+b)}-\frac{2\cos(-(a+b)L)}{i(a+b)}\\
&=\begin{cases}
0&n=m\neq0\\
0&n\neq m
\end{cases}
\end{align*}
(since $\cos(-x)=\cos(x)$) and for $n=m=0$
\begin{equation*}
\int^L_{-L}e^{i(a+b)t}-e^{-i(a+b)t}\,dt=\int^L_{-L}e^{0}-e^{0}\,dt=\int^L_{-L}0\,dt=0
\end{equation*}
so
\begin{equation*}
\int^L_{-L}e^{i(a+b)t}-e^{-i(a+b)t}\,dt=\begin{cases}
0&n=m\neq0\\
0&n\neq m\\
0&n=m=0
\end{cases}
\end{equation*}
see that by exactly the same line of reasoning:
\begin{align*}
\int^L_{-L}e^{i(a-b)t}-e^{-i(a-b)t}\,dt&=
\left[\frac{e^{i(a-b)t}}{i(a-b)}+\frac{e^{-i(a-b)t}}{i(a-b)}\right]^L_{-L}\\
&=\left.\frac{2\cos((a-b)t)}{i(a-b)}\right|^L_{-L}\\
&=\frac{2\cos((a-b)L)}{i(a-b)}-\frac{2\cos(-(a-b)L)}{i(a-b)}\\
&=0\quad\text{for }n\neq m
\end{align*}
for $n=m\neq0$ and $n=m=0$
\begin{equation*}
\int^L_{-L}e^{i(a-b)t}-e^{-i(a-b)t}\,dt=\int^L_{-L}e^{0}-e^{0}\,dt=\int^L_{-L}0\,dt=0
\end{equation*}
so
\begin{equation*}
\int^L_{-L}e^{i(a-b)t}-e^{-i(a-b)t}\,dt=\begin{cases}
0&n=m\neq0\\
0&n\neq m\\
0&n=m=0
\end{cases}
\end{equation*}
(next page)
\newpage
\noindent\textbf{Second relation (cont.)}\\
Finally see that
\begin{equation*}
\frac{1}{L}\int^L_{-L}\cos\left(n\frac{\pi}{L}t\right)\sin\left(m\frac{\pi}{L}t\right)\,dt=
\frac{1}{L}\int^L_{-L}\cos(at)\sin(bt)\,dt
\end{equation*}
using $\cos(x)=(e^{ix}+e^{-ix})/2$ and $\sin(x)=(e^{ix}-e^{-ix})/(2i)$:
\begin{align*}
\frac{1}{L}\int^L_{-L}\cos(at)\sin(bt)\,dt&=
\frac{1}{L}\int^L_{-L}\frac{e^{iat}+e^{-iat}}{2}\cdot
\frac{e^{ibt}-e^{-ibt}}{2i}\,dt\\
&=\frac{1}{4Li}\int^L_{-L}e^{i(a+b)t}-e^{-i(a+b)t}
-e^{i(a-b)t}+e^{-i(a-b)t}\,dt
\end{align*}
as derived above,
\begin{equation*}
=\underbrace{\frac{1}{4Li}\int^L_{-L}e^{i(a+b)t}-e^{-i(a+b)t}\,dt}_{\begin{cases}
0&n=m\neq0\\
0&n\neq m\\
0&n=m=0
\end{cases}}-
\underbrace{\frac{1}{4Li}\int^L_{-L}e^{i(a-b)t}-e^{-i(a-b)t}\,dt}_{\begin{cases}
0&n=m\neq0\\
0&n\neq m\\
0&n=m=0
\end{cases}}
\end{equation*}
so
\begin{equation*}
\frac{1}{L}\int^L_{-L}\cos\left(n\frac{\pi}{L}t\right)\sin\left(m\frac{\pi}{L}t\right)\,dt=0
\end{equation*}
(next page)
\newpage
\noindent\textbf{Third relation}\\
Lastly, see that
\begin{equation*}
\frac{1}{L}\int^L_{-L}\sin\left(n\frac{\pi}{L}t\right)\sin\left(m\frac{\pi}{L}t\right)\,dt=
\frac{1}{L}\int^L_{-L}\sin(at)\sin(bt)\,dt
\end{equation*}
using $\sin(x)=(e^{ix}-e^{-ix})/(2i)$:
\begin{align*}
\frac{1}{L}&\int^L_{-L}\sin(at)\sin(bt)\,dt=
\frac{1}{L}\int^L_{-L}\frac{e^{iat}-e^{-iat}}{2i}\cdot
\frac{e^{ibt}-e^{-ibt}}{2i}\,dt\\
&=\frac{1}{4Li^2}\int^L_{-L}e^{i(a+b)t}+e^{-i(a+b)t}\,dt
-\frac{1}{4Li^2}\int^L_{-L}e^{i(a-b)t}+e^{-i(a-b)t}\,dt\\
&=\underbrace{\frac{1}{4L}\int^L_{-L}e^{i(a-b)t}+e^{-i(a-b)t}\,dt}_{=\begin{cases}
1&n=m\neq0\\
0&n\neq m\\
1&n=m=0
\end{cases}}-
\underbrace{\frac{1}{4L}\int^L_{-L}e^{i(a+b)t}+e^{-i(a+b)t}\,dt}_{=\begin{cases}0&n=m\neq0\\
0&n\neq m\\
1&n=m=0
\end{cases}}
\end{align*}
so
\begin{equation*}
\frac{1}{L}\int^L_{-L}\sin\left(n\frac{\pi}{L}t\right)\sin\left(m\frac{\pi}{L}t\right)\,dt=\begin{cases}
1&n=m\neq0\\
0&n\neq m\\
0&n=m=0
\end{cases}
\end{equation*}
\newpage

\subsection{Orthogonality rules to justify coefficient formulas}
We had the orthogonality rules as: for
$n,m$ integers:
\begin{align*}
\frac{1}{L}\int^L_{-L}\cos\left(n\frac{\pi}{L}t\right)\cos\left(m\frac{\pi}{L}t\right)\,dt&=\begin{cases}
1&n=m\neq0\\
0&n\neq m\\
2&n=m=0
\end{cases}\\
\frac{1}{L}\int^L_{-L}\cos\left(n\frac{\pi}{L}t\right)\sin\left(m\frac{\pi}{L}t\right)\,dt&=0\\
\frac{1}{L}\int^L_{-L}\sin\left(n\frac{\pi}{L}t\right)\sin\left(m\frac{\pi}{L}t\right)\,dt&=\begin{cases}
1&n=m\neq0\\
0&n\neq m\\
0&n=m=0
\end{cases}
\end{align*}
\textbf{Cosine coefficients}\\
Given a function (with arbritrary period $2L$)
\begin{equation*}
f(t)=\frac{a_0}{2}+\sum^{\infty}_{n=1}\left[a_n\cos\left(n\frac{\pi}{L}t\right)+
b_n\sin\left(n\frac{\pi}{L}t\right)\right]
\end{equation*}
First we consider $a_n$ coefficients; say that we want to find $a_k$, where $k$ is an integer and $k>0$:
\begin{align*}
&\frac{1}{L}\int^L_{-L}f(t)\cos\left(k\frac{\pi}{L}t\right)
dt\\
=&\frac{1}{L}\int^L_{-L}\frac{a_0}{2}\cos\left(k\frac{\pi}{L}t\right)+
\sum^{\infty}_{n=1}\left[a_n\cos\left(n\frac{\pi}{L}t\right)\cos\left(k\frac{\pi}{L}t\right)\right]\\&+
\sum^{\infty}_{n=1}\left[b_n\sin\left(n\frac{\pi}{L}t\right)\cos\left(k\frac{\pi}{L}t\right)\right]dt
\end{align*}
Using the orthogonality relations see that all except one  term in this expression are equal to 0 
\begin{equation*}
=0+\ldots+0+\frac{1}{L}\int^L_{-L}a_k\cos\left(k\frac{\pi}{L}t\right)\cos\left(k\frac{\pi}{L}t\right)+0+\ldots+0
\end{equation*}
so
\begin{equation*}
\frac{1}{L}\int^L_{-L}f(t)\cos\left(k\frac{\pi}{L}t\right)
=a_k\underbrace{\frac{1}{L}\int^L_{-L}\cos\left(k\frac{\pi}{L}t\right)\cos\left(k\frac{\pi}{L}t\right)}_{=1}=a_k
\end{equation*}
which gives us a formula for the cosine coefficients.\\
(next page)\newpage
\noindent\textbf{Sine coefficients}\\
The sine coefficients can be proven the same way; say we want to find $b_k$ where again $k$ is an integer and $k>0$:
\begin{align*}
&\frac{1}{L}\int^L_{-L}f(t)\sin\left(k\frac{\pi}{L}t\right)dt\\
=&\frac{1}{L}\int^L_{-L}\frac{a_0}{2}\sin\left(k\frac{\pi}{L}t\right)+\sum^{\infty}_{n=1}\left[a_n\cos\left(n\frac{\pi}{L}t\right)\sin\left(k\frac{\pi}{L}t\right)\right]\\&+
\sum^{\infty}_{n=1}\left[b_n\sin\left(n\frac{\pi}{L}t\right)\sin\left(k\frac{\pi}{L}t\right)\right]dt
\end{align*}
Again see that by the orthogonality rules
\begin{equation*}
=0+\ldots+0+\frac{1}{L}\int^L_{-L}b_k\sin\left(k\frac{\pi}{L}t\right)\sin\left(k\frac{\pi}{L}t\right)+0+\ldots+0
\end{equation*}
so
\begin{equation*}
\frac{1}{L}\int^L_{-L}f(t)\sin\left(k\frac{\pi}{L}t\right)dt=b_k\underbrace{\frac{1}{L}\int^L_{-L}\sin\left(k\frac{\pi}{L}t\right)\sin\left(k\frac{\pi}{L}t\right)}_{=1}=b_k
\end{equation*}
which gives us a formula for the sine coefficients.\\
\vspace{1mm}\\
\textbf{Justifying $a_0/2$}\\
See that should we consider $k=0$, attempting to compute the cosine coefficients gives
\begin{align*}
&\frac{1}{L}\int^L_{-L}f(t)\cos\left(0\cdot\frac{\pi}{L}t\right)dt\\
=&\frac{1}{L}\int^L_{-L}\frac{a_0}{2}\cos\left(0\cdot\frac{\pi}{L}t\right)+
\underbrace{\sum^{\infty}_{n=1}\left[a_n\cos\left(n\frac{\pi}{L}t\right)\cos\left(0\cdot\frac{\pi}{L}t\right)\right]}_{=0}\\&+
\underbrace{\sum^{\infty}_{n=1}\left[b_n\sin\left(n\frac{\pi}{L}t\right)\cos\left(0\cdot\frac{\pi}{L}t\right)\right]}_{=0}dt\\
=&\frac{1}{L}\int^L_{-L}\frac{a_0}{2}\,dt=a_0
\end{align*}
See that
\begin{equation*}
\frac{1}{L}\int^L_{-L}f(t)\cos\left(0\cdot\frac{\pi}{L}t\right)dt=\frac{1}{L}\int^L_{-L}f(t)\,dt=a_0
\end{equation*}
which gives us the formula for $a_0$. Also see that this justifies the convention of scaling $a_0$ by a 
factor of 1/2 in the fourier series formula. (Computing the sine coefficients for $k=0$ amounts to 0 since $\sin(0)=0$)
\newpage

\subsection{Even and Odd Functions}
\textbf{Definition}\\
A function $f(t)$ is called \textit{even} if $f(-t)=f(t)$ for all $t$. Examples include
$t^2,t^4,t^6$ (even powers), $\cos(at)$ or a constant.\\
\vspace{1mm}\\
See that
\begin{equation*}
\text{If }f(t)\text{ is even then}\quad\int^L_{-L}f(t)\,dt=2\int^L_{0}f(t)\,dt
\end{equation*}
A function is $f(t)$ is called \textit{odd} if $f(-t)=-f(t)$ for all $t$. Examples include
$t,t^3,t^5$ (odd powers) and $\sin(at)$.\\
\vspace{1mm}\\
Now see that
\begin{equation*}
\text{If }f(t)\text{ is odd then}\quad\int^L_{-L}f(t)\,dt=0
\end{equation*}
Illustrated:
\begin{center}
\includegraphics[width=10cm]{39}\\
\end{center}
\textbf{Even and odd Fourier coefficient rules}\\
Consider multiplying even and odd functions; intuitively (consider for instance multiplying 
odd and even powers of $t$):
\begin{enumerate}
\item even$\times$even = even
\item odd$\times$odd = even
\item odd$\times$even = odd
\end{enumerate}
We can use this to show that, assuming periodic $f(t)$:
\begin{equation*}
\text{If }f(t)\text{ is \textit{even} then we have }b_n=0,\text{ and }a_n=\frac{2}{L}\int^L_0f(t)\cos\left(n\frac{\pi}{L}\right)\,dt
\end{equation*}
and
\begin{equation*}
\text{If }f(t)\text{ is \textit{odd} then we have }a_n=0,\text{ and }b_n=\frac{2}{L}\int^L_0f(t)\sin\left(n\frac{\pi}{L}\right)\,dt
\end{equation*}
(next page)\newpage
\noindent\textbf{Justification for Even and odd Fourier coefficient rules}\\
Starting with the \textit{Even} case, assuming even $f(t)$, see that attempting to obtain the fourier coefficients
using their respective formulas:
\begin{align*}
a_n&=\frac{1}{L}\int^L_{-L}\underbrace{f(t)\cos\left(n\frac{\pi}{l}t\right)}_{\text{is even}}dt\\
b_n&=\frac{1}{L}\int^L_{-L}\underbrace{f(t)\sin\left(n\frac{\pi}{l}t\right)}_{\text{is odd}}dt
\end{align*}
we can thus rewrite the integrals as
\begin{equation*}
a_n=\frac{2}{L}\int^L_0f(t)\cos\left(n\frac{\pi}{L}\right)dt,\quad b_n=0
\end{equation*}
The odd case can be shown the same way; if $f(t)$ is \textit{odd}:
\begin{align*}
a_n&=\frac{1}{L}\int^L_{-L}\underbrace{f(t)\cos\left(n\frac{\pi}{l}t\right)}_{\text{is odd}}dt\\
b_n&=\frac{1}{L}\int^L_{-L}\underbrace{f(t)\sin\left(n\frac{\pi}{l}t\right)}_{\text{is even}}dt
\end{align*}
so
\begin{equation*}
a_n=0,\quad b_n=\frac{2}{L}\int^L_0f(t)\sin\left(n\frac{\pi}{L}\right)dt
\end{equation*}
\newpage

\subsection{Example: Simple Harmonic Oscillator with square wave input}
Let $f(t)$ be an odd square wave of period $2\pi$ with $f(t)=1$ for $0<t<\pi$. Given the DE
\begin{equation*}
\ddot{x}+9.1x=f(t)
\end{equation*}
We can use the already derived fourier series for $f(t)$:
\begin{equation*}
f(t)=\frac{4}{\pi}\left(
\sin t+\frac{1}{3}\sin(3t)+\frac{1}{5}\sin(5t)+\cdots\right)=\frac{4}{\pi}\sum_{n\text{ odd}}\frac{\sin(nt)}{n}
\end{equation*}
So the DE becomes
\begin{equation*}
\ddot{x}+9.1x=\frac{4}{\pi}\left(
\sin t+\frac{1}{3}\sin(3t)+\frac{1}{5}\sin(5t)+\cdots\right)
\end{equation*}
See that we can solve for each term separately and combine all the outputs using superposition. The general case
would be
\begin{equation*}
\ddot{x}+9.1x=\frac{\sin(nt)}{n}
\end{equation*}
\textbf{Complex replacement and ERF}\\
This is solved by complex replacement and exponential response. To recapitulate:
\begin{equation*}
\ddot{z}+9.1z=\frac{1}{n}e^{int}\quad\text{where }x_p=\text{Im}(z_p)
\end{equation*}
we consider a particular solution of form $z_p=Ae^{int}$ and solve for $A$ (basis for exponential response)
\begin{equation*}
(-n^2+9.1)Ae^{int}=\frac{1}{n}e^{int}
\end{equation*}
comparing coefficients and taking the imaginary part we have
\begin{align*}
A=&\frac{1}{n(9.1-n^2)},\quad\text{so }z_p=\frac{1}{n(9.1-n^2)}e^{int}\\
&x_{n,p}=\text{Im}(z_{n,p})=\frac{\sin(nt)}{n(9.1-n^2)}
\end{align*}
so by superposition we get the \textit{steady periodic solution}:
\begin{align*}
\ddot{x}+9.1x&=\frac{4}{\pi}\left(
\sin t+\frac{1}{3}\sin(3t)+\frac{1}{5}\sin(5t)+\cdots\right)=\frac{4}{\pi}\sum_{n\text{ odd}}\frac{\sin(nt)}{n}\\
x_{sp}(t)&=\frac{4}{\pi}(x_{1,p}(t)+x_{3,p}(t)+x_{5,p}(t))=
\frac{4}{\pi}\sum_{n\text{ odd}}x_{n,p}(t)\\
&=\frac{4}{\pi}\left(\frac{\sin(nt)}{n(9.1-1^2)}+\frac{\sin(nt)}{n(9.1-3^2)}+\frac{\sin(nt)}{n(9.1-5^2)}+
\ldots\right)\\&=\frac{4}{\pi}\sum_{n\text{ odd}}\frac{\sin(nt)}{n(9.1-n^2)}
\end{align*}
(next page)\newpage
\noindent\textbf{Resonant frequencies}\\
We had
\begin{equation*}
x_{sp}(t)=\frac{4}{\pi}\sum_{n\text{ odd}}\frac{\sin(nt)}{n(9.1-n^2)}
\end{equation*}
The amplitudes for the first few terms are
\begin{align*}
\frac{4}{\pi}\left(\frac{1}{9.1-1^2}\right)&\approx0.157,\quad\frac{4}{\pi}\left(\frac{1}{3(9.1-3^2)}\right)
\approx4.244,\\&\frac{4}{\pi}\left(\frac{1}{5(9.1-5^2)}\right)\approx-0.016,\quad\ldots
\end{align*}
for $n=1,3,5$ respectively. Then for $n>5$ the amplitudes are much smaller. See that the $n=3$ term has by far the
largest amplitude.\\
\vspace{1mm}\\
We can explain this by noticing that the natural frequency of this system is $\sqrt{9.1}\approx3$ and
so the system has a resonant-type response to the
``embedded third harmonic'' $\frac{\sin(3t)}{3}$ in the input signal.\\
\vspace{1mm}\\
The input signal has a base frequency 1---the presence of this third harmonic is not apprent to the eye---yet 
the driven oscillator picked it out in its reponse in the form of a dominant frequency \textit{three times} the
fundamental frequency of the input.
\newpage

\subsection{Example: Damped Harmonic Oscillator with\\triangle wave input}
Say $f(t)$ is a triangle wave as shown
\begin{center}
\includegraphics[width=10cm]{40}\\
\end{center}
Computing its fourier series would yield
\begin{equation*}
f(t)=\frac{1}{2}-\frac{4}{\pi^2}\left(\cos t+\frac{\cos3t}{3^2}+\frac{\cos5t}{5^2}+\ldots\right)
\end{equation*}
Now consider the DE
\begin{equation*}
\ddot{x}+2\dot{x}+9x=f(t)
\end{equation*}
\textbf{Solving}\\
We solve for the individual component
\begin{equation*}
\ddot{x}+2\dot{x}+9x=\cos nt
\end{equation*}
First (for the constant), when $n=0$, a particular solution is $x_{n,p}=1/9$. Now for $n\geq1$ we have by complex
replacement
\begin{equation*}
\ddot{z}_n+2\dot{z}_n+9z_n=e^{int},\quad x_n=\text{Re}(z_n)
\end{equation*}
By the ERF we have
\begin{equation*}
z_{n,p}=\frac{e^{int}}{9-n^2+2in}
\end{equation*}
We can write the complex number in polar form $9-n^2+2in=R_ne^{i\phi_n}$ where
\begin{equation*}
R_n=\sqrt{(9-n^2)^2+4n^2}\text{ and }\phi_n=\text{Arg}(9-n^2+2in)=\tan^{-1}\frac{2n}{9-n^2}
\end{equation*}
So we have
\begin{equation*}
z_{n,p}=\frac{1}{R_n}e^{i(nt-\phi_n)},\text{ implying }x_{n,p}=\frac{1}{R_n}\cos(nt-\phi_n)
\end{equation*}
By superposition we have our solution
\begin{equation*}
x_{sp}(t)=\frac{1}{18}-\frac{4}{\pi^2}\left(\frac{\cos(t-\phi_1)}{R_1}+\frac{\cos(3t-\phi_3)}{3^2R_3}
+\frac{\cos(5t-\phi_5)}{5^2R_5}+\ldots\right)
\end{equation*}
where $R_n$ and $\phi_n$ are defined as above.
\newpage

\subsection{General fourier series input to damped harmonic oscillator}
Now a generalisation of the previous example. Consider solving 
\begin{equation*}
m\ddot{x}+b\dot{x}+kx=f(t)
\end{equation*}
for the steady-periodic response $x_{sp}(t)$, where 
\begin{equation*}
f(t)=\frac{a_0}{2}+\sum^\infty_{n=1}a_n\cos\left(n\frac{\pi}{L}t\right)
\end{equation*}
\textbf{Solving}\\
For $n=0$ we have $x_{0,p}=1/k$. For $n\geq1$ we use complex replacement
\begin{equation*}
m\ddot{z}_n+b\dot{z}_n+kz_n=e^{in\frac{\pi}{L}t},\quad x_n=\text{Re}(z_n)
\end{equation*}
and exponential response
\begin{equation*}
z_{n,p}(t)=\frac{e^{in\frac{\pi}{L}t}}{p\left(in\frac{\pi}{L}\right)}
\end{equation*}
We have $p\left(in\frac{\pi}{L}\right)$ (where $p(s)=ms^2+bs+k$)
\begin{equation*}
p\left(in\frac{\pi}{L}\right)=\left(k-m\left(n\frac{\pi}{L}\right)^2\right)+ibn\frac{\pi}{L}
\end{equation*}
written in polar coordinates
\begin{equation*}
=\left|p\left(in\frac{\pi}{L}\right)\right|e^{i\phi_n}
\end{equation*}
where
\begin{equation*}
\left|p\left(in\frac{\pi}{L}\right)\right|=\sqrt{
\left(k-m\left(n\frac{\pi}{L}\right)^2\right)^2+\left(bn\frac{\pi}{L}\right)^2}
\end{equation*}
and
\begin{equation*}
\phi_n=\text{Arg}\left(p\left(in\frac{\pi}{L}\right)\right)
=\tan^{-1}\left(\frac{bn\frac{\pi}{L}}{k-m\left(n\frac{\pi}{L}\right)^2}\right)
\end{equation*}
So we have
\begin{equation*}
z_{n,p}=g_ne^{i\left(n\frac{\pi}{L}t-\phi_n\right)},\quad\text{where }
g_n=\frac{1}{\left|p\left(in\frac{\pi}{L}\right)\right|}
\end{equation*}
taking the real part we have
\begin{equation*}
x_{n,p}=g_n\cos\left(n\frac{\pi}{L}t-\phi_n\right)
\end{equation*}
Using superposition we finally get
\begin{equation*}
x_{sp}(t)=\frac{a_0}{2}x_{0,p}+\sum^\infty_{n=1}a_nx_{n,p}(t)=\frac{a_0}{2k}+\sum^{\infty}_{n=1}a_ng_n\cos\left(
n\frac{\pi}{L}t-\phi_n\right)
\end{equation*}
This is the general formula for the steady state periodic response of a second order LTI DE to an even periodic 
driver $f(t)$.
\newpage

\subsection{Step and Box functions}
\textbf{Step functions}\\
The \textit{unit step function} (also called the \textit{heaviside function}) is defined as
\begin{equation*}
u(t)=\begin{cases}
0&\text{for }t<0\\
1&\text{for }t>0\end{cases}
\end{equation*}
\begin{center}
\includegraphics[width=10cm]{41}\\
\end{center}
See that $u(t-a)$ is just $u(t)$ shifted to the right
\begin{equation*}
u(t-a)=\begin{cases}
0&\text{for }t<a\\
1&\text{for }t>a\end{cases}
\end{equation*}
Note that
\begin{itemize}
\item $u(t)$ is not defined when $t=0$. It has a \textit{jump discontinuity} at that point.
\item The graph shows that $u(0^-)=0$ and $u(0^+)=1$. Where $u(0^-)$ indicates the left-hand limit and
$u(0^+)$ the right-hand limit.
\end{itemize}
\textbf{Box Functions}\\
We define a \textit{box function} as
\begin{equation*}
u_{ab}(t)=\begin{cases}
0&\text{for }t<a\\
1&\text{for }a<t<b\\
0&\text{for }t>b
\end{cases}
\end{equation*}
\begin{center}
\includegraphics[width=10cm]{42}\\
\end{center}
See that this same function can be written as
\begin{equation*}
u_{ab}(t)=u(t-a)-u(t-b)
\end{equation*}
(next page)\newpage
\noindent\textbf{Switches}\\
By multiplying a function $f(t)$ with a step or box function, see that they act as switches to turn
$f(t)$ on and off.
\begin{center}
\includegraphics[width=10cm]{43}\\
\includegraphics[width=10cm]{44}
\end{center}
\begin{itemize}
\item\textit{Top left:} $f(t)$
\item\textit{Top right:} $u(t-a)f(t)$ is 0 for $t<a$ and $f(t)$ for $t>a$
\item\textit{Bottom left:} $[u(t-a)-u(t-b)]f(t)$ is $f(t)$ within the window $a<t<b$ and 0 otherwise.
\item\textit{Bottom right:} $u(t-a)f(t-a)$ translates $f(t)$ to the right $a$ units, 
and the result is switched on at time $a$.
\end{itemize}
(next page)\newpage
\noindent\textbf{Multiple functions}\\
See that we can change formulas for different intervals of $t$. The function
\begin{equation*}
f(t)=\begin{cases}
0&\text{for }t<0\\
f_1(t)&\text{for }0<t<1\\
f_2(t)&\text{for }1<t<2\\
f_3(t)&\text{for }2<t
\end{cases}
\end{equation*}
can be written as
\begin{equation*}
f(t)=[u(t)-u(t-1)]f_1(t)+[u(t-1)-u(t-2)]f_2(t)
+u(t-2)f_3(t)
\end{equation*}
We can also add on new functions at specific timings; see that the function
\begin{equation*}
f(t)=u(t)f_1(t)+u(t-2)f_2(t)+u(t-4)f_3(t)
\end{equation*}
looks like
\begin{equation*}
f(t)=\begin{cases}
0&\text{for }t<0\\
f_1(t)&\text{for }0<t<2\\
f_1(t)+f_2(t)&\text{for }2<t<4\\
f_1(t)+f_2(t)+f_3(t)&\text{for }4<t
\end{cases}
\end{equation*}
With no `off' switch the number of terms in each successive case grows.
\newpage

\subsection{Delta function}
The discontinuity in the unit step function $u(t)$ is an idealised model of a quantity
that goes from 0 to 1 very quickly. We assume it jumps across these values in no time.\\
\vspace{1mm}\\
This modelled `jump' is called the \textit{delta function} or \textit{Dirac delta function} or 
\textit{unit impulse}.
It is important to note that the delta function isn't really a function, but rather a 
\textit{generalised function}.\\
\vspace{1mm}\\
\textbf{Intuition}\\
Consider a substance being added to a container at a rate of $q(t)$. The total amount added to the container
from time 0 to time $t$ is
\begin{equation*}
Q(t)=\int^t_0q(u)du
\end{equation*}
Equivalently
\begin{equation*}
\dot{Q}(t)=q(t)
\end{equation*}
Assume that $q(t)$ is only nonzero for a short period of time and that the total amount of substance added
over that period is 1(unit mass). Consider two of the many possible ways this could happen:
\begin{center}
\includegraphics[width=10cm]{45}\\
\end{center}
See that the area under each box has total area equal to 1, and that the graphs for $Q(t)$ rise linearly to 1 and 
remain constant at that value thereafter.\\
(next page)\newpage
\noindent\textbf{Definition}\\
Now consider a generalisation of the example; let $q_h(t)$ be a box of width $h$ and height $1/h$. As
$h\to0$, the width of the box becomes 0, but the \textit{area of the box remains at 1}:
\begin{center}
\includegraphics[width=10cm]{46}\\
\end{center}
We define the \textit{delta function} to be the formal limit
\begin{equation*}
\delta(t)=\lim_{h\to0}q_h(t)
\end{equation*}
Graphically $\delta(t)$ is represented as a spike or harpoon at $t=0$. It is an infinitely tall spike
of infinitesimal width enclosing a total area of 1.\\
\vspace{1mm}\\
As an input function $\delta(t)$ represents the ideal case where 1 unit of input is applied
at once at time $t=0$.\\
\vspace{1mm}\\
\textbf{Properties}\\
We have
\begin{equation*}
\delta(t)=\begin{cases}
0&\text{if }t\neq0\\
\infty&\text{if }t=0
\end{cases}
\end{equation*}
Since $\delta(t)$ is the limit of graphs of area 1, the area under its graph is 1, meaning
\begin{equation*}
\int^d_c\delta(t)dt=\begin{cases}
1&\text{if }c<0<d\\
0&\text{otherwise}
\end{cases}
\end{equation*}
Also see that for any continuous function we have
\begin{equation*}
f(t)\delta(t)=f(0)\delta(t)\quad\text{and}\quad
\int^d_cf(t)\delta(t)dt=\begin{cases}
f(0)&\text{if }c<0<d\\
0&\text{otherwise}
\end{cases}
\end{equation*}
(next page)\newpage
\noindent\textbf{Properties cont.}\\
As with the unit function, we can translate the delta function; 
where $\delta(t-a)$ is 0 everywhere except $t=a$. Its total area remains as 1:
\begin{center}
\includegraphics[width=5cm]{47}\\
\end{center}
where then
\begin{equation*}
f(t)\delta(t-a)=f(a)\delta(t-a)\\
\end{equation*}
and
\begin{equation*}
\int^d_cf(t)\delta(t-a)dt=\begin{cases}
f(a)&\text{if }c<a<d\\
0&\text{otherwise}
\end{cases}
\end{equation*}
\textbf{More intuition}\\
Note that any sequence of functions with unit area become an $\delta(t)$ in the limit;
practically, $\delta(t)$ should be thought of as \textit{any function} of unit area, concentrated very near $t=0$.\\
\vspace{1mm}\\
In a sense this justifies the idea of $\delta(t)$ not really being a function, but rather a 
\textit{generalised function}.\\
\vspace{1mm}\\
\textbf{Limits}\\
The left and right-handed limits of the impulse are
\begin{equation*}
\delta(0^-)=0,\quad\delta(0^+)=0,\quad\delta(0)=\infty
\end{equation*}
So 0 is in the interval $[0^-,\infty)$ but not $[0^+,\infty)$, and
\begin{equation*}
\int^\infty_{0^-}\delta(t)dt=1\quad\text{and}\quad\int^\infty_{0^+}\delta(t)dt=0
\end{equation*}
further
\begin{equation*}
\int^{0^+}_{0^-}\delta(t)dt=1
\end{equation*}
(next page)\newpage
\noindent\textbf{Generalised Derivative---Intuition}\\
We say that
\begin{equation*}
\delta(t)=u'(t)
\end{equation*}
where $u(t)$ is the unit step function. Given the discontinuous nature of $u(t)$ this isn't like a usual derivative,
but rather a \textit{generalised derivative}. Here we justify this.\\
\vspace{1mm}\\
First see that the slope of the unit function $u(t)$ is 0 everywhere except at $t=0$ and that its slope is
$\infty$ at $t=0$.
\begin{center}
\includegraphics[width=10cm]{48}\\
\end{center}
that is, its derivative is
\begin{equation*}
u'(t)=\begin{cases}
0&\text{if }t\neq0\\
\infty&\text{if }t=0
\end{cases}
\end{equation*}
this derivative matches the properties of the delta function. It doesn't exist in a calculus sense
since discontinuities aren't differentiable. The function $u(t)$ isn't even defined at 0. 
So we call this derivative a \textit{generalised derivative}.\\
\vspace{1mm}\\
Further, consider the anti-derivative of $\delta(t)$; let
\begin{equation*}
f(t)=\int^t_{-\infty}\delta(\tau)d\tau
\end{equation*}
The fundamental theorem of calculus would lead us to say
$f'(t)=\delta(t)$. (this is only in a generalised sense since technically the FTC requires continuity) See however
that we can state
\begin{equation*}
f(t)=\begin{cases}
0&\text{if }t<0\\
1&\text{if }t>0
\end{cases}
\end{equation*}
That is, $f(t)=u(t)$, so $u(t)$ is the antiderivative of $\delta(t)$.\\
(next page)\newpage
\noindent\textbf{Scaling}\\
See that the antiderivative of a scaled impulse is also a scaled unit function, where $a$ constant
\begin{equation*}
f(t)=\int^t_{-\infty}a\delta(\tau)d\tau
\end{equation*}
yields
\begin{equation*}
f(t)=\begin{cases}
0&\text{if }t<0\\
a&\text{if }t>0
\end{cases}
\end{equation*}
which is just $au(t)$.\\
\vspace{1mm}\\
\textbf{Deriving jump discontinuities}\\
In that sense, a jump discontinuity contributes a delta function to the generalised derivative. For instance
suppose $f(t)$ has the following graph
\begin{center}
\includegraphics[width=10cm]{49}\\
\end{center}
The corresponding derivative would look like
\begin{equation*}
f'(t)=2\delta(t)-3\delta(t-2)+\begin{cases}
2t&\text{if }t<0\\
0&\text{if }0<t<2\\
3&\text{if }2<t
\end{cases}
\end{equation*}
\begin{center}
\includegraphics[width=10cm]{50}\\
\end{center}
$f'(t)$ is a generalised function. Referring to different parts of a generalised function, we call the delta
function pieces the \textit{singular part} and the remainder the \textit{regular part}.
If the singular part contains a multiple of $\delta(t-a)$, we say the function \textit{contains} $\delta(t-a)$.
\newpage

\subsection{Initial conditions}
Recall that we had the left and right-handed limits of
the unit function as
\begin{equation*}
u(0^-)=0,\quad u(0^+)=1,\quad u(0),\text{ is undefined}
\end{equation*}
and that of the delta function as
\begin{equation*}
\delta(0^-)=0,\quad\delta(0^+)=0,\quad\delta(0)=\infty
\end{equation*}
Generally, see that at a discontinuity we may need to distinguish between $0^-$ and $0^+$. Assuming $x$ is the output, we
do this by calling $x(0^-),\dot{x}(0^-),\ldots$ the \textit{pre-initial conditions} and 
$x(0^+),\dot{x}(0^+),\ldots$ the \textit{post-initial conditions}.\\
\vspace{1mm}\\
We define the \textit{rest initial conditions} as the pre-initial conditions
\begin{equation*}
x(0^-)=0,\dot{x}(0^-)=0,\ldots,x^{(n-1)}(0^-)=0
\end{equation*}
\newpage

\subsection{First order unit step response}
Consider
\begin{equation*}
\dot{x}+kx=ru(t),\quad x(0^-)=0,\quad k,r\text{ constants}
\end{equation*}
This can be rewritten as
\begin{equation*}
\dot{x}+kx=\begin{cases}0&\text{for }t<0\\
r&\text{for }t>0\end{cases}
\quad x(0^-)=0
\end{equation*}
Solving each case gives
\begin{equation*}
x(t)=\begin{cases}c_1e^{-kt}&\text{for }t<0\\
\frac{r}{k}+c_2e^{-kt}&\text{for }t>0\end{cases}
\end{equation*}
See that $x(0^-)=c_1$ and $x(0^+)=r/k+c_2$. If the two differ then there will be a jump of magnitude
\begin{equation*}
x(0^+)-x(0^-)=r/k+c_2-c_1
\end{equation*}
The initial condition $x(0^-)=0$ implies $c_1=0$ so
\begin{equation*}
x(t)=\begin{cases}0&\text{for }t<0\\
\frac{r}{k}+c_2e^{-kt}&\text{for }t>0\end{cases}
\end{equation*}
To find $c_2$ we substitute this back into the differential equation (we use a generalised derivative
for the jump at $t=0$ from $0^-$ to $0^+$)
\begin{align*}
\dot{x}+kx&=(r/k+c_2)\delta(t)+\begin{cases}
0&\text{for }t<0\\
-kc_2e^{-kt}+r+kc_2e^{-kt}&\text{for }t>0
\end{cases}\\
&=(r/k+c_2)\delta(t)+\begin{cases}
0&\text{for }t<0\\
r&\text{for }t>0\end{cases}
\end{align*}
Comparing this result with the second statement we see that
$r/k+c_2=0$ or $c_2=-r/k$.
so
\begin{equation*}
x(t)=\begin{cases}0&\text{for }t<0\\
\frac{r}{k}(1-e^{-kt})&\text{for }t>0\end{cases}
\quad=\quad\frac{r}{k}(1-e^{-kt})u(t)
\end{equation*}
With $r=1$, this is the \textit{unit step response}, sometimes written as $v(t)$, where 
$v(t)=(1/k)(1-e^{-kt})u(t)$:
\begin{center}
\includegraphics[width=10cm]{51}\\
\end{center}
Notice that it starts at 0 and goes asymptotically up to $1/k$.
\newpage

\subsection{First order unit impulse response}
(Note that the unit impulse and the unit step are very different---the magnitude of the instantaneous
impulse extends to infinity, while the discontinuity of the step function only extends to a fixed value).\\
\vspace{1mm}\\
Consider the initial value problem
\begin{equation*}
\dot{x}+kx=\delta(t),\quad x(0^-)=0,\quad k,r\text{ constants}
\end{equation*}
Due to the rest initial conditions we have $x(t)=0$ for $t<0$. The idea here is that $x(t)$ jumps from 0 to 1 
at $t=0$. That is, $x(0^+)=1$ (not $\delta(0^+)$); for $t>0$ the input $\delta(t)=0$ and therefore for $t>0$ we 
solve
\begin{equation*}
\dot{x}+kx=0,\quad x(0)=1
\end{equation*}
So we have
\begin{equation*}
x(t)=\begin{cases}
0&\text{for }t<0\\
e^{-kt}&\text{for }t>0\end{cases}
\end{equation*}
This is called the \textit{Unit impulse response}, denoted by $w(t)$:
\begin{center}
\includegraphics[width=10cm]{52}\\
\end{center}
At $t=0$ it jumps to $x=1$ and then decays exponentially to 0.\\
(next page)
\newpage
\noindent\textbf{$\delta(t)$ as a limit of box functions}\\
Now we compute the \textit{unit impulse response} as the \textit{limit of the responses to box functions}. Defining
the box function as 
\begin{equation*}
u_h(t)=\begin{cases}
0&\text{for }t<0\\
1/h&\text{for }0<t<h\\
0&\text{for }h<t
\end{cases}
\end{equation*}
It has a total area of 1 for all $h>0$ and the graph of $u_h(t)$ becomes a delta function as $h\to0$:
\begin{equation*}
\lim_{h\to0}u_h(t)=\delta(t)
\end{equation*}
Given the equation
\begin{equation*}
\dot{x}+kx=u_h(t),\quad x(0^-)=0
\end{equation*}
the solution (worked out piecewise) is found to be
\begin{equation*}
x(t)=\begin{cases}
c_1e^{-kt}&\text{for }t<0\\
\frac{1}{hk}+c_2e^{-kt}&\text{for }0<t<h\\
c_3e^{-kt}&\text{for }h<t
\end{cases}
\end{equation*}
Finding the constants, the initial condition $x(0^-)=0$ we find that $c_1=0$; substitution into the original
equation (like in the a step response) gives us $c_2=-1/hk$.\\
\vspace{1mm}\\
For the third case the principle follows that of the impulse response, setting the initial condition 
$x(h)=1/hk(1-e^{-hk})$ (since we found $c_2$). So
\begin{align*}
c_3e^{-hk}&=\frac{1}{hk}(1-e^{-hk})\\
c_3&=\frac{1}{hk}(e^{hk}-1)
\end{align*}
This gives the solution
\begin{equation*}
x(t)=\begin{cases}
0&\text{for }t<0\\
\frac{1}{hk}(1-e^{-kt})&\text{for }0<t<h\\
\frac{1}{hk}(e^{hk}-1)e^{-kt}&\text{for }h<t\end{cases}
\end{equation*}
(next page)\newpage
\noindent\textbf{Cont.}\\
We had
\begin{equation*}
x(t)=\begin{cases}
0&\text{for }t<0\\
\frac{1}{hk}(1-e^{-kt})&\text{for }0<t<h\\
\frac{1}{hk}(e^{hk}-1)e^{-kt}&\text{for }h<t\end{cases}
\end{equation*}
Now consider the limit $h\to0$. The second case becomes irrelevant (its range becomes 0), and since by 
l'hospital's rule we can say
\begin{equation*}
\lim_{h\to0}\frac{e^{hk}-1}{hk}=1
\end{equation*}
Our solution tends to
\begin{equation*}
x(t)=\begin{cases}
0&\text{for }t<0\\
e^{-kt}&\text{for }0<t\end{cases}
\end{equation*}
This limit is \textit{exactly the unit impulse response} found previously:
\begin{center}
\includegraphics[width=10cm]{53}\\
\end{center}
(input in black and output in red) See that the output rises faster and gets closer to 1 as $h\to0$.
\newpage

\subsection{Second order unit step response}
Consider
\begin{equation*}
m\ddot{x}+kx=u(t),\quad x(0^-)=0,\dot{x}(0^-)=0
\end{equation*}
This can be rewritten as
\begin{equation*}
m\ddot{x}+kx=\begin{cases}
0&\text{for }t<0\\
1&\text{for }t>0
\end{cases}\quad x(0^-)=0,\dot{x}(0^-)=0
\end{equation*}
Where the general homogenous solution is
\begin{equation*}
x_h(t)=c_1\cos(\omega_nt)+c_2\sin(\omega_nt),\quad\text{where }\omega_n=\sqrt{k/m}
\end{equation*}
Solving each case gives
\begin{equation*}
x(t)=\begin{cases}
c_1\cos(\omega_nt)+c_2\sin(\omega_nt)&\text{for }t<0\\
1/k+c_3\cos(\omega_nt)+c_4\sin(\omega_nt)&\text{for }t>0
\end{cases}
\end{equation*}
The pre-initial conditions $x(0^-)=\dot{x}(0^-)=0$ easily
imply $c_1=c_2=0$, so
\begin{equation*}
x(t)=\begin{cases}
0&\text{for }t<0\\
1/k+c_3\cos(\omega_nt)+c_4\sin(\omega_nt)&\text{for }t>0
\end{cases}
\end{equation*}
To find $c_3$ and $c_4$ we consider substituting $x(t)$ back into the DE. Computing the second deriative yields
\begin{align*}
\dot{x}&=(1/k+c_3)\delta(t)+\begin{cases}
0&\text{for }t<0\\
-c_3\omega_n\sin(\omega_nt)+c_4\omega_n\cos(\omega_nt)&\text{for }t>0
\end{cases}\\
\ddot{x}&=(1/k+c_3)\delta'(t)+c_4\omega_n\delta(t)+
\begin{cases}
0&\text{for }t<0\\
-c_3\omega^2_n\cos(\omega_nt)-c_4\omega^2_n\sin(\omega_nt)&\text{for }t>0\end{cases}
\end{align*}
We get the scaling factors for the delta function by computing $x(0^+)=1/k+c_3$ and $\dot{x}(0^+)=c_4\omega_n$.\\
\vspace{1mm}\\
See that the right hand side of the initial DE doesn't have any delta functions or $\delta'(t)$ 
(called a \textit{doublet}); the coefficients in front of
these terms must be 0:
\begin{align*}
1/k+c_3=0&\implies c_3=-1/k\\
c_4\omega_n=0&\implies c_4=0
\end{align*}
(We can also fully subsitute in the derivatives and $\omega_n=\sqrt{k/m}$ 
and see that this leads to the same conclusion)\\
(next page)\newpage
\noindent\textbf{Cont.}
Our response is
\begin{equation*}
x(t)=\begin{cases}
0&\text{for }t<0\\
1/k(1-\cos(\omega_nt))&\text{for }t>0
\end{cases}
\end{equation*}
Say $k=1$ and $m=0.5$:
\begin{center}
\includegraphics[width=10cm]{54}\\
\end{center}
If damping were added the homogeneous part would go to 0 and the unit step response would go asymptotically to 
$1/k$.
\newpage

\subsection{Second order impulse response}
\textbf{Initial deductions}\\
Consider the second order system
\begin{equation*}
m\ddot{x}+b\dot{x}+kx=c\delta(t-a)
\end{equation*}
First see that $x(t)$ is continuous at $t=a$:
\begin{equation*}
x(a^+)=x(a^-)
\end{equation*}
This comes from the idea that if $x(t)$ had a `jump' at $a$ then $\dot{x}(t)$ would contain a multiple of
$\delta(t-a)$, and $m\ddot{x}$ correspondingly a multiple of the doublet $\delta'(t-a)$; since this isnt the case
(the input $c\delta(t-a)$ doesn't contain a doublet)
$x(t)$ must be continuous at $t=a$.\\
\vspace{1mm}\\
Next see that $m\dot{x}(t)$ must have a jump discontinuity of $c$ units at $t=a$:
\begin{equation*}
m\dot{x}(a^+)-m\dot{x}(a^-)=c
\end{equation*}
In order to make the left-hand side of the DE match the input $m\ddot{x}(t)$ has to contain the term $c\delta(t-a)$.
To achieve this $m\dot{x}$ has to have a jump discontinuity of magnitude $c$.\\
\vspace{1mm}\\
\textbf{Unit impulse response}\\
Now we consider 
\begin{equation*}
m\ddot{x}+b\dot{x}+kx=\delta(t),\quad x(0^-)=0,\dot{x}(0^-)=0
\end{equation*}
Let $w(t)$ (the standard notation for the unit impulse response) denote the solution we seek. 
We know from the first statement that
$w(0^+)=w(0^-)=0$; the second statement gives us $m\dot{w}(0^+)-m\dot{w}(0^-)=1$, so $\dot{w}(0^+)=1/m$. With
that our equation becomes
\begin{equation*}
m\ddot{w}+b\dot{w}+kw=0,\quad w(0^+)=0,\dot{w}(0^+)=1/m
\end{equation*}
This becomes a constant coefficient linear DE which is easily solvable.
(next page)\newpage
\noindent\textbf{Example}\\
Consider 
\begin{equation*}
2\ddot{x}+8\dot{x}+26x=\delta(t),\quad x(0^-)=0,\dot{x}(0^-)=0
\end{equation*}
The $t<0$ case means solving the homogeneous equation with rest initial conditions, yielding $w(t)=0$. Now
considering $t>0$, the
unit jump discontinuity at $t=0$ implies $2\dot{w}(0^+)=1$. So for $t>0$ we have to solve
\begin{equation*}
2\ddot{w}+8\dot{w}+26w=0,\quad w(0^+)=0,\dot{w}(0^+)=1/2
\end{equation*}
The roots of the characteristic polynomial are $-2\pm3i$, implying
\begin{equation*}
w(t)=c_1e^{-2t}\cos(3t)+c_2e^{-2t}\sin(3t),\quad\text{for }t>0
\end{equation*}
Where substituting the post-initial conditions give $c_1=0,c_2=1/6$. So we have
\begin{equation*}
w(t)=\begin{cases}
0&\text{for }t<0\\
\frac{1}{6}e^{-2t}\sin(3t)&\text{for }t>0
\end{cases}\quad=\quad\frac{1}{6}e^{-2t}\sin(3t)u(t)
\end{equation*}
\begin{center}
\includegraphics[width=10cm]{55}\\
\end{center}
\newpage

\subsection{Higher order unit impulse response}
We can extend our reasoning in the first and second order cases to any order. Consider an $n^{th}$ order system 
with the DE:
\begin{equation*}
a_nx^{(n)}+a_{n-1}x^{(n-1)}+\ldots+a_1x'+a_0x=\delta(t),\quad\text{with rest initial conditions}
\end{equation*}
We can extrapolate the reasoning from the second order response to show
\begin{equation*}
x(0^+)=0,x'(0^+)=0,\ldots,x^{(n-2)}(0^+)=0,x^{(n-1)}(0^+)=1/a_n
\end{equation*}
There is jump discontinuity only in the $n-1$ derivative at $t=0$. The other derivatives are continuous and
thus have the same post and pre-initial conditions; with rest initial conditions this means all their 
post-initial conditions are also 0.\\
\vspace{1mm}\\
If there were a jump discontinuity at a lower derivative, then the highest derivative $x^{(n)}$ would be
some derivative of $\delta(t)$ (such as $\delta'(t),\delta''(t)$, etc.); this is impossible since the right-hand
side of the DE doesn't have any such derivatives.\\
\vspace{1mm}\\
In order to match the right side, we must also have a unit jump discontinuity in the second last derivative
(since only this would produce a generalised
derivative $\delta(t)$):
\begin{equation*}
a_nx^{(n-1)}(0^+)-a_n\underbrace{x^{(n-1)}(0^-)}_{=0}=1\implies x^{(n-1)}(0^+)=1/a_n
\end{equation*}
So we conclude that the solution is 0 for $t<0$ and for $t>0$ it is exactly the same as the solution to
\begin{equation*}
a_nx^{(n)}+a_{n-1}x^{(n-1)}+\ldots+a_1x'+a_0x=0
\end{equation*}
with initial conditions
\begin{equation*}
x(0)=0,x'(0)=0,\ldots,x^{(n-2)}(0)=0,x^{(n-1)}(0)=1/a_n
\end{equation*}
\newpage

\subsection{Convolution---Definition and properties}
\textbf{Definition}\\
The \textit{convolution} of two functions $f$ and $g$,
denoted by $f*g$, is defined as
\begin{equation*}
(f*g)(t)=\int^{t^+}_{0^-}f(\tau)g(t-\tau)d\tau\quad\text{for }t>0
\end{equation*}
This is a \textit{one-sided} convolution, which is only concerned with functions on the interval $(0^-,\infty)$.\\
\vspace{1mm}\\
\textbf{Properties}\\
\textit{Linearity}; for functions $f_1,f_2,g$ and constants $c_1,c_2$:
\begin{equation*}
(c_1f_1+c_2f_2)*g=c_1(f_1*g)+c_2(f_2*g)
\end{equation*}
This follows from the linearity of integration.\\
\vspace{1mm}\\
\textit{Commutivity}: $f*g=g*f$. This follows from the change of variable $v=t-\tau$; the limits become
\begin{equation*}
\tau=0^-\implies t-\tau=t^+\quad\text{and}\quad\tau=t^+\implies t-\tau=0^-
\end{equation*}
\textit{Associativity}: $f*(g*h)=(f*g)*h$. Showing this amounts to changing the order of integration. 
First consider the discrete case:
\begin{align*}
((f*g)*h)(n)&=\sum^n_{k=0}(f*g)(k)h(n-k)\\
&=\sum^n_{k=0}\left(\sum^k_{i=0}f(l)g(k-l)\right)h(n-k)\\
&=\sum_{0\leq l\leq k\leq n}f(l)g(k-l)h(n-k)\\
&=\sum^n_{l=0}\sum^n_{k=l}f(l)g(k-l)h(n-k)\\
&=\sum^n_{l=0}f(l)\left(\sum^{n-1}_{k=0}g(k)h(n-k-l)\right)\\
&=\sum^n_{l=0}f(l)(g*h)(n-l)\\
&=(f*(g*h))(n)
\end{align*}
(next page)\newpage
\noindent\textbf{Properties cont.}\\
The continuous case is analogously:
\begin{align*}
((f*g)*h)(t)&=\int^t_0(f*g)(s)h(t-s)ds\\
&=\int^t_{s=0}\left(\int^s_{u=0}f(u)g(s-u)du\right)h(t-s)ds\\
&=\iint_{0\leq u\leq s\leq t} f(u)g(s-u)h(t-s)du\,ds\\
&=\int^t_{u=0}f(u)\left(\int^{t-u}_{s=0}g(s)h(t-u-s)ds\right)du\\
&=\int^t_{u=0}f(u)(g*h)(t-u)du\\
&=(f*(g*h))(t)
\end{align*}
\textbf{Delta functions}\\
We have











\end{document}
