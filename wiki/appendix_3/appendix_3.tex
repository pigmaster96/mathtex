\documentclass{report} 
\title{Appendix 3}
\date{Started 5 Nov 2024}
\author{Malcolm}
\usepackage{amsmath} %import math
\usepackage{mathtools} %more math
\usepackage{amssymb} %for QED symbol
\usepackage{amsthm} %
\usepackage{bm}%bold math
\usepackage{graphicx} %import imaging
\graphicspath{{./images/}} %set imaging path
\begin{document}
\maketitle

\tableofcontents

\appendix
\chapter{Probability}

\section{Fundamental concepts}
\subsection{Probability Axioms}
\textbf{Nonnegativity}
\begin{equation*}
\mathbb{P}(A)\geq0,\text{ for every event }A.
\end{equation*}
\textbf{Additivity}\\
If $A$ and $B$ are two disjoint (mutually exclusive) events, then the probability of their union satisfies
\begin{equation*}
\mathbb P(A\cup B)=\mathbb P(A)+\mathbb P(B)
\end{equation*}
More generally, if the sample space has an infinite number of elements and $A_1,A_2,\ldots$ is a sequence of disjoint events, then the probability of their union satisfies
\begin{equation*}
\mathbb P(A_1\cup A_2\cup\cdots)=\mathbb P(A_1)+\mathbb P(A_2)+\cdots
\end{equation*}
\textbf{Normalisation}\\
The probability of the entire sample space $\Omega$ is equal to 1, that is, $\mathbb P(\Omega)=1$.
\newpage

\subsection{Discrete probability law}
If the sample space consists of a finite number of possible outcomes, then the probability law is specified by the probabilities of the events that consist of a single element. 
That is, the probability of any event $\{s_1,s_2,\ldots,s_n\}$ is the sum of the probabilities of its elements:
\begin{equation*}
\mathbb P(\{s_1,s_2,\ldots,s_n\})=\mathbb P(s_1)+\mathbb P(s_2)+\cdots+\mathbb P(s_n)
\end{equation*}
\textbf{Discrete uniform probability law}\\
If the sample space consists of $n$ possible outcomes which are equally likely (all single-element events have the same given probability), then the probability
of any event $A$ is given by
\begin{equation*}
\mathbb P(A)=\frac{\text{number of elements of $A$}}{n}
\end{equation*}

\subsection{Some properties of probability laws}
Consider a probability law, and let $A,B$, and $C$ be events.
\begin{enumerate}
\item If $A\subset B$, then $\mathbb P(A)\leq\mathbb P(B)$.
\item $\mathbb P(A\cup B)=\mathbb P(A)+\mathbb P(B)-\mathbb P(A\cap B)$.
\item $\mathbb P(A\cup B)\leq \mathbb P(A)+\mathbb P(B)$.
\item $\mathbb P(A\cup B\cup C)=\mathbb P(A)+\mathbb P(A^c\cap B)+\mathbb P(A^c\cap B^c\cap C)$.
\end{enumerate}
Note that the third property can be generalised as follows:
\begin{equation*}
\mathbb P(A_1\cup A_2\cup\ldots\cup A_n)\leq\sum^n_{i=1}\mathbb P(A_i)
\end{equation*}
which can be shown be recursively applying the property for each element.
\newpage

\subsection{Properties of conditional probability}
The conditional probability of an event $A$, given an event $B$ with $\mathbb P(B)>0$, is defined by
\begin{equation*}
\mathbb P(A|B)=\frac{\mathbb P(A\cap B)}{\mathbb P(B)}
\end{equation*}
If the possible outcomes are finitely many and equally likely, then
\begin{equation*}
\mathbb P(A|B)=\frac{\text{number of elements of }A\cap B}{\text{number of elements of }B}
\end{equation*}
\textbf{Multiplication rule}\\
We have
\begin{equation*}
\mathbb P(\cap^n_{i=1}A_i)=\mathbb P(A_1)\mathbb P(A_2|A_1)\mathbb P(A_3|A_1\cap A_2)\cdots
\mathbb P(A_n|\cap^{n-1}_{i=1}A_i)
\end{equation*}
This can be verified by
\begin{equation*}
\mathbb P(\cap^n_{i=1}A_i)=\mathbb P(A_1)\cdot\frac{\mathbb P(A_1\cap A_2)}{\mathbb P(A_1)}\cdot
\frac{\mathbb P(A_1\cap A_2\cap A_3)}{\mathbb P(A_1\cap A_2)}\cdots
\frac{\mathbb P(\cap^n_{i=1}A_i)}{\mathbb P(\cap^{n-1}_{i=1}A_i)}
\end{equation*}

\subsection{Total probability theorem}
Let $A_1,\ldots,A_n$ be disjoint events that form a partition of the sample space 
and assume that $\mathbb P(A_i)>0$ for all $i$. Then, for any event $B$, we have
\begin{equation*}
\mathbb P(B)=\mathbb P(A_1\cap B)+\cdots+\mathbb P(A_n\cap B)
\end{equation*}
Visualised:
\begin{center}
\includegraphics[width=10cm]{3}\\
\end{center}
\newpage

\subsection{Bayes' rule}
Let $A_1,A_2,\ldots,A_n$ be disjoint events that form a partition of the sample space, and assume that $\mathbb P(A_i)>0$ for all $i$. Then, for any event $B$ such that $\mathbb P(B)>0$ we have
\begin{align*}
\mathbb P(A_i|B)&=\frac{\mathbb P(A_i)\mathbb P(B|A_i)}{\mathbb P(B)}\\
&=\frac{\mathbb P(A_i)\mathbb P(B|A_i)}{\mathbb P(A_1)\mathbb P(B|A_1)+\cdots+
\mathbb P(A_n)\mathbb P(B|A_n)}
\end{align*}

\subsection{Independence}
\textbf{Definition}\\
Two events $\bm A$ and $B$ are said to be independent if
\begin{equation*}
\mathbb P(A\cap B)=\mathbb P(A)\mathbb P(B)
\end{equation*}
If in addition $\mathbb P(B)>0$, independence is equivalent to the condition
\begin{equation*}
\mathbb P(A|B)=\mathbb P(A)
\end{equation*}
\textbf{Complement is also independent}\\
If $A$ and $B$ are independent, so are $A$ and $B^c$. Intuitively, if $\mathbb P(B|A)=\mathbb P(B)$:
\begin{equation*}
\mathbb P(B^c)=1-\mathbb P(B)=1-\mathbb P(B|A)=\mathbb P(B^c|A)
\end{equation*}
to show the final equality, see that 
\begin{align*}
\mathbb P(B^c|A)+\mathbb P(B|A)&=\frac{\mathbb P(B^c\cap A)}{\mathbb P(A)}+\frac{\mathbb P(B\cap A)}{\mathbb P(A)}\\
&=\frac{\mathbb P(B^c\cap A)+\mathbb P(B\cap A)}{\mathbb P(A)}\\
&=\frac{\mathbb P(A)}{\mathbb P(A)}=1
\end{align*}
(next page)\newpage
\noindent\textbf{Conditional independence}\\
Two events $A$ and $B$ are said to be conditionally independent, given another event $C$ with $\mathbb P(C)>0$, if
\begin{equation*}
\mathbb P(A\cap B|C)=\mathbb P(A|C)\mathbb P(B|C)
\end{equation*}
If in addition, $\mathbb P(B\cap C)>0$, conditional independence is equivalent to the condition
\begin{equation*}
\mathbb P(A|B\cap C)=\mathbb P(A|C)
\end{equation*}
To derive this alternative characterisation, see
\begin{align*}
\mathbb P(A\cap B|C)&=\frac{\mathbb P(A\cap B\cap C)}{\mathbb P(C)}\\
&=\frac{\mathbb P(C)\mathbb P(B|C)\mathbb P(A|B\cap C)}{\mathbb P(C)}\\
&=\mathbb P(B|C)\mathbb P(A|B\cap C)
\end{align*}
Compare this with the initial definition and eliminate the common factor $\mathbb P(B|C)$ to get what we want.\\
\vspace{1mm}\\
Note that independence of two events $A$ and $B$ unconditioned does not imply conditional independence, and vice versa.

\subsection{Independence of a collection of events}
We say that the events $A_1,A_2,\ldots,A_n$ are independent if
\begin{equation*}
\mathbb P\left(\bigcap_{i\in S}A_i\right)=\prod_{i\in S}\mathbb P(A_i),\quad\text{for every subset $S$ of $\{1,2,\ldots,n\}$}
\end{equation*}
Take the case of three events $A_1,A_2$, and $A_3$, independence amounts to satisfying the four conditions
\begin{align*}
\mathbb P(A_1\cap A_2)&=\mathbb P(A_1)\mathbb P(A_2),\\
\mathbb P(A_1\cap A_3)&=\mathbb P(A_1)\mathbb P(A_3),\\
\mathbb P(A_2\cap A_3)&=\mathbb P(A_2)\mathbb P(A_3),\\
\mathbb P(A_1\cap A_2\cap A_3)&=\mathbb P(A_1)\mathbb P(A_2)\mathbb P(A_3)
\end{align*}
The first three conditions simply assert that any two events are independent; this property is called \textit{pairwise independence}. The fourth condition is also a requirement for independence. 
Note it is not implied by the first three and vice versa---pairwise independence does not imply independence.
\newpage

\subsection{Permutations and Combinations, Binomial Coefficient}
\textbf{$k$-permutations}\\
Starting with $n$ distinct objects, and letting $k$ be some positive integer where $k\leq n$, 
consider counting the number of different ways that we ca  pick $k$ out of these $n$ objects and arrange them
into a sequence---the number of distinct $k$-object sequences.\\
\vspace{1mm}\\
We first have $n$ choices for the first object. 
Having chosen the first, there are only $n-1$ possible choices for the second, $n-2$ for the third, and so on.
This continues until we have chosen $k-1$ objects, leaving us with $n-(k-1)$ choices for the last one. The 
number of possible sequences, called \textit{$k$-permutations}, can be written as
\begin{equation*}
n(n-1)\cdots(n-k+1)
\end{equation*}
This can be rewritten, giving us
\begin{align*}
n(n-1)\cdots(n-k+1)&=\frac{n(n-1)\cdots(n-k+1)(n-k)\cdots2\cdot1}{(n-k)\cdots2\cdot1}\\
&=\frac{n!}{(n-k)!}
\end{align*}
See that in the special case where $k=n$ we have
\begin{equation*}
n(n-1)(n-2)\cdots2\cdot1=n!
\end{equation*}
(This can also be seen from substituting $k=n$ into the formula and recalling the convention $0!=1$.)\\
(next page)
\newpage
\noindent\textbf{Reordering a set}\\
Starting with $k$ objects, consider trying to find how many ways can we order them in a set of $k$ elements. 
This follows a fairly similar principle to permutation; think of having $k$ `slots' to order $k$ elements in: 
the first `slot' has $k$ possible inputs, the second $k-1$ and so on. See that this just gives us $k!$.\\
\vspace{1mm}\\
\textbf{Combinations}\\
Combinations can be viewed as counting the number of $k$-element subsets of a given $n$-element set.
Combinations are different from permutations in that
\textit{there is no ordering of selected elements}. For instance, where the 2-permutations of the letters 
A, B, C, and D are
\begin{equation*}
\text{AB, BA, AC, CA, AD, DA, BC, CB, BD, DB, CD, DC}
\end{equation*}
the \textit{combinations} of two out of these four letters are
\begin{equation*}
\text{AB, AC, AD, BC, BD, CD}
\end{equation*}
See that the `duplicates' are grouped together; for instance AB and BA are not viewed as distinct.\\
\vspace{1mm}\\
This reasoning can be generalised: each combination is associated with $k!$ `duplicate' $k$-permutations---all
`duplicate' permutations of any given combination is just
that permutation reordered for the maximum number of times:
\begin{equation*}
\text{(any single combination of length $k$)}\cdot k!=\text{(permutations of that combination)}
\end{equation*}
The number $n!/(n-k)!$ of $k$-permutations is equal to the number of combinations times $k!$. Hence the number of
possible combinations is equal to
\begin{equation*}
\frac{n!}{k!\,(n-k)!}
\end{equation*}
\textbf{Binomial Coefficient}\\
Consider a bernoulli process with probability $p$. We want the probility of $k$ `successes' in $n$ trials. 
See that the probability of one \textit{specific} sequence of $n$ trials yielding $k$ `successes' would be
\begin{equation*}
p^k(1-p)^{n-k}
\end{equation*}
We obtain the desired probability by multiplying this by the number of \textit{combinations} of $k$ `successes' we
can obtain in $n$ trials: 
\begin{equation*}
\binom{n}{k}p^k(1-p)^{n-k}
\end{equation*}
(think tossing a coin three times and obtaining two heads---the heads might occur on 
the first and third tosses, or other \textit{combinations} of trials).
\newpage

\section{Discrete random variables}
\subsection{Functions of random variables}
If $Y=g(X)$ is a function of a random variable $X$, then $Y$ is also a random variable, since it provides a
numerical value for each possible outcome. This is because every outcome in the sample space defines a numerical 
value of $x$ for $X$, and hence also a numerical value $y=g(x)$ for $Y$.\\
\vspace{1mm}\\
If $X$ is discrete with PMF $p_X$, then $Y$ is also discrete, and its PMF $p_Y$ can be calculated using the PMF of
$X$. In particular, to obtain $p_Y(y)$ for any $y$, we add the probabilities
of all values of $x$ such that $g(x)=y$:
\begin{equation*}
p_Y(y)=\sum_{\{x|g(x)=y\}}p_X(x)
\end{equation*}

\subsection{Expectation and Variance}
\textbf{Expectation}\\
We define the \textit{expected value} of a random variable $X$ with a PMF $p_X$ by
\begin{equation*}
\boxed{\mathbb{E}[X]=\sum_xxp_X(x)}
\end{equation*}
\textbf{Variance and Standard Deviation}\\
We define the \textit{variance} associated with a random variable $X$ as
\begin{equation*}
\boxed{\text{var}(X)=\mathbb{E}\left[(X-\mathbb{E}[X])^2\right]=\sum_x(X-\mathbb{E}[X])^2p_X(x)}
\end{equation*}
(See that the because of the square the variance is always nonnegative). The variance provides a measure of
dispersion of $X$ around the mean. Another measure of dispersion is the \textit{Standard deviation} of $X$, which 
is defined as the square root of the variance and is denoted by $\sigma_X$:
\begin{equation*}
\boxed{\sigma_X=\sqrt{\text{var}(X)}}
\end{equation*}
The standard deviation is often easier to interpret because it has the same units as $X$.
\newpage

\subsection{Expected value of a function of a RV}
\textbf{Expectation of a function}\\
Let $X$ be a RV with PMF $p_X$, and let $g(X)$ be a function of $X$. Then the expected value
of the random variable $g(X)$ is given by
\begin{equation*}
\boxed{\mathbb{E}[g(X)]=\sum_xg(x)p_X(x)}
\end{equation*}
This can be shown, since
\begin{equation*}
p_Y(y)=\sum_{\{x|g(x)=y\}}p_X(x)
\end{equation*}
we have
\begin{align*}
\mathbb{E}[g(X)]&=\mathbb{E}[Y]\\
&=\sum_yyp_Y(y)\\
&=\sum_yy\sum_{\{x|g(x)=y\}}p_X(x)\\
&=\sum_y\sum_{\{x|g(x)=y\}}yp_X(x)\\
&=\sum_y\sum_{\{x|g(x)=y\}}g(x)p_X(x)\\
&=\sum_xg(x)p_X(x)
\end{align*}
\textbf{Variance}\\
Using this we can write the variance of $X$ as
\begin{equation*}
\text{var}(X)=\mathbb{E}\left[(x-\mathbb{E}[X])^2\right]
=\sum_x(x-\mathbb{E}[X])^2p_X(x)
\end{equation*}
\newpage

\subsection{Expectation and variance of linear functions}
We show for a random variable $X$, and letting $Y=aX+b$:
\begin{equation*}
\boxed{\mathbb{E}[Y]=a\mathbb{E}[X]+b,\quad
\text{var}(Y)=a^2\text{var}(X)}
\end{equation*}
Linearity of Expectations:
\begin{equation*}
\mathbb{E}[Y]=\sum_x(ax+b)p_X(x)=a\underbrace{\sum_xxp_x(x)}_{=\mathbb{E}[X]}+b\underbrace{\sum_xp_x(x)}_{=1}
=a\mathbb{E}[X]+b
\end{equation*}
Variance:
\begin{align*}
\text{var}(Y)&=\sum_x(ax+b-\mathbb{E}[aX+b])^2p_X(x)\\
&=\sum_x(ax+b-a\mathbb{E}[X]+b)^2p_X(x)\\
&=a^2\sum_x(x-\mathbb{E}[X])^2p_X(x)\\
&=a^2\text{var}(X)
\end{align*}
Note that unless $g(X)$ is a linear function, it is not generally true that $\mathbb{E}[g(X)]$ is equal to 
$g(\mathbb{E}[X])$.

\subsection{Variance in terms of Moments Expression}
We show
\begin{equation*}
\boxed{\text{var}(X)=\mathbb{E}[X^2]-(\mathbb{E}[X])^2}
\end{equation*}
see that
\begin{align*}
\text{var}(X)&=\sum_x(x-\mathbb{E}[X])^2p_X(x)\\
&=\sum_x(x^2-2x\mathbb{E}[X]+(\mathbb{E}[X])^2)p_X(x)\\
&=\sum_xx^2p_X(x)-2\mathbb{E}[X]\sum_xxp_X(x)+(\mathbb{E}[X])^2\sum_xp_X(x)\\
&=\mathbb{E}[X^2]-2(\mathbb{E}[X])^2+(\mathbb{E}[X])^2\\
&=\mathbb{E}[X^2]-(\mathbb{E}[X])^2
\end{align*}
\newpage

\subsection{Expectation and Variance of Bernoulli}
Consider a Bernoulli RV $X$ with PMF
\begin{equation*}
p_X(k)=\begin{cases}
p,&\text{if }k=1.\\
1-p,&\text{if }k=0.
\end{cases}
\end{equation*}
The mean, second moment, and variance of $X$ are as follows:
\begin{align*}
\mathbb{E}[X]&=1\cdot p+0\cdot(1-p)=p\\
\mathbb{E}[X^2]&=1^2\cdot p+0\cdot(1-p)=p\\
\text{var}(X)&=\mathbb{E}[X^2]-(\mathbb{E}[X])^2
=p-p^2=p(1-p)
\end{align*}

\subsection{Expectation of Discrete Uniform}
Consider a Discrete Uniform RV $X$ with PMF, for $k\in[a,b]$:
\begin{equation*}
p_X(k)=\begin{cases}
\frac{1}{b-a+1},&\text{if }k=a,a+1\ldots,b\\
0,&\text{otherwise}.
\end{cases}
\end{equation*}
An illustration is useful here:
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{1}\\
\end{center}
\textbf{Expectation}\\
Upon inspection one might suppose that the expectation is
\begin{equation*}
\mathbb{E}[X]=\frac{a+b}{2}
\end{equation*}
\end{figure}\\
(next page)
\newpage
\noindent\textbf{Expectation (cont.)}\\
The formula can be elucidated from the definition of the expectation. First see that a sequence
$\sum^b_{k=a}k$ can be written as
\begin{align*}
\sum^b_{k=a}k&=\sum^b_{k=1}k-\sum^{a-1}_{k=1}k\\
&=\frac{(b)(b+1)}{2}-\frac{(a-1)(a)}{2}\quad\text{(see \ref{supnotes:1})}\\
&=\frac{b^2+b-a^2+a}{2}=\frac{(b-a+1)(a+b)}{2}
\end{align*}
The last step isn't easy to factor, but working back from our `hypothesis' for the expectation it coincides.\\
\vspace{1mm}\\
so now we have
\begin{align*}
\mathbb{E}[X]&=\sum^b_{k=a}k\left(\frac{1}{b-a+1}\right)\\
&=\frac{1}{b-a+1}\sum^b_{k=a}k\\
&=\frac{1}{b-a+1}\cdot\frac{(b-a+1)(a+b)}{2}\\
\mathbb{E}[X]&=\frac{(a+b)}{2}
\end{align*}
\newpage

\subsection{Variance of Discrete Uniform}
\textbf{Case for $k\in[1,n]$}:\\
We can obtain the second moment for a discrete uniform distributed over $k\in[1,n]$ as
\begin{align*}
\mathbb{E}[X^2]&=\sum^{n}_{k=1}k^2\left(\frac{1}{n}\right)\\
&=\frac{1}{n}\sum^{n}_{k=1}k^2\\
&=\frac{1}{n}\cdot\frac{n(n+1)(2n+1)}{6}\quad\text{(see \ref{supnotes:4})}\\
&=\frac{(n+1)(2n+1)}{6}
\end{align*}
We then use the formula for variance in terms of moments expression:
\begin{align*}
\text{var}(X)&=\mathbb{E}[X^2]-(\mathbb{E}[X])^2\\
&=\frac{(n+1)(2n+1)}{6}-\left(\frac{n+1}{2}\right)^2\\
&=\frac{1}{12}(n+1)(4n+2-3n-3)\\
&=\frac{n^2-1}{12}
\end{align*}
\textbf{General case} $k\in[a,b]$:\\
For the general case, note that a RV uniformly distributed over an interval $[a,b]$ has the \textit{same variance}
as one which is uniformly distributed over $[1,b-a+1]$---the PMF of the second is just a shifted version
of the PMF of the first.\\
\vspace{1mm}\\
Therefore, the desired variance is given by the first case, but instead with $n=b-a+1$, yielding
\begin{equation*}
\boxed{\text{var}(X)=\frac{(b-a+1)^2-1}{12}=\frac{(b-a)(b-a+2)}{12}}
\end{equation*}
\newpage

\subsection{Joint PMFs of multiple random variables}
We extend the concepts of PMF to multiple variables. Consider two discrete random variables $X$ and $Y$ associated with the same experiment. The probabilities
of the values that $X$ and $Y$ can take are captured by the \textit{joint PMF} of $X$ and $Y$, denoted $p_{XY}$.\\
\vspace{1mm}\\
If $(x,y)$ is a pair of possible values of $X$ and $Y$, the probability mass of 
$(x,y)$ is the probability of the event $\{X=x,Y=y\}$:
\begin{equation*}
p_{XY}(x,y)=\bm P(X=x,Y=y)
\end{equation*}
We use the abbreviated notation $\bm P(X=x,Y=y)$ instead of the more precise notations $\bm P(\{X=x\}\cap\{Y=y\})$ or $\bm P(X=x\text{ and }Y=y)$.\\
\vspace{1mm}\\
If $A$ is the set of all pairs $(x,y)$ that have a certain property, then
\begin{equation*}
\bm P((X,Y)\in A)=\sum_{(x,y)\in A}p_{X,Y}(x,y)
\end{equation*}
In fact we can calculate the PMFs of $X$ and $Y$ using the formulas
\begin{equation*}
p_X(x)=\sum_yp_{X,Y}(x,y),\quad p_Y(y)=\sum_xp_{X,Y}(x,y)
\end{equation*}
This comes from the total probability theorem
\begin{align*}
p_X(x)&=\bm P(X=x)\\
&=\sum_y\bm P(X=x,Y=y)\\
&=\sum_yp_{X,Y}(x,y)
\end{align*}
Where the second equality follows by noting that the event $\{X=x\}$ is the union of the disjoint events $\{X=x,Y=y\}$ as $y$ ranges over all the different values of $Y$.  The formula for 
$p_Y(y)$ is verified similarly. We may refer to $p_X$ and $p_Y$ as the \textit{marginal} PMFs. Illustration on next page.\\
(next page)\newpage
\noindent\textbf{Cont.}
\begin{center}
\includegraphics[width=10cm]{4}\\
\end{center}
\textbf{Functions of Multiple Random Variables}\\
A function $Z=g(X,Y)$ of the random variables $X$ and $Y$ defines another random variable. Its PMF 
can be calculated from the joint PMF $p_{X,Y}$ according to
\begin{equation*}
p_Z(z)=\sum_{\{(x,y)|g(x,y)=z\}}p_{X,Y}(x,y)
\end{equation*}
Furthermore, the expected value rule for functions naturally extends and takes the form
\begin{equation*}
\bm E[g(X,Y)]=\sum_x\sum_yg(x,y)p_{X,Y}(x,y)
\end{equation*}
Where the verification for this is very similar to the earlier case for a function of a single random variable. In the special case where $g$ is linear and of the form
$aX+bY+c$, where $a,b$, and $c$ are given scalars, we have
\begin{equation*}
\bm E[aX+bY+c]=a\bm E[X]+b\bm E[Y]+c
\end{equation*}
(next page)\newpage
\noindent\textbf{More random variables}\\
The joint PMF of three random variables $X,Y,Z$ is defined analagously as
\begin{equation*}
p_{X,Y,Z}(x,y,z)=\bm P(X=x,Y=y,Z=z)
\end{equation*}
for all possible triplets of $(x,y,z)$. The corresponding marginal PMFs are analagously obtained using 
\begin{equation*}
p_{X,Y}(x,y)=\sum_zp_{X,Y,Z}(x,y,z)
\end{equation*}
and
\begin{equation*}
p_{X}(x)=\sum_y\sum_zp_{X,Y,Z}(x,y,z)
\end{equation*}
The expected value rule for functions is given by
\begin{equation*}
\bm E[g(X,Y,Z)]=\sum_x\sum_y\sum_zg(x,y,z)p_{X,Y,Z}(x,y,z)
\end{equation*}
where if $g$ is linear and has the form $aX+bY+cZ+d$, then
\begin{equation*}
\bm E[aX+bY+cZ+d]=a\bm E[X]+b\bm E[Y]+c\bm E[Z]+d
\end{equation*}
\begin{center}
\includegraphics[width=10cm]{4}\\
\end{center}
\newpage

\subsection{Conditioning}
A \textit{conditional PMF} of a random variable $X$, conditioned on a particular event $A$ with $\bm P(A)>0$, is defined by
\begin{equation*}
p_{X|A}(x)=\bm P(X=x|A)=\frac{\bm P(\{X=x\}\cap A)}{\bm P(A)}
\end{equation*}
By the total probability theorem we have
\begin{equation*}
\bm P(A)=\sum_x\bm P(\{X=x\}\cap A)
\end{equation*}
Combining the two formulas, see that
\begin{equation*}
\sum_xp_{X|A}(x)=1
\end{equation*}
which reinforces the idea that $p_{X|A}(x)$ is a PMF.\\
\vspace{1mm}\\
\textbf{Conditioning one random variable on another}\\
Let $X$ and $Y$ be two random variables associated with the same experiment. If we know that $Y$ is some particular $y$ with some nonzero probability, this provides partial knowledge about
the value of $X$.\\
\vspace{1mm}\\
This knowledge is captured by the \textit{condiional} PMF $p_{X|Y}$ of $X$ given $Y$, which is defined by specialising the definition of $p_{X|A}$ to events $A$ of the form
$\{Y=y\}$:
\begin{equation*}
p_{X|Y}(x|y)=\bm P(X=x|Y=y)
\end{equation*}
By the definition of conditional probabilities, we have
\begin{equation*}
p_{X|Y}(x|y)=\frac{\bm P(X=x,Y=y)}{\bm P(Y=y)}=\frac{p_{X,Y}(x,y)}{p_Y(y)}
\end{equation*}
Intuitively, let us fix some $y$ with $p_Y(y)>0$, and consider $p_{X|Y}(x|y)$ as a function of $x$. This gives us a PMF for that specific $y$, with
\begin{equation*}
\sum_xp_{X|Y}(x|y)=1
\end{equation*}
which can be verified in a similar manner to earlier. Visualisation on next page.\\
(next page)\newpage
\noindent\textbf{Cont.}
\begin{center}
\includegraphics[width=10cm]{6}\\
\end{center}
The conditional PMF is often convenient for the calculation of the joint PMF, using a sequential approach and the formulas
\begin{equation*}
p_{X,Y}(x,y)=p_Y(y)p_{X|Y}(x|y)
\end{equation*}
or
\begin{equation*}
p_{X,Y}(x,y)=p_X(x)p_{Y|X}(y|x)
\end{equation*}
See that the conditional PMF can then also be used to calculate the marginal PMFs:
\begin{equation*}
p_X(x)=\sum_yp_{X,Y}(x,y)=\sum_yp_Y(y)p_{X|Y}(x|y)
\end{equation*}
We finally note that one can define conditional PMFs involving more than two random variables such as
$p_{X,Y|Z}(x,y|z)$ or $p_{X|Y,Z}(x|y,z)$. The concepts and methods described above generalise easily.\\
(next page)\newpage
\noindent\textbf{Cont.}
\begin{center}
\includegraphics[width=10cm]{7}\\
\end{center}
To justify the second part of the third point:
\begin{align*}
p_{X|B}(x)&=\frac{\bm P(\{X=x\}\cap B)}{\bm P(B)}\\
&=\frac{\sum^n_{i=1}\bm P(\{X=x\}\cap B\cap A_i)}{\bm P(B)}\\
&=\frac{\sum^n_{i=1}\bm P(A_i\cap B)\bm P(\{X=x\}|A_i\cap B)}{\bm P(B)}\\
&=\frac{\sum^n_{i=1}\bm P(B)\bm P(A_i|B)\bm P(\{X=x\}|A_i\cap B)}{\bm P(B)}
\end{align*}
\newpage

\subsection{Conditional Expectation}
We define the conditional expectation and describe a few of its properties:
\begin{center}
\includegraphics[width=10cm]{8}\\
\end{center}(next page)
\newpage\noindent\textbf{Cont}\\
The last three equalities apply in different contexts, but are essentially equivalent; we refer to them as the \textit{total expectation theorem}. To verify the first of the three,
since we have 
\begin{equation*}
p_X(x)=\sum^n_{i=1}p_{X,A}(x,A_i)=\sum^n_{i=1}P(A_i)p_{X|A_i}(x|A_i)
\end{equation*}
We can express the expectation as
\begin{align*}
E[X]&=\sum_xxp_X(x)\\
&=\sum_xx\sum^n_{i=1}P(A_i)p_{X|A_i}(x|A_i)\\
&=\sum^n_{i=1}P(A_i)\sum_xxp_{X|A_i}(x|A_i)\\
&=\sum^n_{i=1}P(A_i)E[X|A_i]
\end{align*}
The remaining two equalities are verified similarly.
\newpage

\subsection{Independence}
We say that the random variable $X$ is independent of the event $A$ if
\begin{equation*}
P(X=x\text{ and }A)=P(X=x)P(A)=p_X(x)P(A)\quad\text{for all $x$}
\end{equation*}
From the definition of the conditional PMF, we have
\begin{equation*}
P(X=x\text{ and }A)=p_{X|A}(x)P(A)
\end{equation*}
So that as long as $P(A)>0$, independence is the same as the condition
\begin{equation*}
p_{X|A}(x)=p_X(x)\quad\text{for all $x$}
\end{equation*}
\textbf{Two random variables}\\
We say that two random variables $X$ and $Y$ are independent if
\begin{equation*}
p_{X,Y}(x,y)=p_X(x)p_Y(y)\quad\text{for all $x,y$}
\end{equation*}
This is the same as requiring that the two events $\{X=x\}$ and $\{Y=y\}$ be independent for every $x$ and $y$. 
The formula
\begin{equation*}
p_{X,Y}(x,y)=p_{X|Y}(x|y)p_Y(y)
\end{equation*}
Leads us to
\begin{equation*}
p_{X|Y}(x|y)=p_X(x)\quad\text{for all $y$ with $p_Y(y)>0$ and all $x$}
\end{equation*}
\textbf{Conditional independence}\\
Random variables $X$ and $Y$ are said to be conditionally independent given a positive probability event $A$, if
\begin{equation*}
P(X=x,Y=y|A)=P(X=x|A)P(Y=y|A)\quad\text{for all $x,y$}
\end{equation*}
or in a more compact notation
\begin{equation*}
p_{X,Y|A}(x,y)=p_{X|A}(x)p_{Y|A}(y)\quad\text{for all $x,y$}
\end{equation*}
in a manner analagous to the previous ideas, this is equivalent to
\begin{equation*}
p_{X|Y,A}(x|y)=p_{X|A}(x)\quad\text{for all $x,y$ such that $p_{Y|A}(y)>0$}
\end{equation*}
As mentioned previously, note that conditional independence may not imply unconditional independence and vice versa.
\newpage
\subsection{Expectation of independent variables}
If $X$ and $Y$ are independent random variables, then
\begin{equation*}
E[XY]=E[X]E[Y]
\end{equation*}
this can be shown:
\begin{align*}
E[XY]&=\sum_x\sum_yxyp_{XY}(x,y)\\
&=\sum_x\sum_yxyp_{X}(x)p_{Y}(y)\quad\text{(independence)}\\
&=\sum_xxp_X(x)\sum_yyp_Y(y)\\
&=E[X]E[Y]
\end{align*}
It is proven that if $X$ and $Y$ are independent, then the same is true for $g(X)$ and $h(Y)$ (not proven here). Given this, it is clear that if $X$ and $Y$ are independent, then
\begin{equation*}
E[g(X)h(Y)]=E[g(X)]E[h(Y)]
\end{equation*}

\subsection{For independent $X,Y$, $\text{var}(X+Y)=\text{var}(X)+\text{var}(Y)$}
Consider now the sum $X+Y$ of two independent varables $X$ and $Y$, and let us calculate its variance. Since the variance of a random variable is unchanged when it is shifted by a constant, it is
convenient to work with the zero-mean random variables
$\tilde X=X-E[X]$ and $\tilde Y=Y-E[Y]$. We have
\begin{align*}
\text{var}(X+Y)&=\text{var}(\tilde X+\tilde Y)\\
&=E[(\tilde X+\tilde Y)^2]\\
&=E[\tilde X^2+2\tilde X\tilde Y+\tilde Y^2]\\
&=E[\tilde X^2]+2E[\tilde X\tilde Y]+E[\tilde Y^2]\\
&=E[\tilde X^2]+E[\tilde Y^2]\\
&=\text{var}(\tilde X)+\text{var}(\tilde Y)\\
&=\text{var}(X)+\text{var}(Y)
\end{align*}
Where the property $E[\tilde X\tilde Y]=0$ is justified by the idea that functions of independent random variables are independent, so $\tilde X=X-E[X]$ and $\tilde Y=Y-E[Y]$ are independent. 
Since they have zero-mean, we obtain
\begin{equation*}
E[\tilde X\tilde Y]=E[\tilde X]E[\tilde Y]=0
\end{equation*}
The variance of the sum of two independent random variables is equal to the sum of their variances. This is different from the mean, where the mean of the sum of two random variables is always equal
to the sum of their means even if they are not independent.
\newpage

\subsection{Summary for independent random variables}
\begin{center}
\includegraphics[width=10cm]{9}\\
\end{center}

\subsection{Independence of several random variables}
The preceding discussion extends naturally to the case of more than two random variables; three random variables $X,Y$ and $Z$ are said to be independent if
\begin{equation*}
p_{X,Y,Z}(x,y,z)=p_X(x)p_Y(y)p_Z(z),\quad\text{for all }x,y,z
\end{equation*}
As before (although we didn't prove this), if $X,Y,Z$ are independent random variables, then any three random variables of the form $f(X)$, $g(Y)$, and $h(Z)$ are also independent.\\
\vspace{1mm}\\
Similarly, any two random variables of the form $g(X,Y)$ and $h(Z)$ are independent. Note however that random variables of the form $g(X,Y)$ and $h(Y,Z)$ are usually not independent because they 
are both affected by $Y$.\\
\vspace{1mm}\\
Properties such as these are intuitively clear; they can be formally verified but this is sometimes tedious. Fortunately there is a general agreement between intuition and what is
mathematically correct.
\newpage

\subsection{Variance of sum of independent random variables}
If $X_1,X_2,\ldots,X_n$ are independent random variables, then
\begin{equation*}
\text{var}(X_1+X_2+\cdots+X_n)=\text{var}(X_1)+\text{var}(X_2)+\cdots+\text{var}(X_n)
\end{equation*}
This can be verified by repeated use of the formula $\text{var}(X+Y)$ for two independent random variables $X$ and $Y$.

\subsection{Mean and variance of the Possion}
The mean of the Poisson PMF
\begin{equation*}
p_X(k)=e^{-\lambda}\frac{\lambda^k}{k!},\quad k=0,1,2,\ldots
\end{equation*}
can be calculated as follows:
\begin{align*}
E[X]&=\sum^\infty_{k=0}ke^{-\lambda}\frac{\lambda^k}{k!}\\
&=\sum^\infty_{k=1}ke^{-\lambda}\frac{\lambda^k}{k!}\quad\text{(the $k=0$ term is zero)}\\
&=\lambda\sum^\infty_{k=1}e^{-\lambda}\frac{\lambda^{k-1}}{(k-1)!}\\
&=\lambda\sum^\infty_{m=0}e^{-\lambda}\frac{\lambda^{m}}{m!}\quad\text{(let $m=k-1$)}\\
&=\lambda
\end{align*}
where the last equality is obtained by noting that
\begin{equation*}
\sum^\infty_{m=0}e^{-\lambda}\frac{\lambda^{m}}{m!}=\sum^\infty_{k=0}p_X(m)=1
\end{equation*}
Which is the normalisation property for the Poisson PMF.
(next page)\newpage
\noindent\textbf{Variance of the Poisson}\\
We have
\begin{align*}
E[X^2]&=\sum^{\infty}_{k=1}k^2e^{-\lambda}\frac{\lambda^k}{k!}\quad\text{($k=0$ term is 0)}\\
&=\lambda\sum^{\infty}_{k=1}ke^{-\lambda}\frac{\lambda^{k-1}}{(k-1)!}\\
&=\lambda\sum^{\infty}_{m=0}(m+1)e^{-\lambda}\frac{\lambda^{m}}{m!}\\
&=\lambda(E[X]+1)\\
&=\lambda(\lambda+1)
\end{align*}
from which
\begin{equation*}
\text{var}(X)=E[X^2]-(E[X])^2=\lambda(\lambda+1)-\lambda^2=\lambda
\end{equation*}

\subsection{Mean and variance of the Binomial}
For each $i$, we let $X_i$ be an independent bernoulli random variable. Then $X=X_1+X_2+\cdots+X_n$ is a binomial random variable. Its mean is $E[X]=E[X_1]+E[X_2]+\ldots+E[X_n]=np$.\\
\vspace{1mm}\\
The variance of single bernoulli random variable is $E[X^2]-(E[X])^2=p-p^2=p(1-p)$. By independence of the bernoullis, the variance of the binomial is
\begin{equation*}
\text{var}(X)=\sum^n_{i=1}\text{var}(X_i)=np(1-p)
\end{equation*}
\newpage

\subsection{Mean and variance of the Geometric}
A geometric random variable $X$ has the PMF
\begin{equation*}
p_X(k)=(1-p)^{k-1}p,\quad k=1,2,\ldots
\end{equation*}
The mean and variance are given by
\begin{equation*}
E[X]=\sum^{\infty}_{k=1}k(1-p)^{k-1}p,\quad\text{var}(X)=\sum^{\infty}_{k=1}(k-E[X])^2(1-p)^{k-1}p
\end{equation*}
evaluating these sums is somewhat tedious. As an alternative here we will apply the total expectation theorem, with $A_1=\{X=1\}=\{\text{first try is a success}\}$, and $A_2=\{X>1\}=\{\text{first try is a failiure}\}$, and end up with a much simpler calculation.\\
\vspace{1mm}\\
If the first try is successful, we have $X=1$, and
\begin{equation*}
E[X|X=1]=1
\end{equation*}
Intuitively, if the first try fails then $(X>1)$; we have wasted one try and are back to where we started. So the expected number of remaining tries is $E[X]$, and
\begin{equation*}
E[X|X>1]=1+E[X]
\end{equation*}
Thus,
\begin{align*}
E[X]&=P(X=1)E[X|X=1]+P(X>1)E[X|X>1]\\
&=p+(1-p)(1+E[X])
\end{align*}
From which we obtain
\begin{equation*}
E[X]=\frac{1}{p}
\end{equation*}
With similar reasoning we also have 
\begin{equation*}
E[X^2|X=1]=1,\quad E[X^2|X>1]=E[(1+X)^2]=1+2E[X]+E[X^2]
\end{equation*}
so that
\begin{equation*}
E[X^2]=p\cdot 1+(1-p)(1+2E[X]+E[X^2])
\end{equation*}
from which we obtain
\begin{equation*}
E[X^2]=\frac{1+2(1-p)E[X]}{p}
\end{equation*}
and using our formula $E[X]=1/p$,
\begin{equation*}
E[X^2]=\frac{2}{p^2}-\frac{1}{p}
\end{equation*}
and we conclude that
\begin{equation*}
\text{var}(X)=E[X^2]-(E[X])^2=\frac{2}{p^2}-\frac{1}{p}-\frac{1}{p^2}=\frac{1-p}{p^2}
\end{equation*}
\newpage

\subsection{Summary of results for some PMFs}
\begin{center}
\includegraphics[width=10cm]{10}\\
\includegraphics[width=10cm]{11}
\end{center}
\newpage

\section{General Random Variables}
\subsection{Continuous random variables and PDFs}
A random variable $X$ is \textit{continuous} if there is a nonnegative function $f_X$ calld the probability density function of $X$, or PDF for short, such that
\begin{equation*}
P(X\in B)=\int_B f_X(x)dx
\end{equation*}
for every subset $B$ of the real line. In particular, the probability that the value of $X$ falls within an interval is
\begin{equation*}
P(a\leq X\leq b)=\int^b_af_X(x)dx
\end{equation*}
and can be interpreted as the area under a graph of the PDF:
\begin{center}
\includegraphics[width=10cm]{12}\\
\end{center}
For any single value $a$, we have $\int^a_af_X(x)dx=0$. For this reason, including or excluding the endpoints of an interval has no effect on its probability:
\begin{equation*}
P(a\leq X\leq b)=P(a<X<b)=P(a\leq X<b)=P(a<X\leq b)
\end{equation*}
Note that to qualify as a PDF, a function $f_X$ must be nonnegative, meaning $f_X(x)\geq0$ for every $x$, and must also have the normalisation property
\begin{equation*}
\int^\infty_{-\infty}f_X(x)dx=P(-\infty<X<\infty)=1
\end{equation*}
The entire area under the graph of the PDF must sum to 1.
\newpage

\subsection{Continuous Uniform Random Variable}
Consider a random variable $X$ that takes values in an interval $[a,b]$, and that any two subintervals of the same 
length have the same probability. We refer to this type of random variable as \textit{uniform}. Its PDF has the form
\begin{equation*}
f_X(x)=\begin{cases}
\frac{1}{b-a},&\text{if $a\leq x\leq b$},\\
0,&\text{otherwise}
\end{cases}
\end{equation*}
\begin{center}
\includegraphics[width=10cm]{13}\\
\end{center}
The constant value of the PDF within $[a,b]$ is determined from the normalisation property. We have
\begin{equation*}
1=\int^\infty_{-\infty}f(x)dx=\int^b_a\frac{1}{b-a}dx
\end{equation*}

\subsection{Summary of PDF properties}
\begin{center}
\includegraphics[width=11cm]{14}\\
\end{center}
\newpage

\subsection{Expectation}
The \textit{expected value/expectation/mean} of a continuous random variable $X$ is defined by
\begin{equation*}
E[X]=\int^\infty_{-\infty}xf_X(x)dx
\end{equation*}
If $X$ is a continuous random variable with a given PDF, any real-valued function $Y=g(X)$ of $X$ is also a random variable.
In which case the mean of $g(X)$ satisfies the \textit{expected value rule}
\begin{equation*}
E[g(X)]=\int^\infty_{-\infty}g(x)f_X(x)dx
\end{equation*}
In complete analogy with the discrete case. The idea of moments of expectation and variance are also the
same as that for the discrete case.
\begin{center}
\includegraphics[width=10cm]{15}\\
\end{center}
\newpage

\subsection{Mean and variance of the Uniform Random Variable}
Consider a uniform PDF over an interval $[a,b]$. We have
\begin{align*}
E[X]&=\int^\infty_{-\infty}xf_X(x)dx=\int^b_ax\cdot\frac{1}{b-a}dx\\
&=\left.\frac{1}{b-a}\cdot\frac{1}{2}x^2\right|^b_a\\
&=\frac{1}{b-a}\cdot\frac{b^2-a^2}{2}\\
&=\frac{a+b}{2}
\end{align*}
To obtain the variance we calculate the second moment. We have
\begin{align*}
E[X^2]&=\int^b_a\frac{x^2}{b-a}dx=\frac{1}{b-a}\int^b_ax^2dx\\
&=\left.\frac{1}{b-a}\cdot\frac{1}{3}x^3\right|^b_a=\frac{b^3-a^3}{3(b-a)}\\
&=\frac{a^2+ab+b^2}{3}
\end{align*}
Thus the variance is obtained as
\begin{equation*}
\text{var}(X)=E[X^2]-(E[X])^2=\frac{a^2+ab+b^2}{3}-\frac{(a+b)^2}{4}=\frac{(b-a)^2}{12}
\end{equation*}
\newpage

\subsection{Exponential Random Variable}
An exponential random variable has a PDF of the form
\begin{equation*}
f_X(x)=\begin{cases}
\lambda e^{-\lambda x},&\text{if $x\geq0$}\\
0,&\text{otherwise}
\end{cases}
\end{equation*}
where $\lambda$ is a positive parameter characterising the PDF.
\begin{center}
\includegraphics[width=10cm]{17}\\
\end{center}
This is a legitimate PDF because
\begin{equation*}
\int^\infty_{-\infty}f_X(x)dx=\int^\infty_0\lambda e^{-\lambda x}dx=\left.-e^{-\lambda x}\right|^\infty_0=1
\end{equation*}
The probability that $X$ exceeds a certain value decreases exponentially. For any $a\geq0$, we have
\begin{equation*}
P(X\geq a)=\int^\infty_a\lambda e^{-\lambda x}dx=\left.-e^{-\lambda x}\right|^\infty_a=e^{-\lambda a}
\end{equation*}
The mean and variance can be calculated as
\begin{equation*}
E[X]=\frac{1}{\lambda},\quad\text{var}(X)=\frac{1}{\lambda^2}
\end{equation*}
We verify the mean using integration by parts,
\begin{align*}
E[X]&=\int^\infty_0x\lambda e^{-\lambda x}dx\\
&=\left.(-xe^{-\lambda x})\right|^\infty_0+\int^\infty_0e^{-\lambda x}dx\\
&=0-\left.\frac{e^{-\lambda x}}{\lambda}\right|^\infty_0\\
&=\frac{1}{\lambda}
\end{align*}
(next page)
\newpage
\noindent\textbf{Cont.}\\
Again using integration by parts, we have the second moment as
\begin{align*}
E[X^2]&=\int^\infty_0x^2\lambda e^{-\lambda x}dx\\
&=\left.(-x^2e^{-\lambda x})\right|^\infty_0+\int^\infty_02xe^{-\lambda x}dx\\
&=0+\frac{2}{\lambda}E[X]\\
&=\frac{2}{\lambda^2}
\end{align*}
Finally, using var$(X)=E[X^2]-(E[X])^2$, we obtain
\begin{equation*}
\text{var}(X)=\frac{2}{\lambda^2}-\frac{1}{\lambda^2}=\frac{1}{\lambda^2}
\end{equation*}
\newpage

\subsection{Cumulative distribution functions}
The CDF of a random variable $X$ is denoted $F_X$ and provides the probability $P(X\leq x)$. In particular, for every $x$ we have
\begin{equation*}
F_X(x)=P(X\leq x)=\begin{cases}
\sum_{k\leq x}p_X(k),&\text{if $X$ is discrete},\\
\int^x_{-\infty}f_X(t)dt,&\text{if $X$ is continuous}.
\end{cases}
\end{equation*}
\begin{center}
\includegraphics[width=9.8cm]{18}\\
\includegraphics[width=10cm]{19}\\
\end{center}
(next page)\newpage
\noindent\textbf{Cont.}
\begin{center}
\includegraphics[width=11cm]{20}\\
\end{center}
\newpage

\subsection{The geometric and exponential CDFs}
Let $X$ be a geometric random variable with parameter $p$; that is, $X$ is the number of trials until the first success in a sequence of independent bernoulli trials, where the probability
of success at each trial is $p$. For $k=1,2,\ldots$, we have $P(X=k)=p(1-p)^{k-1}$ and the CDF is given by
\begin{equation*}
F_{\text{geo}}(n)=\sum^n_{k=1}p(1-p)^{k-1}=p\frac{1-(1-p)^n}{1-(1-p)}=1-(1-p)^n,\quad\text{for }n=1,2,\ldots
\end{equation*}
Suppose now that $X$ is an exponential random variable with parameter $\lambda>0$. Its CDF is given by
\begin{equation*}
F_{\text{exp}}(x)=P(X\leq x)=0,\quad\text{for $x\leq0$}
\end{equation*}
and
\begin{equation*}
F_{\text{exp}}(x)=\int^x_0\lambda e^{-\lambda t}dt=\left.-e^{-\lambda t}\right|^x_0=1-e^{-\lambda x},\quad\text{for }x>0
\end{equation*}
To compare the two CDFs, we can define $\delta=-\text{ln}(1-p)/\lambda$, so that
\begin{equation*}
e^{-\lambda\delta}=1-p
\end{equation*}
Then we see that the values of the exponential and the geometric CDFs are equal whenever $x=n\delta$, with $n=1,2,\ldots$
\begin{equation*}
F_{\text{exp}}(n\delta)=F_{\text{geo}}(n)\quad n-1,2,\ldots
\end{equation*}
\begin{center}
\includegraphics[width=12cm]{21}\\
\end{center}
\newpage

\subsection{Normal random variables}
A continuous random variable




\newpage

\section{Limit Theorems}
\subsection{Sample mean}
\textbf{Definition}\\
Here we discuss asymptomatic behaviour of sequences of random variables. The principal context involves a sequence 
$X_1,X_2,\ldots$ of independent identically distributed random variables with expectation $\mu$ and variance 
$\sigma^2$.
We denote
\begin{equation*}
S_n=X_1+\cdots+X_n
\end{equation*}
to be the sum of the first $n$ of them. Since they are independent we also have
\begin{equation*}
\text{var}(S_n)=\text{var}(X_1)+\ldots+\text{var}(X_n)=n\sigma^2
\end{equation*}
See that the distribution of $S_n$ spreads out (it's variance increases) as $n$ increases and doesn't have a 
meaningful limit. Consider instead the \textit{sample mean}
\begin{equation*}
M_n=\frac{X_1+\cdots+X_n}{n}=\frac{S_n}{n}
\end{equation*}
\textbf{Expectation and Variance}\\
We have the expectation as
\begin{align*}
\mathbb{E}[M_n]&=\frac{\mathbb{E}[X_1+\ldots+X_n]}{n}\\
&=\frac{\mathbb{E}[X_1]+\ldots+\mathbb{E}[X_n]}{n}\\
&=\frac{n\mu}{n}=\mu
\end{align*}
and the variance as
\begin{equation*}
\text{var}(M_n)=\frac{1}{n^2}\,\text{var}(S_n)=\frac{\sigma^2}{n}
\end{equation*}
See that the variance of $M_n$ decreases to 0 as $n$ increases.\\
\vspace{1mm}\\
With this consider a new random variable, that we modify based off $M_n$ and $S_n$:
\begin{equation*}
Z_n=\frac{S_n-n\mu}{\sigma\sqrt{n}}
\end{equation*}
This has the properties
\begin{equation*}
\mathbb{E}[Z_n]=0,\quad\text{var}(Z_n)=\frac{\text{var}(S_n-n\mu)}{\sigma^2n}=1
\end{equation*}
\newpage

\subsection{Markov Inequality}
\textbf{Definition}\\
Here we consider the \textit{Markov inequality}. Loosely speaking it asserts that
if a \textit{nonnegative} random variable has a small mean, then the probability that it takes a large value must
also be small:
\begin{equation*}
\boxed{\mathbb{P}(X\geq a)\leq\frac{\mathbb{E}[X]}{a},\quad\text{if $X\geq0$ and $a>0$.}}
\end{equation*}
(intuitively, as $a$ increases, the probability that $X$ is greater than it decreases)
\begin{figure}[h]
\textbf{Justification}\\
Consider fixing a positive number $a$ and considering the random variable $Y_a$ defined by
\begin{equation*}
Y_a=\begin{cases}
0,&\text{if }X<a,\\
a,&\text{if }X\geq a.
\end{cases}
\end{equation*}
See that the relation 
\begin{equation*}
Y_a\leq X
\end{equation*}
always holds and therefore
\begin{equation*}
\mathbb{E}[Y_a]\leq\mathbb{E}[X]
\end{equation*}
\begin{center}
\includegraphics[width=10cm]{2}\\
\end{center}
See that all of the probability mass in the PDF of $X$ between 0 and $a$ is assigned to 0, and that above $a$
assigned to $a$. Since mass is shifted to the left, the expectation can only decrease:
\begin{equation*}
\mathbb{E}[X]\geq\mathbb{E}[Y_a]=a\mathbb{P}(Y_a=a)
=a\mathbb{P}(X\geq a)
\end{equation*}
from which we obtain
\begin{equation*}
a\mathbb{P}(X\geq a)
\leq\mathbb{E}[X]
\end{equation*}
\end{figure}\\
(next page)
\newpage
\noindent\textbf{Another justification}\\
See that if $X\geq0$ and $a>0$:
\begin{align*}
\mathbb{E}[X]=\int^\infty_0&xf_X(x)\,dx\geq\int^\infty_axf_X(x)\,dx\\
&\geq\int^\infty_aaf_X(x)\,dx\\
&=a\mathbb{P}(X\geq a)
\end{align*}
so
\begin{equation*}
\mathbb{E}[X]\geq a\mathbb{P}(X\geq a)
\end{equation*}
and
\begin{equation*}
\mathbb{P}(X\geq a)\leq\frac{\mathbb{E}[X]}{a}
\end{equation*}
\newpage

\subsection{Chebyshev Inequality}
\textbf{Definition}\\
The \textit{Chebyshev inequality}, loosely speaking, asserts that if a random variable has small variance, then
the probability that it takes a value far from its mean is also small: Given a random
variable $X$ with mean $\mu$ and variance $\sigma^2$,
\begin{equation*}
\boxed{\mathbb{P}(|X-\mu|\geq c)\leq\frac{\sigma^2}{c^2},\quad\text{for all $c>0$}}
\end{equation*}
Note that the Chebyshev inequality does not require the random variable to be negative.\\
\vspace{1mm}\\
\textbf{Justification}\\
Consider the nonnegative random variable $(X-\mu)^2$ and apply the Markov inequality with $a=c^2$ to obtain:
\begin{equation*}
\mathbb{P}((X-\mu)^2\geq c^2)\leq\frac{\mathbb{E}\left[(X-\mu)^2\right]}{c^2}=\frac{\sigma^2}{c^2}
\end{equation*}
Now observe that since the event $(X-\mu)^2\geq c^2$ is identical to the event $|X-\mu|\geq c$, so that
\begin{equation*}
\mathbb{P}(|X-\mu|\geq c)=\mathbb{P}((X-\mu)^2\geq c^2)\leq\frac{\sigma^2}{c^2}
\end{equation*}
The Chebyshev inequality tends to be more powerful than the Markov inequality since it also uses information on
the variance of $X$. An alternative form can also be obtained by letting $c=k\sigma, k>0$, which yields
\begin{equation*}
\mathbb{P}(|X-\mu|\geq k\sigma)\leq\frac{\sigma^2}{k^2\sigma^2}=\frac{1}{k^2}
\end{equation*}
(the probability that a random variable takes a value more than $k$ standard deviations away from its mean is at 
most $1/k^2$)\\
\vspace{1mm}\\
\textbf{Another justifcation}\\
For a derivation that doesn't use the Markov inequality, introducing the function
\begin{equation*}
g(x)=\begin{cases}
0,&\text{if }|x-\mu|<c,\\
c^2,&\text{if }|x-\mu|\geq c
\end{cases}
\end{equation*}
since $(x-\mu)^2\geq g(x)$ for all $x$ we can write
\begin{align*}
\sigma^2=\int^\infty_{-\infty}&(x-\mu)^2f_X(x)\,dx\geq
\int^\infty_{-\infty}g(x)f_X(x)\,dx\\
&=c^2\left(\int^{\mu-c}_{-\infty}f_X(x)\,dx+\int_{\mu+c}^\infty f_X(x)\,dx\right)\\
&=c^2\mathbb{P}(|X-\mu|\geq c)
\end{align*}
which can be arranged into the desired inequality.
\newpage

\subsection{Weak law of large numbers}
\textbf{Justification}\\
The weak law of large numbers asserts that the \textit{sample mean} of a large number of independent
identically distributed random variables is very close to the expectation with high probability.\\
\vspace{1mm}\\
Considering a sequence of $X_1,X_2,\ldots$ of independent identically distributed random variables with 
expectation $\mu$ and variance $\sigma^2$, recall the sample mean
is defined as
\begin{equation*}
M_n=\frac{X_1+\ldots+X_n}{n}
\end{equation*}
We had the expectation as
\begin{equation*}
\mathbb{E}[M_n]=\frac{\mathbb{E}[X_1]+\ldots+\mathbb{E}[X_n]}{n}
=\frac{n\mu}{n}=\mu
\end{equation*}
and the variance as
\begin{equation*}
\text{var}(M_n)=\frac{1}{n^2}\,\text{var}(X_1+\ldots+X_n)=\frac{n\text{var}(X)}{n^2}=\frac{\sigma^2}{n}
\end{equation*}
Applying the Chebyshev inequality gives us
\begin{equation*}
\mathbb{P}(|M_n-\mu|\geq\epsilon)\leq\frac{\sigma^2}{n\epsilon^2},\quad\text{for any $\epsilon>0$}
\end{equation*}
We observe that for any fixed $\epsilon>0$, the right hand side of this equation goes to 0 as $n$ increases.\\
\vspace{1mm}\\
\textbf{Definition}\\
This is called the \textit{weak law of large numbers}: 
Letting $X_1,X_2,\ldots$ be independent identically distributed random variables with mean $\mu$, 
for every $\epsilon>0$ we have
\begin{equation*}
\boxed{\mathbb{P}(|M_n-\mu|\geq\epsilon)=\mathbb{P}\left(\left|\frac{X_1+\ldots+X_n}{n}-\mu\right|\geq\epsilon\right)
\to0\quad\text{as $n\to\infty$}}
\end{equation*}
Intuitively, this means that for large $n$, the bulk of the distribution of $M_n$ is concentrated near $\mu$.
That is, if we consider an interval $[\mu-\epsilon,\mu+\epsilon]$ around $\mu$, then there
is a high probability that $M_n$ falls in that interval; as $n\to\infty$, this probability converges to 1.
\newpage

\subsection{}








\newpage

\chapter{Supplementary Notes}

\section{The sum of the first $n$ natural numbers is $n(n+1)/2$}
\label{supnotes:1}
We have that
\begin{equation*}
\sum^i_{i=1}i=1+2+\cdots+n
\end{equation*}
Now consider $2\sum^n_{i=1}i$:
\begin{align*}
2\sum^n_{i=1}i&=2(1+2+\cdots+(n-1)+n)\\
&=(1+2+\cdots+(n-1)+n)+(n+(n-1)+\cdots+2+1)\\
&=(1+n)+(2+(n-1))+\cdots+((n-1)+2)+(n+1)\\
&=(n+1)_1+(n+1)_2+\cdots+(n+1)_n\\
&=n(n+1)
\end{align*}
so
\begin{align*}
2\sum^n_{i=1}i&=n(n+1)\\
\sum^n_{i=1}i&=\frac{n(n+1)}{2}
\end{align*}
\newpage

\section{Telescoping series}
\label{supnotes:2}
Let $\langle b_n\rangle$ be a sequence in $\mathbb{R}$. Let $\langle a_n\rangle$ be a sequence defined as
\begin{equation*}
a_k=b_k-b_{k-1}
\end{equation*}
we show
\begin{equation*}
\boxed{\sum^n_{k=m}a_k=b_n-b_{m-1}}
\end{equation*}
See that
\begin{align*}
\sum^n_{k=m}a_k&=\sum^n_{k=m}(b_k-b_{k-1})\\
&=\sum^n_{k=m}b_k-\sum^n_{k=m}b_{k-1}\\
&=\sum^n_{k=m}b_k-\sum^{n-1}_{k=m-1}b_k\\
&=\left(\sum^{n-1}_{k=m}b_k+b_n\right)-\left(b_{m-1}+\sum^{n-1}_{k=m}b_k\right)\\
&=b_n-b_{m-1}
\end{align*}
\newpage

\section{Sum of series of products of consecutive integers}
\label{supnotes:3}
We show
\begin{align*}
\boxed{\sum^n_{j=1}j(j+1)=1\cdot2+2\cdot3+\cdots
+n(n+1)=\frac{n(n+1)(n+2)}{3}}
\end{align*}
See that
\begin{align*}
3i(i+1)&=i(i+1)(i+2)-i(i+1)(i-1)\\
&=(i+1)((i+1)+1)((i+1)-1)-i(i+1)(i-1)
\end{align*}
Thus we have the basis of a telescoping series (see (\ref{supnotes:2})):
\begin{equation*}
3i(i+1)=b(i+1)-b(i)
\end{equation*}
where
\begin{equation*}
b(i)=i(i+1)(i-1)
\end{equation*}
So we have
\begin{align*}
\sum^n_{j=1}3j(j+1)&=\sum^n_{j=1}(j+1)((j+1)+1)((j+1)-1)-j(j+1)(j-1)\\
&=n(n+1)(n+2)-0(0+1)(0-1)\\
&=n(n+1)(n+2)
\end{align*}
Thus
\begin{equation*}
\sum^n_{j=1}j(j+1)=\frac{n(n+1)(n+2)}{3}
\end{equation*}
\newpage

\section{Sum of sequence of squares}
\label{supnotes:4}
We show
\begin{equation*}
\boxed{\forall n\in\mathbb{N}:
\sum^n_{i=1}i^2=\frac{n(n+1)(2n+1)}{6}}
\end{equation*}
See that this follows from (\ref{supnotes:3}):
\begin{align*}
\sum^n_{i=1}3i(i+1)&=n(n+1)(n+2)\\
\sum^n_{i=1}3i^2+\sum^n_{i=1}3i&=n(n+1)(n+2)\\
\sum^n_{i=1}3i^2&=n(n+1)(n+2)-3\frac{n(n+1)}{2}\quad\text{see (\ref{supnotes:1}))}\\
\sum^n_{i=1}i^2&=\frac{n(n+1)(n+2)}{3}-\frac{n(n+1)}{2}\\
&=\frac{2n(n+1)(n+2)-3n(n+1)}{6}\\
&=\frac{n(n+1)(2n+1)}{6}
\end{align*}














\end{document}
